# AutoScore for ordinal outcomes (AutoScore-Ordinal) {#top}

```{r, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

AutoScore-Ordinal refers to the AutoScore framework for developing point-based scoring models for ordinal outcomes. Similar to the implementation described in [Chapter 4](04-autoscore.qmd#top) for binary outcomes, AutoScore-Ordinal is implemented by five functions: `AutoScore_rank_Ordinal()`, `AutoScore_parsimony_Ordinal()`, `AutoScore_weighting_Ordinal()`, `AutoScore_fine_tuning_Ordinal()` and `AutoScore_testing_Ordinal()`.

In this chapter, we demonstrate the use of AutoScore-Ordinal to develop sparse
risk scores for an ordinal outcome, adjust parameters to improve
interpretability, assess the performance of the final model and generate a
conversion table to easily map score to risk. To facilitate clinical
applications, in the following sections we demonstrate AutoScore application
with large and small datasets and with missing information.

::: callout-important
- *Scoring models below are based on simulated data to demonstrate AutoScore usage.* 
- *Variable names are intentionally masked to avoid misinterpretation and misuse.*
:::

Cite the following paper for AutoScore-Ordinal:

Saffari SE, Ning Y, Feng X, Chakraborty B, Volovici V, Vaughan R, Ong
ME, Liu N, AutoScore-Ordinal: An interpretable machine learning framework for
generating scoring models for ordinal outcomes, arXiv:2202.08407
(<https://doi.org/10.48550/arxiv.2202.08407>)

## Demo 1: large sample {#demo1}

In Demo 1, we demonstrate the use of AutoScore-Ordinal on a dataset with 20,000
observations using split-sample approach (i.e., to randomly divide the full
dataset into training, validation and test sets) for model development.

::: callout-important
*- Before proceeding, follow the steps in [Chapter 2](03-data_processing.qmd#top)
to ensure all data requirements are met.*
*- Refer to [Chapter 3](02-desc_analysis.qmd#ordinal) for how to generate simple
descriptive statistics before building prediction models.*
:::

### Load package and data

```{r, warning=TRUE, message=TRUE}
library(AutoScore)
data("sample_data_ordinal")
dim(sample_data_ordinal)
head(sample_data_ordinal)
check_data_ordinal(sample_data_ordinal)
```

### Prepare training, validation, and test datasets

- Option 1: Prepare three separate datasets to train, validate, and test models.
- Option 2: Use demo codes below to randomly split your dataset into training,
validation, and test datasets (70%, 10%, 20%, respectively), possibly stratified
by outcome categories (`strat_by_label = TRUE`) to ensure they are well
represented in all three datasets.

```{r}
set.seed(4)
out_split <- split_data(data = sample_data_ordinal, ratio = c(0.7, 0.1, 0.2), 
                        strat_by_label = TRUE)
train_set <- out_split$train_set
validation_set <- out_split$validation_set
test_set <- out_split$test_set
```

### STEP(i): Generate variable ranking list 

<p class="autoscore-module">AutoScore-Ordinal Module 1</p>

- `ntree`: Number of trees in the random forest algorithm (Default: 100).

```{r rank_ord, fig.width=6, fig.height=7, cache=TRUE}
ranking <- AutoScore_rank_Ordinal(train_set = train_set, ntree = 100)
```

### STEP(ii): Select model with parsimony plot {#large-data}

<p class="autoscore-module">AutoScore-Ordinal Modules 2+3+4</p>

-   `n_min`: Minimum number of selected variables (Default: 1).
-   `n_max`: Maximum number of selected variables (Default: 20).
-   `categorize`: Methods for categorizing continuous variables. Options include
`"quantile"` or `"kmeans"` (Default: `"quantile"`).
-   `quantiles`: Predefined quantiles to convert continuous variables to
categorical ones (Default: `c(0, 0.05, 0.2, 0.8, 0.95, 1)`). Available if 
`categorize = "quantile"`.
-   `max_cluster`: The max number of cluster (Default: 5). Available if
`categorize = "kmeans"`.
-   `max_score`: Maximum total score (Default: 100).
-   `auc_lim_min`: y-axis limits (min) of the parsimony plot (Default: 0.5)
-   `auc_lim_max`: y-axis limits (max) of the parsimony plot (Default: `"adaptive"`)
-   `link`: link function in the ordinal regression, which affects predictive
performance. Options include `"logit"` (for proportional odds model),
`"cloglog"` (for proportional hazard model) and `"probit"` (Default: `"logit"`).

::: callout-important
*Use the same link throughout descriptive analysis and model building steps.* 
:::

```{r mauc_ord, fig.width=7, fig.height=5, cache=TRUE}
link <- "logit"
mAUC <- AutoScore_parsimony_Ordinal(
  train_set = train_set, validation_set = validation_set, 
  rank = ranking, link = link, max_score = 100, n_min = 1, n_max = 20,
  categorize = "quantile", quantiles = c(0, 0.05, 0.2, 0.8, 0.95, 1), 
  auc_lim_min = 0
)
```

-   Determine the optimal number of variables (`num_var`) based on the parsimony
plot obtained in STEP(ii).
-   The final list of variables is the first `num_var` variables in the ranked
list `ranking` obtained in STEP(i).
-   Optional: User can adjust the finally included variables `final_variables`
based on the clinical preferences and knowledge.

```{r}
# Example 1: Top 5 variables are selected
num_var <- 5
final_variables <- names(ranking[1:num_var])

# Example 2: Top 14 variables are selected
num_var <- 14
final_variables <- names(ranking[1:num_var])

# Example 3: Top 5 variables, the 11th and 14th variable are selected
final_variables <- names(ranking[c(1:5, 11, 14)])
```

### STEP(iii): Generate initial scores with final variables 

<p class="autoscore-module">Re-run AutoScore-Ordinal Modules 2+3</p>

- Generate `cut_vec` with current cutoffs of continuous variables, which can be
fine-tuned in STEP(iv).
- Performance of resulting scores is evaluated using the mean AUC across
dichotomous classifications (mAUC), with 95% CI computed using bootstrap 
(Default: `n_boot = 100` bootstrap samples). Setting `n_boot = 1` disables
bootstrap and reports mAUC without CI.

```{r}
cut_vec <- AutoScore_weighting_Ordinal(
  train_set = train_set, validation_set = validation_set, 
  final_variables = final_variables, link = link, max_score = 100,
  categorize = "quantile", quantiles = c(0, 0.05, 0.2, 0.8, 0.95, 1), 
  n_boot = 100
)
```

### STEP(iv): Fine-tune initial score from STEP(iii) {#fine-tune}

<p class="autoscore-module">AutoScore-Ordinal Module 5 & Re-run AutoScore-Ordinal Modules 2+3</p>

-   Revise `cut_vec` with domain knowledge to update the scoring table
(AutoScore-Ordinal Module 5).
-   Re-run AutoScore-Ordinal Modules 2+3 to generate the updated scores.
-   Users can choose any cutoff values and/or any number of categories, but are
suggested to choose numbers close to the automatically determined values.

```{r}
## For example, we have current cutoffs of continuous variable: Age 
## ==============  ===========  =====
## variable        interval     point
## ==============  ===========  =====
## Age                 <27          0  
##                     [27,46)      2  
##                     [46,78)     12  
##                     [78,87)     16 
##                     >=87        19 
```

- Current cutoffs:`c(27, 46, 78, 87)`. We can fine tune the cutoffs as follows:

```{r}
# Example 1: rounding to a nice number
cut_vec$Age <- c(25, 45, 75, 85)

# Example 2: changing cutoffs according to clinical knowledge or preference 
cut_vec$Age <- c(25, 50, 75, 85)

# Example 3: combining categories
cut_vec$Age <- c(45, 75, 85)
```

- mAUC and 95% bootstrap CI (Default: `n_boot = 100` bootstrap samples) are
reported after fine-tuning.

```{r, warning = FALSE}
cut_vec$Util_D <- c(2 / 3, 4 / 3, 4, 6)
cut_vec$Vital_F <- c(17, 20, 25, 28)
cut_vec$Vital_A <- c(60, 70, 95, 115)
cut_vec$Lab_A <- c(45, 60, 135, 595)
cut_vec$Age <- c(25, 45, 75, 85)
scoring_table <- AutoScore_fine_tuning_Ordinal(
  train_set = train_set, validation_set = validation_set,
  final_variables = final_variables, link = link, cut_vec = cut_vec,
  max_score = 100, n_boot = 100
)
```

### STEP(v): Evaluate final risk scores on test dataset 

<p class="autoscore-module">AutoScore-Ordinal Module 6</p>

- mAUC and generalised c-index are reported for the test set, with 95% bootstrap
CI (Default: `n_boot = 100` bootstrap samples).

```{r}
pred_score <- AutoScore_testing_Ordinal(
  test_set = test_set, link = link, final_variables = final_variables, 
  cut_vec = cut_vec, scoring_table = scoring_table, 
  with_label = TRUE, n_boot = 100
)
head(pred_score)
```

- Users can compute mAUC and generalised c-index (with 95% bootstrap CI) for
previously saved `pred_score`.

```{r}
print_performance_ordinal(
  label = pred_score$Label, score = pred_score$pred_score, 
  n_boot = 100, report_cindex = TRUE
)
```

- Users can use `pred_score` for further analysis or save it as CSV to other software.

```{r, eval=FALSE}
write.csv(pred_score, file = "pred_score.csv")
```

### Map score to risk {#demo1-map}

- The interactive figure below maps score to predicted risks.
- `point_size`: Size of points indicating all attainable scores (Default: 0.5).

```{r}
plot_predicted_risk(pred_score = pred_score, max_score = 100, 
                    final_variables = final_variables, link = link,
                    scoring_table = scoring_table, point_size = 1)
```

- Given the proportion of subjects for each score value (see figure above),
select reasonable score breaks (Default: 5, 10, 15, ..., 70) to report the
average predicted risk within each score interval, which can be used to predict
risk for a new subject.
- When selecting score breaks, avoid creating score intervals with too few observations.

```{r}
conversion_table_ordinal(pred_score = pred_score, link = link,
                         score_breaks = seq(from = 5, to = 70, by = 5), 
                         digits = 4)
```

## Demo 2: small sample {#demo2}

In Demo 2, we demonstrate AutoScore-Ordinal application for a small dataset with
5000 samples using cross-validation.

### Get small dataset with 5000 samples

For demonstration purpose, randomly sample a small dataset with 5000 samples
from original sample data, stratified by the outcome.

```{r}
data("sample_data_ordinal_small")
```

### Prepare training, validation, and test datasets

- Option 1: Prepare two separate datasets to train and test models.
- Option 2: Use demo codes below to randomly split your dataset into training
and test datasets (70% and 30%, respectively). For cross-validation, `train_set`
is equal to `validation_set` and the ratio of `validation_set` should be 0. Then
cross-validation will be implemented in the STEP(ii),
`AutoScore_parsimony_Ordinal()`.

```{r}
set.seed(4)
out_split <- split_data(data = sample_data_ordinal_small, 
                        ratio = c(0.7, 0, 0.3), cross_validation = TRUE,
                        strat_by_label = TRUE)
train_set <- out_split$train_set
validation_set <- out_split$validation_set
test_set <- out_split$test_set
```

### STEP(i): Generate variable ranking list 

<p class="autoscore-module">AutoScore-Ordinal Module 1</p>

- `ntree`: Number of trees in the random forest algorithm (Default: 100).

```{r rank_ord_small, fig.width=6, fig.height=7, cache=TRUE}
ranking <- AutoScore_rank_Ordinal(train_set = train_set, ntree = 100)
```

### STEP(ii): Select the best model with parsimony plot 

<p class="autoscore-module">AutoScore-Ordinal Modules 2+3+4</p>

-   `n_min`: Minimum number of selected variables (Default: 1).
-   `n_max`: Maximum number of selected variables (Default: 20).
-   `categorize`: Methods for categorizing continuous variables. Options include
`"quantile"` or `"kmeans"` (Default: `"quantile"`).
-   `quantiles`: Predefined quantiles to convert continuous variables to
categorical ones (Default: `c(0, 0.05, 0.2, 0.8, 0.95, 1)`). Available if 
`categorize = "quantile"`.
-   `max_cluster`: The max number of cluster (Default: 5). Available if
`categorize = "kmeans"`.
-   `max_score`: Maximum total score (Default: 100).
-   `auc_lim_min`: y-axis limits (min) of the parsimony plot (Default: 0.5)
-   `auc_lim_max`: y-axis limits (max) of the parsimony plot (Default: `"adaptive"`)
-   `link`: link function in the ordinal regression, which affects predictive
performance. Options include `"logit"` (for proportional odds model),
`"cloglog"` (for proportional hazard model) and `"probit"` (Default: `"logit"`).

::: callout-important
*Use the same link throughout descriptive analysis and model building steps.* 
:::

```{r mauc_ord_small, fig.width=7, fig.height=5, cache=TRUE}
link <- "logit"
mAUC <- AutoScore_parsimony_Ordinal(
  train_set = train_set, validation_set = validation_set, link = link,
  rank = ranking, max_score = 100, n_min = 1, n_max = 20,
  categorize = "quantile", quantiles = c(0, 0.05, 0.2, 0.8, 0.95, 1), 
  auc_lim_min = 0
)
```

-   Determine the optimal number of variables (`num_var`) based on the parsimony plot obtained in STEP(ii).
-   The final list of variables is the first `num_var` variables in the ranked list `ranking` obtained in STEP(i).
-   Optional: User can adjust the finally included variables `final_variables` based on the clinical preferences and knowledge.

```{r}
# Example 1: Top 6 variables are selected
num_var <- 6
final_variables <- names(ranking[1:num_var])

# Example 2: Top 14 variables are selected
num_var <- 14
final_variables <- names(ranking[1:num_var])

# Example 3: Top 3 variables, the 6th, 11th and 14th variable are selected
final_variables <- names(ranking[c(1:3, 6, 11, 14)])
```

### STEP(iii): Generate initial scores with final variables 

<p class="autoscore-module">Re-run AutoScore-Ordinal Modules 2+3</p>

- Generate `cut_vec` with current cutoffs of continuous variables, which can be
fine-tuned in STEP(iv).
- Performance of resulting scores is evaluated using the mean AUC across
dichotomous classifications (mAUC), with 95% CI computed using bootstrap 
(Default: `n_boot = 100` bootstrap samples). Setting `n_boot = 1` disables
bootstrap and reports mAUC without CI.

```{r}
cut_vec <- AutoScore_weighting_Ordinal(
  train_set = train_set, validation_set = validation_set, 
  final_variables = final_variables, link = link,
  max_score = 100,
  categorize = "quantile",
  quantiles = c(0, 0.05, 0.2, 0.8, 0.95, 1), n_boot = 100
)
```

### STEP(iv): Fine-tune initial score from STEP(iii) 

<p class="autoscore-module">AutoScore-Ordinal Module 5 & Re-run AutoScore-Ordinal Modules 2+3</p>

Similar to the [large sample scenario](#fine-tune), users can fine-tune cutoff
values for continuous variables, rerun AutoScore-Ordinal Modules 2+3 to generate
the updated scores and assess model performance of the updated model using mAUC.

```{r}
cut_vec$Util_D <- c(2 / 3, 4 / 3, 4, 6)
cut_vec$Lab_A <- c(45, 60, 135, 595)
cut_vec$Age <- c(25, 45, 75, 85)
cut_vec$Vital_A <- c(60, 70, 95, 115)
scoring_table <- AutoScore_fine_tuning_Ordinal(
  train_set = train_set, validation_set = validation_set, link = link,
  final_variables = final_variables, cut_vec = cut_vec, max_score = 100, 
  n_boot = 100
)
```

### STEP(v): Evaluate final risk scores on test dataset 

<p class="autoscore-module">AutoScore-Ordinal Module 6</p>

- mAUC and generalised c-index are reported for the test set, with 95% bootstrap
CI (Default: `n_boot = 100` bootstrap samples).

```{r}
pred_score <- AutoScore_testing_Ordinal(
  test_set = test_set, link = link,
  final_variables = final_variables, cut_vec = cut_vec, 
  scoring_table = scoring_table, 
  with_label = TRUE, n_boot = 100
)
head(pred_score)
```

- Users can use `pred_score` for further analysis or save it as CSV to other software.

```{r, eval=FALSE}
write.csv(pred_score, file = "pred_score.csv")
```

### Map score to risk

- As illustrated in [Demo 1](#demo1-map), users can use `plot_predicted_risk()`
and `conversion_table_ordinal()` functions to map score to risk.

## Demo 3: data with missing values

In Demo 3, we demonstrate AutoScore-Ordinal for application to data with missing
values.

### Load package and data

```{r}
library(AutoScore)
data("sample_data_ordinal")
check_data_ordinal(sample_data_ordinal)
```

### Create informative missing

- For demonstration, we manually induce missing to two variables in the [large
sample](#demo1) (20% in `Lab_A` and 60% in `Vital_A`.), assuming values close to
normal range (i.e., median value in the complete data) are more likely to be
missing.
- Such missing pattern is more common in laboratory tests and vital signs, where
doctors may not choose to order a test if there is no reason to suspect
deviation from normal range.

```{r}
sample_data_ordinal_missing <- AutoScore:::induce_informative_missing(
  df = sample_data_ordinal, vars_to_induce = c("Lab_A", "Vital_A"), 
  prop_missing = c(0.2, 0.6)
)
```

### AutoScore steps

- Use the same code in [Demo 1](#demo1) to analyse large sample with missing.
For illustrative purpose we repeat STEP(i)-(iii) below.
- As shown below, such missingness pattern has little impact on variable ranking
and cutoff values.

::: callout-note
- *High missing rate may cause the variable ranking less reliable, and thus, caution is needed for variable selection using parsimony plot.*
:::

```{r missing_ord, fig.width=7, fig.height=5, cache=TRUE}
set.seed(4)
out_split <- split_data(data = sample_data_ordinal_missing, 
                        ratio = c(0.7, 0.1, 0.2), strat_by_label = TRUE)
train_set <- out_split$train_set
validation_set <- out_split$validation_set
test_set <- out_split$test_set

link <- "logit"
mAUC <- AutoScore_parsimony_Ordinal(
  train_set = train_set, validation_set = validation_set, link = link,
  rank = ranking, max_score = 100, n_min = 1, n_max = 20,
  categorize = "quantile", quantiles = c(0, 0.05, 0.2, 0.8, 0.95, 1), 
  auc_lim_min = 0
)
final_variables <- names(ranking[c(1:3, 6, 11, 14)])
cut_vec <- AutoScore_weighting_Ordinal(
  train_set = train_set, validation_set = validation_set, 
  final_variables = final_variables, link = link,
  max_score = 100,
  categorize = "quantile",
  quantiles = c(0, 0.05, 0.2, 0.8, 0.95, 1), n_boot = 100
)
```
