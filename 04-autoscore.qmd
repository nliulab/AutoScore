# AutoScore for binary outcomes {#top}

## development with large sample

(sample size = 20000)**

### load package and data
```{r}
library(AutoScore)
data("sample_data")
names(sample_data)[names(sample_data) == "Mortality_inpatient"] <- "label"
check_data(sample_data)
```


In Demo #1, we demonstrate the use of AutoScore on a comparably large dataset where separate training and validation sets are available. 
Please note that it is just a demo using simulated data, and thus, the result might not be clinically meaningful.

### Prepare training, validation, and test datasets
- Option 1: Prepare three separate datasets to train, validate, and test models.
- Option 2: Use demo codes below to randomly split your dataset into training, validation, and test datasets (70%, 10%, 20%, respectively).
```{r}
set.seed(4)
out_split <- split_data(data = sample_data, ratio = c(0.7, 0.1, 0.2))
train_set <- out_split$train_set
validation_set <- out_split$validation_set
test_set <- out_split$test_set
```


### STEP(i): Generate variable ranking list (AutoScore Module 1)
- `method`: "rf" (default) or "auc".

- method = `rf`: "rf" refers to random forest-based ranking  
- `ntree`: Number of trees required only if when `method` is "rf" (Default: 100).
```{r}
ranking <- AutoScore_rank(train_set = train_set, method = "rf", ntree = 100)
```


```{r,echo=FALSE}
ranking_temp <- ranking
```


- method = `auc`: "auc" refers to the AUC-based ranking. For "auc", univariate models will be built based on the train set, and the variable ranking is constructed via the AUC performance of corresponding univariate models on the validation set (`validation_set`).  
- `validation_set`: validation set required only if when `method` is "auc".
```{r}
ranking <- AutoScore_rank(train_set = train_set, method = "auc", validation_set = validation_set)
```

```{r,echo=FALSE}
ranking <- ranking_temp
```



### STEP(ii): Select the best model with parsimony plot (AutoScore Modules 2+3+4) {#rf-pars}
- `nmin`: Minimum number of selected variables (Default: 1).
- `nmax`: Maximum number of selected variables (Default: 20).
- `categorize`: Methods for categorizing continuous variables. Options include `"quantile"` or `"kmeans"` (Default: `"quantile"`).
- `quantiles`: Predefined quantiles to convert continuous variables to categorical ones. (Default: `c(0, 0.05, 0.2, 0.8, 0.95, 1)`) Available if `categorize = "quantile"`.
- `max_cluster`: The max number of cluster (Default: 5). Available if `categorize = "kmeans"`.
- `max_score`: Maximum total score (Default: 100).
- `auc_lim_min`: Min y_axis limit in the parsimony plot (Default: 0.5). 
- `auc_lim_max`: Max y_axis limit in the parsimony plot (Default: "adaptive"). 
```{r, }
AUC <- AutoScore_parsimony(
    train_set = train_set,
    validation_set = validation_set,
    rank = ranking,
    max_score = 100,
    n_min = 1,
    n_max = 20,
    categorize = "quantile",
    quantiles = c(0, 0.05, 0.2, 0.8, 0.95, 1),
    auc_lim_min = 0.5,
    auc_lim_max = "adaptive"
  )

```

- Users could use the `AUC` for further analysis or export it as the CSV to other software for plotting.
```{r csv_generate2, eval=FALSE}
write.csv(data.frame(AUC), file = "D:/AUC.csv")
```

- Determine the optimal number of variables (`num_var`) based on the parsimony plot obtained in STEP(ii). 
- The final list of variables is the first `num_var` variables in the ranked list `ranking` obtained in STEP(i). 
- Optional: User can adjust the finally included variables `final_variables` based on the clinical preferences and knowledge.
```{r}
# Example 1: Top 6 variables are selected
num_var <- 6
final_variables <- names(ranking[1:num_var])

# Example 2: Top 9 variables are selected
num_var <- 9
final_variables <- names(ranking[1:num_var])

# Example 3: Top 6 variables, the 9th and 10th variable are selected
num_var <- 6
final_variables <- names(ranking[c(1:num_var, 9, 10)])
```


```{r finalvariab2, results = "hide", warning=TRUE, message=FALSE,eval=TRUE,include=FALSE}
num_var <- 6
final_variables <- names(ranking[1:num_var])
```

### STEP(iii): Generate initial scores with the final list of variables (Re-run AutoScore Modules 2+3) {#model}
- Generate `cut_vec` with current cutoffs of continuous variables, which can be fine-tuned in STEP(iv).
```{r weighting,fig.width=5,fig.height=5,warning = FALSE}
cut_vec <- AutoScore_weighting( 
    train_set = train_set,
    validation_set = validation_set,
    final_variables = final_variables,
    max_score = 100,
    categorize = "quantile",
    quantiles = c(0, 0.05, 0.2, 0.8, 0.95, 1)
  )

```

### STEP(iv): Fine-tune the initial score generated in STEP(iii) (AutoScore Module 5 & Re-run AutoScore Modules 2+3) 
- Revise `cut_vec` with domain knowledge to update the scoring table (AutoScore Module 5).
- Re-run AutoScore Modules 2+3 to generate the updated scores.
- Users can choose any cutoff values and/or any number of categories, but are suggested to choose numbers close to the automatically determined values.

```{r}
## For example, we have current cutoffs of continuous variable: Age 
## ==============  ===========  =====
## variable        interval     point
## ==============  ===========  =====
## Age             <35            0  
##                 [35,49)        7  
##                 [49,76)       17  
##                 [76,89)       23  
##                 >=89          27  
```

- Current cutoffs:`c(35, 49, 76, 89)`. We can fine tune the cutoffs as follows:
```{r}

# Example 1: rounding up to a nice number
cut_vec$Age <- c(35, 50, 75, 90)

# Example 2: changing cutoffs according to clinical knowledge or preference 
cut_vec$Age <- c(25, 50, 75, 90)

# Example 3: combining categories
cut_vec$Age <- c(50, 75, 90)

```
- Then we do similar checks for other variables and update scoring table using new cutoffs if needed.
```{r scoring,fig.width=5,fig.height=5, warning = FALSE}
cut_vec$Lab_H <- c(0.2, 1, 3, 4)
cut_vec$Lab_K <- c(10, 40)
cut_vec$Lab_B <- c(10, 17)
cut_vec$hVital_A<- c(70, 98)


scoring_table <- AutoScore_fine_tuning(train_set,
                        validation_set,
                        final_variables,
                        cut_vec,
                        max_score = 100)

```

### STEP(v): Evaluate final risk scores on test dataset (AutoScore Module 6)
- `threshold`: Score threshold for the ROC analysis to generate sensitivity, specificity, etc. If set to `"best"`, the optimal threshold will be calculated (Default: `"best"`).
- `with_label`: Set to `TRUE` if there are labels in the `test_set` and performance will be evaluated accordingly (Default: `TRUE`).
- Set the `with_label` to `FALSE` if there are not `label` in the `test_set` and the final predicted scores will be the output without performance evaluation.
```{r,fig.width=5,fig.height=5,warning = FALSE}
pred_score <-
  AutoScore_testing(
    test_set = test_set,
    final_variables = final_variables,
    cut_vec = cut_vec,
    scoring_table = scoring_table,
    threshold = "best",
    with_label = TRUE
  )
head(pred_score)
```

- Use `print_roc_performance()` to generate the performance under different score thresholds (e.g., 50).
```{r}
print_roc_performance(pred_score$Label, pred_score$pred_score, threshold = 50)
```

## evaluation (conversion table, calibration etc)

Further analysis based on the final scoring systems (e.g., conversion table, model calibration, output the score)
- Use `conversion_table()` to generate the performance under different risk (i.e., probability of mortality based on logistic regression estimation) cut-off (e.g., 0.01, 0.05, 0.1, 0.2, 0.5).
```{r,message=FALSE, warning=FALSE, error=FALSE}
conversion_table(pred_score = pred_score, by ="risk", values = c(0.01,0.05,0.1,0.2,0.5))
```

- Use `conversion_table()` to generate the performance under different score thresholds (e.g., 20, 40, 60, 75).
```{r, message=FALSE, warning = FALSE}
conversion_table(pred_score = pred_score, by = "score", values = c(20,40,60,75))
```

-   the conversion table can also be visualized using an interactive figure.

```{r}
plot_predicted_risk(pred_score = pred_score, max_score = 100, 
                    final_variables = final_variables, 
                    scoring_table = scoring_table, point_size = 1)
```


- You can also generate the `pred_score_train` based on training data for further analysis (e.g., conversion table based on the training data).
```{r,fig.show="hide",fig.width=5,fig.height=5,warning = FALSE}
pred_score_train <-
  AutoScore_testing(
    test_set = train_set,
    final_variables = final_variables,
    cut_vec = cut_vec,
    scoring_table = scoring_table,
    threshold = "best",
    with_label = TRUE
  )
```

- Generate conversion table based on the training data based on predictive risk (i.e., probability of mortality based on logistic regression estimation) cut-off (e.g., 0.01, 0.05, 0.1, 0.2, 0.5) using `conversion_table()`
```{r,message=FALSE, warning=FALSE, error=FALSE}
conversion_table(pred_score = pred_score_train, by ="risk", values = c(0.01,0.05,0.1,0.2,0.5))
```

-   the conversion table can also be visualized using an interactive figure.
```{r}
plot_predicted_risk(pred_score = pred_score_train, max_score = 100, 
                    final_variables = final_variables, 
                    scoring_table = scoring_table, point_size = 1)
```

- Users could use the `pred_score` or `pred_score_train` for further analysis or export it as the CSV to other software (e.g., generating the calibration curve).
```{r csv_generate3, eval=FALSE}
write.csv(pred_score, file = "D:/pred_score.csv")
write.csv(pred_score_train, file = "D:/pred_score_train.csv")
```




## development on Small dataset (sample size = 1000) with cross-validation**

In Demo #2, we demonstrate the use of AutoScore on a comparably small dataset where there are no sufficient samples to form a separate training and validation datasets. Thus, the cross validation is employed to generate the parsimony plot.

### Get small dataset with 1000 samples
```{r}
data("sample_data_small")
```

### Prepare training and test datasets
- Option 1: Prepare two separate datasets to train and test models.
- Option 2: Use demo codes below to randomly split your dataset into training and test datasets (70% and 30%, respectively). For cross-validation, `train_set` is equal to `validation_set` and the ratio of `validation_set` should be 0. Then cross-validation will be implemented in the STEP(ii) `AutoScore_parsimony()`.
```{r}
set.seed(4)
out_split <- split_data(data = sample_data_small, ratio = c(0.7, 0, 0.3), cross_validation = TRUE)
train_set <- out_split$train_set
validation_set <- out_split$validation_set
test_set <- out_split$test_set
```


### STEP(i): Generate variable ranking list (AutoScore Module 1)
- `method`: "rf" (default) or "auc".

method = `auc`: "auc" refers to the AUC-based ranking. For "auc", univariate models will be built based on the train set, and the variable ranking is constructed via the AUC performance of corresponding univariate models on the validation set (`validation_set`).  
- `validation_set`: validation set required only if when `method` is "auc".
```{r}
ranking <- AutoScore_rank(train_set, validation_set = validation_set, ntree = 100)
```

method = `rf`: "rf" refers to random forest-based ranking  
- `ntree`: Number of trees required only if when `method` is "rf" (Default: 100).
```{r}
ranking <- AutoScore_rank(train_set, ntree = 100)
```

### STEP(ii): Select the best model with parsimony plot (AutoScore Modules 2+3+4)
- `nmin`: Minimum number of selected variables (Default: 1).
- `nmax`: Maximum number of selected variables (Default: 20).
- `categorize`: Methods for categorize continuous variables. Options include `"quantile"` or `"kmeans"` (Default: `"quantile"`).
- `quantiles`: Predefined quantiles to convert continuous variables to categorical ones. (Default: `c(0, 0.05, 0.2, 0.8, 0.95, 1)`) Available if `categorize = "quantile"`.
- `max_cluster`: The max number of cluster (Default: 5). Available if `categorize = "kmeans"`.
- `max_score` Maximum total score (Default: 100).
- `cross_validation` : `TRUE` if cross-validation is needed, especially for small datasets.
- `fold` The number of folds used in cross validation (Default: 10). Available if `cross_validation = TRUE`.
- `do_trace` If set to `TRUE`, all results based on each fold of cross-validation would be printed out and plotted (Default: `FALSE`). Available if `cross_validation = TRUE`.
```{r parsi, warning = FALSE}
AUC <- AutoScore_parsimony(
    train_set,
    validation_set,
    rank = ranking,
    max_score = 100,
    n_min = 1,
    n_max = 20,
    cross_validation = TRUE,
    categorize = "quantile",
    fold = 10,
    quantiles = c(0, 0.25, 0.5, 0.75, 1), #c(0, 0.05, 0.2, 0.8, 0.95, 1)
    do_trace = FALSE
  )

```


- Users could use the `AUC` for further analysis or export it as the CSV to other software for plotting.
```{r csv_generate_auc, eval=FALSE}
write.csv(data.frame(AUC), file = "D:/AUC.csv")
```


- Determine the optimal number of variables (`num_var`) based on the parsimony plot obtained in STEP(ii). 
- The final list of variables is the first `num_var` variables in the ranked list `ranking` obtained in STEP(i). 
- Optional: User can adjust the finally included variables `final_variables` based on the clinical preferences and knowledge).
```{r}
# Example 1: Top 6 variables are selected
num_var <- 6
final_variables <- names(ranking[1:num_var])

# Example 2: Top 9 variables are selected
num_var <- 9
final_variables <- names(ranking[1:num_var])

# Example 3: Top 6 variables, the 9th and 10th variable are selected
num_var <- 6
final_variables <- names(ranking[c(1:num_var, 9, 10)])
```

```{r finalvariab, results = "hide", warning=FALSE, message=FALSE,eval=TRUE,include=FALSE}
num_var <- 5
final_variables <- names(ranking[1:num_var])
```
### STEP(iii): Generate initial scores with the final list of variables (Re-run AutoScore Modules 2+3)
- Generate `cut_vec` with current cutoffs of continuous variables, which can be fine-tuned in STEP(iv).
```{r weighting2,fig.width=5,fig.height=5,warning = TRUE}
cut_vec <- AutoScore_weighting( 
    train_set,
    validation_set,
    final_variables,
    max_score = 100,
    categorize = "quantile",
    quantiles = c(0, 0.05, 0.2, 0.8, 0.95, 1)
  )

```

### STEP(iv): Fine-tune the initial score generated in STEP(iii) (AutoScore Module 5 & Re-run AutoScore Modules 2+3) 
- Revise `cut_vec` with domain knowledge to update the scoring table (AutoScore Module 5).
- Re-run AutoScore Modules 2+3 to generate the updated scores.
- Users can choose any cutoff values and/or any number of categories, but are suggested to choose numbers close to the automatically determined values.
```{r}
## For example, we have current cutoffs of continuous variable:
## ==============  ===========  =====
## variable        interval     point
## ==============  ===========  =====
#> Lab_K           <9             0  
#>                [9,43.2)       1  
#>                [43.2,59)      9  
#>                >=59          13  
```

- Current cutoffs: `c(9, 43.2, 59)`. We can fine tune the cutoffs as follows:
- Note: It is just a demo using simulated data, and thus, the result might not be clinically meaningful.
```{r}

# Example 1: rounding up to a nice number
cut_vec$Lab_K <- c(9, 45, 60)

# Example 2: changing cutoffs according to clinical knowledge or preference 
cut_vec$Lab_K <- c(15, 45, 60)

# Example 3: combining categories
cut_vec$Lab_K <- c(45, 60)

```

```{r, results = "hide", warning=FALSE, message=FALSE,eval=TRUE,include=FALSE}
cut_vec$Lab_K <- c(45, 60)
```
- Then we do similar checks for other variables and update scoring table using new cutoffs if needed.
```{r scoring2,fig.width=5,fig.height=5, warning = TRUE}
cut_vec$Lab_H <- c(1, 2, 3)
cut_vec$Age <- c(35, 50, 80)
cut_vec$Lab_B <- c(8, 12, 18)
cut_vec$Vital_E <- c(15, 22)
scoring_table <- AutoScore_fine_tuning(train_set,
                        validation_set,
                        final_variables,
                        cut_vec,
                        max_score = 100)

```

### STEP(v): Evaluate final risk scores on test dataset (AutoScore Module 6)
- `threshold`: Score threshold for the ROC analysis to generate sensitivity, specificity, etc. If set to `"best"`, the optimal threshold will be calculated (Default: `"best"`).
- `with_label`: Set to `TRUE` if there are labels in the `test_set` and performance will be evaluated accordingly (Default: `TRUE`).
- Set the `with_label` to `FALSE` if there are not `label` in the `test_set` and the final predicted scores will be the output without performance evaluation.
```{r,fig.width=5,fig.height=5}
pred_score <-
  AutoScore_testing(
    test_set,
    final_variables,
    cut_vec,
    scoring_table,
    threshold = "best",
    with_label = TRUE
  )
head(pred_score)
```

- Use `print_roc_performance()` to generate the performance under different score thresholds (e.g., 90).
- Note: It is just a demo using simulated data, and thus, the result might not be clinically meaningful.
```{r}
print_roc_performance(pred_score$Label, pred_score$pred_score, threshold = 90)
```


### STEP(vi): Further analysis based on the final scoring systems (e.g., conversion table, model calibration, output the score)
- You can also generate conversion table using `conversion_table()`. Please refer to our. But we skipped it here. 

- Users could use the `pred_score` for further analysis or export it as the CSV to other software.
```{r csv_generate, eval=FALSE}
write.csv(pred_score, file = "D:/pred_score.csv")
```


## development on dataset with missing values (informative missing)**

In Demo #3, we demonstrate the use of AutoScore on a dataset with missing values. AutoScore can automatically treat the missingness as a new category named `Unknown`. Please note that it is just a demo using simulated data, and thus, the result might not be clinically meaningful.

### load package and data
```{r, warning=FALSE}
library(AutoScore)
data("sample_data")
names(sample_data)[names(sample_data) == "Mortality_inpatient"] <- "label"
check_data(sample_data)
```
### Create informative missing
```{r}
sample_data <- AutoScore:::induce_informative_missing(sample_data, vars_to_induce = names(sample_data)[1:2], prop_missing = c(0.2, 0.6))
```

The following steps are similar to Demo #1. 

```{r, include=FALSE}
set.seed(4)
out_split <- split_data(data = sample_data, ratio = c(0.7, 0.1, 0.2))
train_set <- out_split$train_set
validation_set <- out_split$validation_set
test_set <- out_split$test_set

ranking <- AutoScore_rank(train_set, method = "rf", ntree = 100)
```

NOTE: 

- High missing rate may cause the variable ranking less reliable, and thus, caution is needed for variable selection using parsimony plot.

```{r, results = "hide"}
AUC <- AutoScore_parsimony(
    train_set,
    validation_set,
    rank = ranking,
    max_score = 100,
    n_min = 1,
    n_max = 20,
    categorize = "quantile",
    quantiles = c(0, 0.05, 0.2, 0.8, 0.95, 1),
    auc_lim_min = 0.5,
    auc_lim_max = "adaptive"
  )

```

```{r, results = "hide", warning=TRUE, message=FALSE,eval=TRUE,include=FALSE}
num_var <- 6
final_variables <- names(ranking[1:num_var])

cut_vec <- AutoScore_weighting( 
    train_set,
    validation_set,
    final_variables,
    max_score = 100,
    categorize = "quantile",
    quantiles = c(0, 0.05, 0.2, 0.8, 0.95, 1)
  )

cut_vec$Age <- c(50, 75, 90)
cut_vec$Lab_H <- c(0.2, 1, 3, 4)
cut_vec$Lab_K <- c(10, 40)
cut_vec$Lab_B <- c(10, 17)
cut_vec$hVital_A<- c(70, 98)
```

- The `Unknown` category indicating the missingness will be displayed in the final scoring table.
```{r echo=FALSE,fig.width=5,fig.height=5}
scoring_table <- AutoScore_fine_tuning(train_set,
                        validation_set,
                        final_variables,
                        cut_vec,
                        max_score = 100)
```


```{r,fig.width=5,fig.height=5,warning = FALSE, include=FALSE}
pred_score <-
  AutoScore_testing(
    test_set,
    final_variables,
    cut_vec,
    scoring_table,
    threshold = "best",
    with_label = TRUE
  )
head(pred_score)
print_roc_performance(pred_score$Label, pred_score$pred_score, threshold = 50)
```
