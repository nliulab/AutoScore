[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AutoScore: An Interpretable Machine Learning-Based Automatic Clinical Score Generator",
    "section": "",
    "text": "AutoScore is a novel machine learning framework to automate the development of interpretable clinical scoring models. AutoScore consists of six modules: 1) variable ranking with machine learning, 2) variable transformation, 3) score derivation, 4) model selection, 5) domain knowledge-based score fine-tuning, and 6) performance evaluation. The original AutoScore structure is elaborated in the article (http://dx.doi.org/10.2196/21798) and its flowchart is shown in the following figure. AutoScore was originally designed for binary outcomes and later extended to survival outcomes (http://dx.doi.org/10.1016/j.jbi.2021.103959) and ordinal outcomes (https://doi.org/10.48550/arxiv.2202.08407). AutoScore could seamlessly generate risk scores using a parsimonious set of variables for different types of clinical outcomes, which can be easily implemented and validated in clinical practice. Moreover, it enables users to build transparent and interpretable clinical scores quickly in a straightforward manner.\n\n\nThe five pipeline functions constitute the 5-step AutoScore-based process for generating point-based clinical scores for binary (Chapter 5), survival (Chapter 6) and ordinal outcomes (Chapter 7).\nThis 5-step process gives users the flexibility of customization (e.g., determining the final list of variables according to the parsimony plot, and fine-tuning the cutoffs in variable transformation). Please follow the step-by-step instructions (in following Demos) to build your own scores.\n\nSTEP(i): AutoScore_rank()or AutoScore_rank_Survival() or AutoScore_rank_Ordinal() - Rank variables with machine learning (AutoScore Module 1)\nSTEP(ii): AutoScore_parsimony() or AutoScore_parsimony_Survival() or AutoScore_parsimony_Ordinal() - Select the best model with parsimony plot (AutoScore Modules 2+3+4)\nSTEP(iii): AutoScore_weighting() or AutoScore_weighting_Survival() or AutoScore_weighting_Ordinal() - Generate the initial score with the final list of variables (Re-run AutoScore Modules 2+3)\nSTEP(iv): AutoScore_fine_tuning() or AutoScore_fine_tuning_Survival() or AutoScore_fine_tuning_Ordinal() - Fine-tune the score by revising cut_vec with domain knowledge (AutoScore Module 5)\nSTEP(v): AutoScore_testing() or AutoScore_testing_Survival() or AutoScore_testing_Ordinal() - Evaluate the final score with ROC analysis (AutoScore Module 6)\n\nWe also include several optional functions in the package, which could help with data analysis and result reporting. As demonstrated in Chapter 3, these functions are compute_descriptive_table() for generating the table of descriptive analysis for your dataset, compute_uni_variable_table() or compute_uni_variable_table_survival() or compute_uni_variable_table_ordinal() for creating the table of univariable analysis for your dataset, and compute_multi_variable_table() for generating the table of multivariable analysis for your dataset.\n\n\n\nXie F, Chakraborty B, Ong MEH, Goldstein BA, Liu N. AutoScore: A Machine Learning-Based Automatic Clinical Score Generator and Its Application to Mortality Prediction Using Electronic Health Records. JMIR Medical Informatics 2020;8(10):e21798 (http://dx.doi.org/10.2196/21798)\nXie F, Ning Y, Yuan H, et al. AutoScore-Survival: Developing interpretable machine learning-based time-to-event scores with right-censored survival data. J Biomed Inform. 2022;125:103959. (http://dx.doi.org/10.1016/j.jbi.2021.103959)\nSaffari SE, Ning Y, Feng X, Chakraborty B, Volovici V, Vaughan R, Ong ME, Liu N, AutoScore-Ordinal: An interpretable machine learning framework for generating scoring models for ordinal outcomes, arXiv:2202.08407 (https://doi.org/10.48550/arxiv.2202.08407)\nYilin Ning, Siqi Li, Marcus Eng Hock Ong, Feng Xie, Bibhas Chakraborty, Daniel Shu Wei Ting, Nan Liu, A novel interpretable machine learning system to generate clinical risk scores: An application for predicting early mortality or unplanned readmission in a retrospective cohort study, PLOS Digit Health 1(6): e0000062 (https://doi.org/10.1371/journal.pdig.0000062).\n\n\n\n\nFeng Xie (Email: xief@u.duke.nus.edu)\nYilin Ning (Email: yilin.ning@duke-nus.edu.sg)\nNan Liu (Email: liu.nan@duke-nus.edu.sg)"
  },
  {
    "objectID": "01-related_papers.html",
    "href": "01-related_papers.html",
    "title": "1  Related Published papers",
    "section": "",
    "text": "Original Paper:\nXie F, Chakraborty B, Ong MEH, Goldstein BA, Liu N. AutoScore: A Machine Learning-Based Automatic Clinical Score Generator and Its Application to Mortality Prediction Using Electronic Health Records. JMIR Medical Informatics 2020;8(10):e21798 (http://dx.doi.org/10.2196/21798)\nExtension to Survival outcomes:\nXie F, Ning Y, Yuan H, et al. AutoScore-Survival: Developing interpretable machine learning-based time-to-event scores with right-censored survival data. J Biomed Inform. 2022;125:103959. (http://dx.doi.org/10.1016/j.jbi.2021.103959)\nExtension to ordinal outcomes:\nSaffari SE, Ning Y, Feng X, Chakraborty B, Volovici V, Vaughan R, Ong ME, Liu N, AutoScore-Ordinal: An interpretable machine learning framework for generating scoring models for ordinal outcomes, arXiv:2202.08407 (https://doi.org/10.48550/arxiv.2202.08407)\nExtension to unbalanced data:\nHan Yuan, Feng Xie, Marcus Eng Hock Ong, Yilin Ning, Marcel Lucas Chee, Seyed Ehsan Saffari, Hairil Rizal Abdullah, Benjamin Alan Goldstein, Bibhas Chakraborty, Nan Liu, AutoScore-Imbalance: An interpretable machine learning tool for development of clinical scores with rare events data, Journal of Biomedical Informatics,Volume 129,2022,104072,ISSN 1532-0464 (https://doi.org/10.1016/j.jbi.2022.104072)\nIntegrated AutoScore-ShapleyVIC framework for robust and interpretable variable ranking:\nNing Y, Li S, Ong ME, Xie F, Chakraborty B, Ting DS, Liu N. A novel interpretable machine learning system to generate clinical risk scores: An application for predicting early mortality or unplanned readmission in a retrospective cohort study. PLOS Digit Health 1(6): e0000062. (https://doi.org/10.1371/journal.pdig.0000062)"
  },
  {
    "objectID": "01-related_papers.html#autoscore-clinical-applications",
    "href": "01-related_papers.html#autoscore-clinical-applications",
    "title": "1  Related Published papers",
    "section": "1.2 AutoScore Clinical Applications",
    "text": "1.2 AutoScore Clinical Applications\nA collection of clinical applications using AutoScore and its extensions can be found on this page. The list is categorized according to medical specialties and is updated regularly. However, due to the manual process of updating, we are unable to keep track of all publications.\n\nEmergency Medicine\nNeurology\nOut-of-Hospital Cardiac Arrest\nRenal Medicine\n\n\n1.2.1 Emergency Medicine\n\nXie F, et al. Development and validation of an interpretable machine learning scoring tool for estimating time to emergency readmissions. eClinicalMedicine 2022 Mar; 45: 101315.\nXie F, et al. Development and assessment of an interpretable machine learning triage tool for estimating mortality after emergency admissions. JAMA Network Open 2021 Aug; 4(8): e2118467.\n\n\n\n1.2.2 Neurology\n\nPetersen KK, et al. Predicting Amyloid Positivity in Cognitively Unimpaired Older Adults: A Machine Learning Approach Using the A4 Data. Neurology 2022 Apr.\n\n\n\n1.2.3 Out-of-Hospital Cardiac Arrest\n\nLiu N, et al. Development and validation of interpretable prehospital return of spontaneous circulation (P-ROSC) score for out-of-hospital cardiac arrest patients using machine learning. eClinicalMedicine 2022 Jun; 48: 101422.\nWong XY, et al. Development and validation of the SARICA score to predict survival after return of spontaneous circulation in out of hospital cardiac arrest using an interpretable machine learning framework. Resuscitation 2022 Jan; 170: 126-133.\n\n\n\n1.2.4 Renal Medicine\n\nAng Y, et al. Development and validation of an interpretable clinical score for early identification of acute kidney injury at the emergency department. Scientific Reports 2022 May; 12: 7111."
  },
  {
    "objectID": "011Installation.html#install-the-development-version-from-github-or-the-stable-version-from-cran-recommended",
    "href": "011Installation.html#install-the-development-version-from-github-or-the-stable-version-from-cran-recommended",
    "title": "2  Package Installation and import",
    "section": "2.2 Install the development version from GitHub or the stable version from CRAN (recommended):",
    "text": "2.2 Install the development version from GitHub or the stable version from CRAN (recommended):\n\n# From Github\ninstall.packages(\"devtools\")\nlibrary(devtools)\ninstall_github(repo = \"nliulab/AutoScore\", build_vignettes = TRUE)\n\n# From CRAN (recommended)\ninstall.packages(\"AutoScore\")"
  },
  {
    "objectID": "011Installation.html#load-r-package",
    "href": "011Installation.html#load-r-package",
    "title": "2  Package Installation and import",
    "section": "2.3 Load R package",
    "text": "2.3 Load R package\n\nlibrary(AutoScore)"
  },
  {
    "objectID": "02-desc_analysis.html",
    "href": "02-desc_analysis.html",
    "title": "3  Descriptive analysis",
    "section": "",
    "text": "Before building AutoScore models ,users can utilize our package and codes below to do descriptive analysis (E.g., univariate analysis, multivariate analysis) for data with binary, survival, or ordinal outcomes"
  },
  {
    "objectID": "02-desc_analysis.html#binary-outcome",
    "href": "02-desc_analysis.html#binary-outcome",
    "title": "3  Descriptive analysis",
    "section": "3.1 Binary Outcome",
    "text": "3.1 Binary Outcome\n\nCompute descriptive table (usually Table 1 in medical literature) for the dataset.\n\n\nlibrary(AutoScore)\nlibrary(knitr)\ndata(\"sample_data\")\nnames(sample_data)[names(sample_data) == \"Mortality_inpatient\"] <- \"label\"\ncompute_descriptive_table(sample_data)\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverall\nFALSE\nTRUE\np\ntest\n\n\n\n\nn\n20000\n18412\n1588\n\n\n\n\nVital_A (mean (SD))\n85.40 (15.23)\n84.81 (15.11)\n92.25 (14.98)\n<0.001\n\n\n\nVital_B (mean (SD))\n119.13 (16.72)\n119.57 (16.67)\n114.01 (16.49)\n<0.001\n\n\n\nVital_C (mean (SD))\n61.15 (10.81)\n61.50 (10.77)\n57.13 (10.48)\n<0.001\n\n\n\nVital_D (mean (SD))\n78.41 (11.14)\n78.74 (11.10)\n74.60 (10.95)\n<0.001\n\n\n\nVital_E (mean (SD))\n18.57 (3.92)\n18.35 (3.86)\n21.13 (3.74)\n<0.001\n\n\n\nVital_F (mean (SD))\n36.84 (0.59)\n36.84 (0.59)\n36.79 (0.57)\n0.001\n\n\n\nVital_G (mean (SD))\n97.17 (1.98)\n97.20 (1.97)\n96.76 (2.03)\n<0.001\n\n\n\nLab_A (mean (SD))\n138.24 (41.69)\n137.36 (41.73)\n148.42 (39.80)\n<0.001\n\n\n\nLab_B (mean (SD))\n14.12 (3.44)\n13.94 (3.39)\n16.23 (3.31)\n<0.001\n\n\n\nLab_C (mean (SD))\n24.04 (4.34)\n24.18 (4.31)\n22.47 (4.39)\n<0.001\n\n\n\nLab_D (mean (SD))\n1.55 (1.30)\n1.52 (1.29)\n1.86 (1.35)\n<0.001\n\n\n\nLab_E (mean (SD))\n104.56 (5.54)\n104.62 (5.53)\n103.90 (5.52)\n<0.001\n\n\n\nLab_F (mean (SD))\n32.80 (5.53)\n32.94 (5.51)\n31.11 (5.47)\n<0.001\n\n\n\nLab_G (mean (SD))\n11.04 (1.97)\n11.11 (1.96)\n10.30 (1.97)\n<0.001\n\n\n\nLab_H (mean (SD))\n2.09 (1.14)\n2.03 (1.12)\n2.82 (1.15)\n<0.001\n\n\n\nLab_I (mean (SD))\n229.36 (113.73)\n230.81 (113.62)\n212.51 (113.65)\n<0.001\n\n\n\nLab_J (mean (SD))\n4.23 (0.62)\n4.22 (0.61)\n4.28 (0.63)\n<0.001\n\n\n\nLab_K (mean (SD))\n25.92 (18.29)\n24.85 (17.88)\n38.34 (18.52)\n<0.001\n\n\n\nLab_L (mean (SD))\n138.27 (4.25)\n138.29 (4.24)\n138.07 (4.39)\n0.051\n\n\n\nLab_M (mean (SD))\n12.28 (8.27)\n12.04 (8.21)\n14.98 (8.60)\n<0.001\n\n\n\nAge (mean (SD))\n62.46 (16.29)\n61.62 (16.12)\n72.21 (15.12)\n<0.001\n\n\n\nlabel = TRUE (%)\n1588 (7.9)\n0 (0.0)\n1588 (100.0)\n<0.001\n\n\n\n\n\n\n\nPerform univariable analysis and generate the result table with odd ratios.\n\n\nuni_table <- compute_uni_variable_table(sample_data)\nkable(uni_table)\n\n\n\n\n\nOR\np value\n\n\n\n\nVital_A\n1.033(1.03-1.037)\n<0.001\n\n\nVital_B\n0.98(0.977-0.983)\n<0.001\n\n\nVital_C\n0.963(0.958-0.968)\n<0.001\n\n\nVital_D\n0.967(0.963-0.972)\n<0.001\n\n\nVital_E\n1.209(1.192-1.226)\n<0.001\n\n\nVital_F\n0.867(0.794-0.946)\n0.001\n\n\nVital_G\n0.897(0.875-0.92)\n<0.001\n\n\nLab_A\n1.006(1.005-1.008)\n<0.001\n\n\nLab_B\n1.222(1.202-1.241)\n<0.001\n\n\nLab_C\n0.912(0.902-0.923)\n<0.001\n\n\nLab_D\n1.208(1.163-1.254)\n<0.001\n\n\nLab_E\n0.977(0.968-0.986)\n<0.001\n\n\nLab_F\n0.942(0.933-0.95)\n<0.001\n\n\nLab_G\n0.81(0.788-0.831)\n<0.001\n\n\nLab_H\n1.815(1.734-1.9)\n<0.001\n\n\nLab_I\n0.999(0.998-0.999)\n<0.001\n\n\nLab_J\n1.176(1.082-1.278)\n<0.001\n\n\nLab_K\n1.039(1.036-1.042)\n<0.001\n\n\nLab_L\n0.988(0.976-1)\n0.051\n\n\nLab_M\n1.042(1.036-1.048)\n<0.001\n\n\nAge\n1.042(1.039-1.046)\n<0.001\n\n\n\n\n\n\nPerform multivariable analysis and generate the result table with adjusted odd ratios.\n\n\nmulti_table <- compute_multi_variable_table(sample_data)\nkable(multi_table)\n\n\n\n\n\nadjusted_OR\np value\n\n\n\n\nVital_A\n1.032(1.027-1.037)\n<0.001\n\n\nVital_B\n0.976(0.97-0.983)\n<0.001\n\n\nVital_C\n0.958(0.945-0.971)\n<0.001\n\n\nVital_D\n1.049(1.031-1.067)\n<0.001\n\n\nVital_E\n1.153(1.133-1.173)\n<0.001\n\n\nVital_F\n0.844(0.757-0.942)\n0.002\n\n\nVital_G\n0.995(0.965-1.027)\n0.774\n\n\nLab_A\n1.001(1-1.002)\n0.184\n\n\nLab_B\n1.111(1.067-1.156)\n<0.001\n\n\nLab_C\n0.946(0.912-0.981)\n0.003\n\n\nLab_D\n0.774(0.728-0.823)\n<0.001\n\n\nLab_E\n0.928(0.897-0.96)\n<0.001\n\n\nLab_F\n1.122(1.081-1.165)\n<0.001\n\n\nLab_G\n0.636(0.572-0.707)\n<0.001\n\n\nLab_H\n1.615(1.526-1.71)\n<0.001\n\n\nLab_I\n0.997(0.997-0.998)\n<0.001\n\n\nLab_J\n0.73(0.654-0.814)\n<0.001\n\n\nLab_K\n1.033(1.029-1.038)\n<0.001\n\n\nLab_L\n1.04(1.005-1.077)\n0.026\n\n\nLab_M\n1.029(1.022-1.037)\n<0.001\n\n\nAge\n1.047(1.042-1.051)\n<0.001"
  },
  {
    "objectID": "02-desc_analysis.html#survival-outcome",
    "href": "02-desc_analysis.html#survival-outcome",
    "title": "3  Descriptive analysis",
    "section": "3.2 Survival Outcome",
    "text": "3.2 Survival Outcome\n\nCompute descriptive table (usually Table 1 in medical literature) for the data with survival outcome\n\n\ndata(\"sample_data_survival\")\ncompute_descriptive_table(sample_data_survival)\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverall\nFALSE\nTRUE\np\ntest\n\n\n\n\nn\n20000\n5350\n14650\n\n\n\n\nVital_A (mean (SD))\n85.40 (15.23)\n80.14 (14.64)\n87.33 (14.99)\n<0.001\n\n\n\nVital_B (mean (SD))\n119.13 (16.72)\n123.03 (16.45)\n117.70 (16.59)\n<0.001\n\n\n\nVital_C (mean (SD))\n61.15 (10.81)\n64.47 (10.54)\n59.94 (10.66)\n<0.001\n\n\n\nVital_D (mean (SD))\n78.41 (11.14)\n81.41 (10.87)\n77.31 (11.04)\n<0.001\n\n\n\nVital_E (mean (SD))\n18.57 (3.92)\n16.46 (3.64)\n19.35 (3.74)\n<0.001\n\n\n\nVital_F (mean (SD))\n36.84 (0.59)\n36.88 (0.59)\n36.82 (0.59)\n<0.001\n\n\n\nVital_G (mean (SD))\n97.17 (1.98)\n97.49 (1.92)\n97.05 (1.99)\n<0.001\n\n\n\nLab_A (mean (SD))\n138.24 (41.69)\n129.87 (41.42)\n141.29 (41.36)\n<0.001\n\n\n\nLab_B (mean (SD))\n14.12 (3.44)\n12.40 (3.25)\n14.75 (3.29)\n<0.001\n\n\n\nLab_C (mean (SD))\n24.04 (4.34)\n25.25 (4.22)\n23.60 (4.29)\n<0.001\n\n\n\nLab_D (mean (SD))\n1.55 (1.30)\n1.30 (1.22)\n1.64 (1.32)\n<0.001\n\n\n\nLab_E (mean (SD))\n104.56 (5.54)\n105.11 (5.59)\n104.36 (5.50)\n<0.001\n\n\n\nLab_F (mean (SD))\n32.80 (5.53)\n34.05 (5.43)\n32.34 (5.50)\n<0.001\n\n\n\nLab_G (mean (SD))\n11.04 (1.97)\n11.62 (1.93)\n10.83 (1.95)\n<0.001\n\n\n\nLab_H (mean (SD))\n2.09 (1.14)\n1.52 (1.00)\n2.30 (1.12)\n<0.001\n\n\n\nLab_I (mean (SD))\n229.36 (113.73)\n244.73 (114.08)\n223.74 (113.08)\n<0.001\n\n\n\nLab_J (mean (SD))\n4.23 (0.62)\n4.19 (0.62)\n4.24 (0.61)\n<0.001\n\n\n\nLab_K (mean (SD))\n25.92 (18.29)\n16.56 (15.10)\n29.34 (18.17)\n<0.001\n\n\n\nLab_L (mean (SD))\n138.27 (4.25)\n138.40 (4.29)\n138.22 (4.23)\n0.008\n\n\n\nLab_M (mean (SD))\n12.28 (8.27)\n10.27 (7.79)\n13.01 (8.32)\n<0.001\n\n\n\nAge (mean (SD))\n62.46 (16.29)\n54.06 (15.27)\n65.53 (15.56)\n<0.001\n\n\n\nlabel_status = TRUE (%)\n14650 (73.2)\n0 (0.0)\n14650 (100.0)\n<0.001\n\n\n\nlabel_time (mean (SD))\n70.03 (19.27)\n91.00 (0.00)\n62.37 (16.97)\n<0.001\n\n\n\n\n\n\n\nPerform univariable analysis and generate the result table with hazard ratios.\n\n\nuni_table_survival <- compute_uni_variable_table_survival(sample_data_survival)\nkable(uni_table_survival)\n\n\n\n\n\nOR\np value\n\n\n\n\nVital_A\n1.021(1.02-1.022)\n<0.001\n\n\nVital_B\n0.988(0.987-0.989)\n<0.001\n\n\nVital_C\n0.976(0.974-0.977)\n<0.001\n\n\nVital_D\n0.979(0.978-0.98)\n<0.001\n\n\nVital_E\n1.139(1.134-1.144)\n<0.001\n\n\nVital_F\n0.904(0.879-0.929)\n<0.001\n\n\nVital_G\n0.932(0.924-0.939)\n<0.001\n\n\nLab_A\n1.004(1.004-1.005)\n<0.001\n\n\nLab_B\n1.145(1.139-1.15)\n<0.001\n\n\nLab_C\n0.945(0.942-0.949)\n<0.001\n\n\nLab_D\n1.136(1.122-1.15)\n<0.001\n\n\nLab_E\n0.984(0.981-0.987)\n<0.001\n\n\nLab_F\n0.964(0.961-0.967)\n<0.001\n\n\nLab_G\n0.876(0.868-0.883)\n<0.001\n\n\nLab_H\n1.518(1.496-1.541)\n<0.001\n\n\nLab_I\n0.999(0.999-0.999)\n<0.001\n\n\nLab_J\n1.088(1.059-1.117)\n<0.001\n\n\nLab_K\n1.027(1.026-1.028)\n<0.001\n\n\nLab_L\n0.993(0.989-0.997)\n<0.001\n\n\nLab_M\n1.027(1.025-1.029)\n<0.001\n\n\nAge\n1.03(1.029-1.031)\n<0.001\n\n\n\n\n\n\nPerform multivariable analysis and generate the result table with adjusted hazard ratios.\n\n\nmulti_table_survival <- compute_multi_variable_table_survival(sample_data_survival)\nkable(multi_table_survival)\n\n\n\n\n\nadjusted_OR\np value\n\n\n\n\nVital_A\n1.031(1.03-1.032)\n<0.001\n\n\nVital_B\n0.975(0.973-0.977)\n<0.001\n\n\nVital_C\n0.955(0.951-0.959)\n<0.001\n\n\nVital_D\n1.053(1.048-1.058)\n<0.001\n\n\nVital_E\n1.155(1.149-1.16)\n<0.001\n\n\nVital_F\n0.84(0.815-0.866)\n<0.001\n\n\nVital_G\n0.99(0.981-0.999)\n0.022\n\n\nLab_A\n1.001(1-1.001)\n<0.001\n\n\nLab_B\n1.115(1.103-1.127)\n<0.001\n\n\nLab_C\n0.952(0.943-0.962)\n<0.001\n\n\nLab_D\n0.778(0.765-0.792)\n<0.001\n\n\nLab_E\n0.934(0.925-0.943)\n<0.001\n\n\nLab_F\n1.143(1.131-1.155)\n<0.001\n\n\nLab_G\n0.603(0.586-0.621)\n<0.001\n\n\nLab_H\n1.614(1.588-1.64)\n<0.001\n\n\nLab_I\n0.997(0.997-0.997)\n<0.001\n\n\nLab_J\n0.737(0.715-0.76)\n<0.001\n\n\nLab_K\n1.032(1.031-1.034)\n<0.001\n\n\nLab_L\n1.034(1.024-1.044)\n<0.001\n\n\nLab_M\n1.03(1.028-1.032)\n<0.001\n\n\nAge\n1.046(1.045-1.047)\n<0.001"
  },
  {
    "objectID": "02-desc_analysis.html#ordinal",
    "href": "02-desc_analysis.html#ordinal",
    "title": "3  Descriptive analysis",
    "section": "3.3 Ordinal Outcome",
    "text": "3.3 Ordinal Outcome\n\nCompute descriptive table (usually Table 1 in medical literature) for the dataset.\n\n\ndata(\"sample_data_ordinal\")\ncompute_descriptive_table(sample_data_ordinal)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverall\n1\n2\n3\np\ntest\n\n\n\n\nn\n20000\n16360\n2449\n1191\n\n\n\n\nlabel (%)\n\n\n\n\n<0.001\n\n\n\n1\n16360 (81.8)\n16360 (100.0)\n0 ( 0.0)\n0 ( 0.0)\n\n\n\n\n2\n2449 (12.2)\n0 ( 0.0)\n2449 (100.0)\n0 ( 0.0)\n\n\n\n\n3\n1191 ( 6.0)\n0 ( 0.0)\n0 ( 0.0)\n1191 (100.0)\n\n\n\n\nAge (mean (SD))\n61.68 (18.19)\n60.64 (18.37)\n65.37 (16.64)\n68.32 (16.18)\n<0.001\n\n\n\nGender = MALE (%)\n9863 (49.3)\n8109 ( 49.6)\n1173 ( 47.9)\n581 ( 48.8)\n0.284\n\n\n\nUtil_A (%)\n\n\n\n\n0.626\n\n\n\nP1\n3750 (18.8)\n3082 ( 18.8)\n437 ( 17.8)\n231 ( 19.4)\n\n\n\n\nP2\n11307 (56.5)\n9218 ( 56.3)\n1413 ( 57.7)\n676 ( 56.8)\n\n\n\n\nP3 and P4\n4943 (24.7)\n4060 ( 24.8)\n599 ( 24.5)\n284 ( 23.8)\n\n\n\n\nUtil_B (mean (SD))\n0.93 (2.20)\n0.78 (1.98)\n1.40 (2.73)\n1.96 (3.18)\n<0.001\n\n\n\nUtil_C (mean (SD))\n3.54 (8.73)\n3.55 (8.77)\n3.38 (7.83)\n3.60 (9.90)\n0.632\n\n\n\nUtil_D (mean (SD))\n2.76 (1.70)\n2.80 (1.71)\n2.66 (1.69)\n2.49 (1.63)\n<0.001\n\n\n\nComorb_A = 1 (%)\n1555 ( 7.8)\n888 ( 5.4)\n348 ( 14.2)\n319 ( 26.8)\n<0.001\n\n\n\nComorb_B = 1 (%)\n2599 (13.0)\n2094 ( 12.8)\n336 ( 13.7)\n169 ( 14.2)\n0.202\n\n\n\nComorb_C = 1 (%)\n526 ( 2.6)\n414 ( 2.5)\n70 ( 2.9)\n42 ( 3.5)\n0.088\n\n\n\nComorb_D = 1 (%)\n1887 ( 9.4)\n1538 ( 9.4)\n242 ( 9.9)\n107 ( 9.0)\n0.645\n\n\n\nComorb_E = 1 (%)\n310 ( 1.6)\n253 ( 1.5)\n43 ( 1.8)\n14 ( 1.2)\n0.411\n\n\n\nLab_A (mean (SD))\n146.85 (199.74)\n143.75 (198.13)\n153.25 (196.98)\n176.29 (223.44)\n<0.001\n\n\n\nLab_B (mean (SD))\n4.15 (0.68)\n4.15 (0.68)\n4.17 (0.69)\n4.14 (0.66)\n0.326\n\n\n\nLab_C (mean (SD))\n135.15 (4.81)\n135.16 (4.81)\n135.06 (4.92)\n135.18 (4.47)\n0.607\n\n\n\nVital_A (mean (SD))\n82.67 (17.10)\n82.11 (16.78)\n84.45 (18.40)\n86.65 (17.92)\n<0.001\n\n\n\nVital_B (mean (SD))\n17.86 (1.82)\n17.86 (1.81)\n17.86 (1.88)\n17.86 (1.84)\n0.995\n\n\n\nVital_C (mean (SD))\n97.96 (3.26)\n97.97 (3.06)\n97.92 (4.07)\n97.93 (3.91)\n0.741\n\n\n\nVital_D (mean (SD))\n71.23 (13.51)\n71.21 (13.48)\n71.35 (13.70)\n71.30 (13.49)\n0.877\n\n\n\nVital_E (mean (SD))\n133.47 (25.27)\n134.13 (25.15)\n130.87 (25.45)\n129.74 (25.91)\n<0.001\n\n\n\nVital_F (mean (SD))\n22.82 (3.53)\n22.86 (3.46)\n22.73 (3.76)\n22.36 (3.94)\n<0.001\n\n\n\n\n\n\n\nPerform univariable analysis and generate the result table.\nBy default the odds ratio from the commonly used proportional odds models are reported for link = \"logit\". If other link functions are selected (i.e., \"cloglog\" link corresponding to the proportional hazards model, or the \"probit\" link), the exponentiated coefficients are reported.\n\n\nuni_table_ordinal <- compute_uni_variable_table_ordinal(sample_data_ordinal)\nkable(uni_table_ordinal)\n\n\n\n\n\nOR\np value\n\n\n\n\nAge\n1.019 (1.017 - 1.021)\n<0.001\n\n\nGenderMALE\n0.948 (0.883 - 1.019)\n0.147\n\n\nUtil_AP2\n1.040 (0.946 - 1.145)\n0.421\n\n\nUtil_AP3 and P4\n0.998 (0.894 - 1.115)\n0.974\n\n\nUtil_B\n1.136 (1.121 - 1.153)\n<0.001\n\n\nUtil_C\n0.999 (0.994 - 1.003)\n0.564\n\n\nUtil_D\n0.929 (0.908 - 0.950)\n<0.001\n\n\nComorb_A1\n4.154 (3.737 - 4.616)\n<0.001\n\n\nComorb_B1\n1.099 (0.989 - 1.218)\n0.076\n\n\nComorb_C1\n1.236 (0.997 - 1.520)\n0.049\n\n\nComorb_D1\n1.017 (0.899 - 1.147)\n0.791\n\n\nComorb_E1\n0.994 (0.739 - 1.315)\n0.969\n\n\nLab_A\n1.000 (1.000 - 1.001)\n<0.001\n\n\nLab_B\n1.010 (0.958 - 1.064)\n0.717\n\n\nLab_C\n0.998 (0.990 - 1.005)\n0.534\n\n\nVital_A\n1.010 (1.008 - 1.012)\n<0.001\n\n\nVital_B\n0.999 (0.980 - 1.019)\n0.956\n\n\nVital_C\n0.996 (0.986 - 1.007)\n0.451\n\n\nVital_D\n1.001 (0.998 - 1.003)\n0.622\n\n\nVital_E\n0.994 (0.993 - 0.996)\n<0.001\n\n\nVital_F\n0.979 (0.969 - 0.989)\n<0.001\n\n\n\n\n\n\nPerform multivariable analysis and generate the result table, with adjusted odd ratios from a proportional odds model by default.\n\n\nmulti_table_ordinal <- compute_multi_variable_table_ordinal(sample_data_ordinal)\nkable(multi_table_ordinal)\n\n\n\n\n\nOR\np value\n\n\n\n\nAge\n1.020 (1.018 - 1.023)\n<0.001\n\n\nGenderMALE\n0.944 (0.876 - 1.017)\n0.128\n\n\nUtil_AP2\n1.020 (0.924 - 1.127)\n0.695\n\n\nUtil_AP3 and P4\n0.970 (0.865 - 1.088)\n0.603\n\n\nUtil_B\n1.144 (1.127 - 1.160)\n<0.001\n\n\nUtil_C\n0.998 (0.994 - 1.002)\n0.390\n\n\nUtil_D\n0.924 (0.902 - 0.945)\n<0.001\n\n\nComorb_A1\n4.497 (4.034 - 5.011)\n<0.001\n\n\nComorb_B1\n1.057 (0.948 - 1.177)\n0.317\n\n\nComorb_C1\n1.287 (1.032 - 1.593)\n0.023\n\n\nComorb_D1\n1.039 (0.915 - 1.177)\n0.552\n\n\nComorb_E1\n0.963 (0.706 - 1.291)\n0.808\n\n\nLab_A\n1.000 (1.000 - 1.001)\n<0.001\n\n\nLab_B\n1.007 (0.953 - 1.063)\n0.808\n\n\nLab_C\n0.995 (0.988 - 1.003)\n0.234\n\n\nVital_A\n1.011 (1.009 - 1.013)\n<0.001\n\n\nVital_B\n1.004 (0.984 - 1.024)\n0.708\n\n\nVital_C\n0.995 (0.985 - 1.007)\n0.388\n\n\nVital_D\n1.001 (0.998 - 1.003)\n0.654\n\n\nVital_E\n0.993 (0.992 - 0.995)\n<0.001\n\n\nVital_F\n0.978 (0.968 - 0.988)\n<0.001"
  },
  {
    "objectID": "03-data_processing.html",
    "href": "03-data_processing.html",
    "title": "4  Data processing and checking",
    "section": "",
    "text": "Read data from CSV or Excel files.\nFor this demo, use the integrated data samples in the package.\nsample_data has 20000 simulated samples with binary outcomes, with the same distribution as the data in the MIMIC-III ICU database (https://mimic.mit.edu/).\nsample_data_survival has 20000 simulated samples with survival outcomes, which are also from MIMIC-III ICU database.\nsample_data_ordinal has 20000 simulated samples with a 3-category ordinal outcome, based on emergency department data from a tertiary hospital.\n\n\nlibrary(AutoScore)\ndata(\"sample_data\")"
  },
  {
    "objectID": "03-data_processing.html#data-checking-and-preprocessing",
    "href": "03-data_processing.html#data-checking-and-preprocessing",
    "title": "4  Data processing and checking",
    "section": "4.2 Data checking and preprocessing",
    "text": "4.2 Data checking and preprocessing\n\n4.2.1 Check outcome\nEnsure that there are dependent variable (outcome). Binary or ordinal outcomes should be named as “label”. Survival outcoms should be named as “label_time” and “label_status”. (Can use the codes below to do it).\n\nFor binary and ordinal outcomes: Change the name of outcome to “label” (make sure no variables using the same name).\nFor survival outcomes: Change the name of outcome to “label_time” and “label_status”\nYou may use the following code to revise or check the names of outcomes\n\n\nnames(sample_data)[names(sample_data) == \"Mortality_inpatient\"] <- \"label\"\n\n# Sample data with ordinal outcome already has appropriate variable names:\nnames(sample_data_ordinal)\n\n [1] \"label\"    \"Age\"      \"Gender\"   \"Util_A\"   \"Util_B\"   \"Util_C\"  \n [7] \"Util_D\"   \"Comorb_A\" \"Comorb_B\" \"Comorb_C\" \"Comorb_D\" \"Comorb_E\"\n[13] \"Lab_A\"    \"Lab_B\"    \"Lab_C\"    \"Vital_A\"  \"Vital_B\"  \"Vital_C\" \n[19] \"Vital_D\"  \"Vital_E\"  \"Vital_F\" \n\n\n\nCheck if data fulfill the basic requirement by AutoScore using the appropriate function for the outcome type.\nFix the problem if you see any warnings.\n\n\ncheck_data(sample_data)\n\n\nYour dataset doesn't have any missing values.\n\n\nData type check passed.\n\ncheck_data_survival(sample_data_survival)\n\nWarning in check_data_survival(sample_data_survival): Please keep outcome status variable binary\n\n\n\nYour dataset doesn't have any missing values.\n\nData type check passed.\n\ncheck_data_ordinal(sample_data_ordinal)\n\n\nYour dataset doesn't have any missing values.\n\nData type check passed.\n\n\n\nModify your data, and re-run the appropriate function to check the data again until there are no warning messages.\n\n\n\n4.2.2 Check variables\nUse check_data(), check_data_survival() or check_data_ordinal() to check whether current data fulfill the following requirements:\n\nRemove special characters from variable names, e.g., [, ], (, ),,. (Suggest using _ to replace them if needed)\nName of the variable should be unique and not entirely included by other variable names.\nIndependent variables should be numeric (class: num/int) or categorical (class: factor/logic)\n\n\nCheck missing values\nThe check data functions check_data(), check_data_survival() or check_data_ordinal() will display the missing rates as warnings for variables that contain missingness.\n\nAs input, AutoScore requires a complete dataset (no missing values). Thus, if your data is complete and fulfill other requirements, you can then move forward to modelling.\nIf there are missing values in your dataset and you believe the missingness in your dataset is informative and prevalent enough that you prefer to preserve them as NA rather than removing or doing imputation, you can also move forward because AutoScore can automatically handle missing values by treating them as a new category named ‘Unknown’.\nOtherwise, we suggest you first handle your missing values using appropriate imputation methods.\n\n\n\n\n\n\n\nNote\n\n\n\nIn either way, imputation or treating as a new category, variables with high missing rate (e.g., >80%) may reduce model stability and should be analysed with caution.\n\n\n\n\nOptional Operations\n\nCheck variable distribution.\nHandle outliers. The raw EHR data may contain outliers caused by system errors or clerical mistakes. User are recommended to handle them well before using AutoScore to make sure a good performance."
  },
  {
    "objectID": "04-autoscore.html",
    "href": "04-autoscore.html",
    "title": "5  AutoScore for binary outcomes",
    "section": "",
    "text": "(sample size = 20000)**\n\n\n\nlibrary(AutoScore)\ndata(\"sample_data\")\nnames(sample_data)[names(sample_data) == \"Mortality_inpatient\"] <- \"label\"\ncheck_data(sample_data)\n\n\nYour dataset doesn't have any missing values.\n\n\nData type check passed.\n\n\nIn Demo #1, we demonstrate the use of AutoScore on a comparably large dataset where separate training and validation sets are available. Please note that it is just a demo using simulated data, and thus, the result might not be clinically meaningful.\n\n\n\n\nOption 1: Prepare three separate datasets to train, validate, and test models.\nOption 2: Use demo codes below to randomly split your dataset into training, validation, and test datasets (70%, 10%, 20%, respectively).\n\n\nset.seed(4)\nout_split <- split_data(data = sample_data, ratio = c(0.7, 0.1, 0.2))\ntrain_set <- out_split$train_set\nvalidation_set <- out_split$validation_set\ntest_set <- out_split$test_set\n\n\n\n\n\nmethod: “rf” (default) or “auc”.\nmethod = rf: “rf” refers to random forest-based ranking\n\nntree: Number of trees required only if when method is “rf” (Default: 100).\n\n\nranking <- AutoScore_rank(train_set = train_set, method = \"rf\", ntree = 100)\n\nThe ranking based on variable importance was shown below for each variable: \n      Age     Lab_H     Lab_K     Lab_B   Vital_E   Vital_A     Lab_I     Lab_A \n152.24380 151.60648 147.96133 134.85962 130.84508 128.05477 106.01727  96.68171 \n  Vital_C   Vital_B     Lab_M     Lab_J   Vital_D     Lab_D   Vital_F     Lab_E \n 94.91108  94.41436  92.31222  84.42058  83.63048  80.23488  77.07122  75.73559 \n    Lab_C     Lab_F     Lab_L     Lab_G   Vital_G \n 75.48998  75.08788  72.61001  56.33592  56.08578 \n\n\n\n\n\n\n\n\n\nmethod = auc: “auc” refers to the AUC-based ranking. For “auc”, univariate models will be built based on the train set, and the variable ranking is constructed via the AUC performance of corresponding univariate models on the validation set (validation_set).\n\nvalidation_set: validation set required only if when method is “auc”.\n\n\nranking <- AutoScore_rank(train_set = train_set, method = \"auc\", validation_set = validation_set)\n\nThe auc-based ranking based on variable importance was shown below for each variable: \n    Lab_H       Age     Lab_B     Lab_K   Vital_E   Vital_A     Lab_M     Lab_C \n0.7016120 0.6926165 0.6796975 0.6741446 0.6708401 0.6319503 0.6010980 0.6010525 \n    Lab_G   Vital_C   Vital_D     Lab_F     Lab_L     Lab_A   Vital_F   Vital_G \n0.5777848 0.5743282 0.5617414 0.5606434 0.5427415 0.5392167 0.5380191 0.5345188 \n  Vital_B     Lab_D     Lab_I     Lab_E     Lab_J \n0.5326130 0.5186835 0.5139225 0.4888609 0.4845179 \n\n\n\n\n\n\n\n\n\n\n\n\nnmin: Minimum number of selected variables (Default: 1).\nnmax: Maximum number of selected variables (Default: 20).\ncategorize: Methods for categorizing continuous variables. Options include \"quantile\" or \"kmeans\" (Default: \"quantile\").\nquantiles: Predefined quantiles to convert continuous variables to categorical ones. (Default: c(0, 0.05, 0.2, 0.8, 0.95, 1)) Available if categorize = \"quantile\".\nmax_cluster: The max number of cluster (Default: 5). Available if categorize = \"kmeans\".\nmax_score: Maximum total score (Default: 100).\nauc_lim_min: Min y_axis limit in the parsimony plot (Default: 0.5).\nauc_lim_max: Max y_axis limit in the parsimony plot (Default: “adaptive”).\n\n\nAUC <- AutoScore_parsimony(\n    train_set = train_set,\n    validation_set = validation_set,\n    rank = ranking,\n    max_score = 100,\n    n_min = 1,\n    n_max = 20,\n    categorize = \"quantile\",\n    quantiles = c(0, 0.05, 0.2, 0.8, 0.95, 1),\n    auc_lim_min = 0.5,\n    auc_lim_max = \"adaptive\"\n  )\n\nSelect 1 Variable(s):  Area under the curve: 0.6649\nSelect 2 Variable(s):  Area under the curve: 0.7466\nSelect 3 Variable(s):  Area under the curve: 0.7729\nSelect 4 Variable(s):  Area under the curve: 0.7915\nSelect 5 Variable(s):  Area under the curve: 0.8138\nSelect 6 Variable(s):  Area under the curve: 0.8268\nSelect 7 Variable(s):  Area under the curve: 0.822\nSelect 8 Variable(s):  Area under the curve: 0.8196\nSelect 9 Variable(s):  Area under the curve: 0.8188\nSelect 10 Variable(s):  Area under the curve: 0.8184\nSelect 11 Variable(s):  Area under the curve: 0.8178\nSelect 12 Variable(s):  Area under the curve: 0.8238\nSelect 13 Variable(s):  Area under the curve: 0.8224\nSelect 14 Variable(s):  Area under the curve: 0.8256\nSelect 15 Variable(s):  Area under the curve: 0.8301\nSelect 16 Variable(s):  Area under the curve: 0.8278\nSelect 17 Variable(s):  Area under the curve: 0.8269\nSelect 18 Variable(s):  Area under the curve: 0.8273\nSelect 19 Variable(s):  Area under the curve: 0.8244\nSelect 20 Variable(s):  Area under the curve: 0.8259\n\n\n\n\n\n\nUsers could use the AUC for further analysis or export it as the CSV to other software for plotting.\n\n\nwrite.csv(data.frame(AUC), file = \"D:/AUC.csv\")\n\n\nDetermine the optimal number of variables (num_var) based on the parsimony plot obtained in STEP(ii).\nThe final list of variables is the first num_var variables in the ranked list ranking obtained in STEP(i).\nOptional: User can adjust the finally included variables final_variables based on the clinical preferences and knowledge.\n\n\n# Example 1: Top 6 variables are selected\nnum_var <- 6\nfinal_variables <- names(ranking[1:num_var])\n\n# Example 2: Top 9 variables are selected\nnum_var <- 9\nfinal_variables <- names(ranking[1:num_var])\n\n# Example 3: Top 6 variables, the 9th and 10th variable are selected\nnum_var <- 6\nfinal_variables <- names(ranking[c(1:num_var, 9, 10)])\n\n\n\n\n\nGenerate cut_vec with current cutoffs of continuous variables, which can be fine-tuned in STEP(iv).\n\n\ncut_vec <- AutoScore_weighting( \n    train_set = train_set,\n    validation_set = validation_set,\n    final_variables = final_variables,\n    max_score = 100,\n    categorize = \"quantile\",\n    quantiles = c(0, 0.05, 0.2, 0.8, 0.95, 1)\n  )\n\n****Included Variables: \n  variable_name\n1           Age\n2         Lab_H\n3         Lab_K\n4         Lab_B\n5       Vital_E\n6       Vital_A\n****Initial Scores: \n\n\n========  ==========  =====\nvariable  interval    point\n========  ==========  =====\nAge       <35           0  \n          [35,49)       7  \n          [49,76)      17  \n          [76,89)      23  \n          >=89         27  \n                           \nLab_H     <0.2          0  \n          [0.2,1.1)     4  \n          [1.1,3.1)     9  \n          [3.1,4)      15  \n          >=4          18  \n                           \nLab_K     <8            0  \n          [8,42)        6  \n          [42,58)      11  \n          >=58         14  \n                           \nLab_B     <8.5          0  \n          [8.5,11.2)    4  \n          [11.2,17)     7  \n          [17,19.8)    10  \n          >=19.8       12  \n                           \nVital_E   <12           0  \n          [12,15)       2  \n          [15,22)       7  \n          [22,25)      12  \n          >=25         15  \n                           \nVital_A   <60           0  \n          [60,73)       1  \n          [73,98)       6  \n          [98,111)     10  \n          >=111        13  \n========  ==========  =====\n\n\n\n\n\n***Performance (based on validation set):\nAUC:  0.8268   95% CI: 0.7953-0.8583 (DeLong)\nBest score threshold: >= 57 \nOther performance indicators based on this score threshold: \nSensitivity: 0.8065\nSpecificity: 0.6775\nPPV:         0.1736\nNPV:         0.9766\n***The cutoffs of each variable generated by the AutoScore are saved in cut_vec. You can decide whether to revise or fine-tune them \n\n\n\n\n\n\nRevise cut_vec with domain knowledge to update the scoring table (AutoScore Module 5).\nRe-run AutoScore Modules 2+3 to generate the updated scores.\nUsers can choose any cutoff values and/or any number of categories, but are suggested to choose numbers close to the automatically determined values.\n\n\n## For example, we have current cutoffs of continuous variable: Age \n## ==============  ===========  =====\n## variable        interval     point\n## ==============  ===========  =====\n## Age             <35            0  \n##                 [35,49)        7  \n##                 [49,76)       17  \n##                 [76,89)       23  \n##                 >=89          27  \n\n\nCurrent cutoffs:c(35, 49, 76, 89). We can fine tune the cutoffs as follows:\n\n\n# Example 1: rounding up to a nice number\ncut_vec$Age <- c(35, 50, 75, 90)\n\n# Example 2: changing cutoffs according to clinical knowledge or preference \ncut_vec$Age <- c(25, 50, 75, 90)\n\n# Example 3: combining categories\ncut_vec$Age <- c(50, 75, 90)\n\n\nThen we do similar checks for other variables and update scoring table using new cutoffs if needed.\n\n\ncut_vec$Lab_H <- c(0.2, 1, 3, 4)\ncut_vec$Lab_K <- c(10, 40)\ncut_vec$Lab_B <- c(10, 17)\ncut_vec$hVital_A<- c(70, 98)\n\n\nscoring_table <- AutoScore_fine_tuning(train_set,\n                        validation_set,\n                        final_variables,\n                        cut_vec,\n                        max_score = 100)\n\n***Fine-tuned Scores: \n\n\n========  ========  =====\nvariable  interval  point\n========  ========  =====\nAge       <50         0  \n          [50,75)    11  \n          [75,90)    19  \n          >=90       24  \n                         \nLab_H     <0.2        0  \n          [0.2,1)     6  \n          [1,3)      11  \n          [3,4)      17  \n          >=4        21  \n                         \nLab_K     <10         0  \n          [10,40)     8  \n          >=40       14  \n                         \nLab_B     <10         0  \n          [10,17)     3  \n          >=17        8  \n                         \nVital_E   <12         0  \n          [12,15)     1  \n          [15,22)     8  \n          [22,25)    15  \n          >=25       18  \n                         \nVital_A   <60         0  \n          [60,73)     1  \n          [73,98)     7  \n          [98,111)   11  \n          >=111      15  \n========  ========  =====\n\n\n\n\n\n***Performance (based on validation set, after fine-tuning):\nAUC:  0.8209   95% CI: 0.7884-0.8533 (DeLong)\nBest score threshold: >= 54 \nOther performance indicators based on this score threshold: \nSensitivity: 0.8452\nSpecificity: 0.658\nPPV:         0.1719\nNPV:         0.9806\n\n\n\n\n\n\nthreshold: Score threshold for the ROC analysis to generate sensitivity, specificity, etc. If set to \"best\", the optimal threshold will be calculated (Default: \"best\").\nwith_label: Set to TRUE if there are labels in the test_set and performance will be evaluated accordingly (Default: TRUE).\nSet the with_label to FALSE if there are not label in the test_set and the final predicted scores will be the output without performance evaluation.\n\n\npred_score <-\n  AutoScore_testing(\n    test_set = test_set,\n    final_variables = final_variables,\n    cut_vec = cut_vec,\n    scoring_table = scoring_table,\n    threshold = \"best\",\n    with_label = TRUE\n  )\n\n\n\n\n***Performance using AutoScore:\nAUC:  0.836   95% CI: 0.8147-0.8573 (DeLong)\nBest score threshold: >= 57 \nOther performance indicators based on this score threshold: \nSensitivity: 0.7557 95% CI: 0.7068-0.8046\nSpecificity: 0.7669 95% CI: 0.7536-0.7809\nPPV:         0.2125 95% CI: 0.1976-0.2269\nNPV:         0.9742 95% CI: 0.9691-0.9792\n\nhead(pred_score)\n\n  pred_score Label\n1         19 FALSE\n2         40 FALSE\n3         69  TRUE\n4         37 FALSE\n5         48 FALSE\n6         34 FALSE\n\n\n\nUse print_roc_performance() to generate the performance under different score thresholds (e.g., 50).\n\n\nprint_roc_performance(pred_score$Label, pred_score$pred_score, threshold = 50)\n\nAUC:  0.836   95% CI: 0.8147-0.8573 (DeLong)\nScore threshold: >= 50 \nOther performance indicators based on this score threshold: \nSensitivity: 0.8925\nSpecificity: 0.5754\nPPV:         0.1488\nNPV:         0.9847"
  },
  {
    "objectID": "04-autoscore.html#evaluation-conversion-table-calibration-etc",
    "href": "04-autoscore.html#evaluation-conversion-table-calibration-etc",
    "title": "5  AutoScore for binary outcomes",
    "section": "5.2 evaluation (conversion table, calibration etc)",
    "text": "5.2 evaluation (conversion table, calibration etc)\nFurther analysis based on the final scoring systems (e.g., conversion table, model calibration, output the score) - Use conversion_table() to generate the performance under different risk (i.e., probability of mortality based on logistic regression estimation) cut-off (e.g., 0.01, 0.05, 0.1, 0.2, 0.5).\n\nconversion_table(pred_score = pred_score, by =\"risk\", values = c(0.01,0.05,0.1,0.2,0.5))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredicted Risk [>=]\nScore cut-off [>=]\nPercentage of patients (%)\nAccuracy (95% CI)\nSensitivity (95% CI)\nSpecificity (95% CI)\nPPV (95% CI)\nNPV (95% CI)\n\n\n\n\n1%\n38\n79\n28.3% (27-29.6%)\n99% (97.7-100%)\n22.4% (21-23.8%)\n9.6% (9.4-9.8%)\n99.6% (99.2-100%)\n\n\n5%\n52\n42\n63.4% (61.9-64.8%)\n87.9% (84.4-91.5%)\n61.4% (59.8-62.9%)\n15.9% (15.1-16.7%)\n98.4% (97.9-98.9%)\n\n\n10%\n59\n24\n78.9% (77.7-80.2%)\n71.7% (66.8-76.2%)\n79.6% (78.3-80.8%)\n22.6% (21-24.2%)\n97.1% (96.6-97.6%)\n\n\n20%\n66\n11\n88.6% (87.7-89.4%)\n46.3% (40.4-52.1%)\n92.1% (91.3-93%)\n32.8% (29.1-36.3%)\n95.4% (94.9-95.9%)\n\n\n50%\n79\n1\n92.7% (92.4-93%)\n11.1% (7.5-14.7%)\n99.5% (99.3-99.7%)\n65.5% (52.6-77.3%)\n93.1% (92.8-93.3%)\n\n\n\n\n\n\nUse conversion_table() to generate the performance under different score thresholds (e.g., 20, 40, 60, 75).\n\n\nconversion_table(pred_score = pred_score, by = \"score\", values = c(20,40,60,75))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScore cut-off [>=]\nPredicted Risk [>=]\nPercentage of patients (%)\nAccuracy (95% CI)\nSensitivity (95% CI)\nSpecificity (95% CI)\nPPV (95% CI)\nNPV (95% CI)\n\n\n\n\n20\n0.1%\n99\n8.4% (8.2-8.7%)\n100% (100-100%)\n0.8% (0.5-1.1%)\n7.7% (7.7-7.8%)\n100% (100-100%)\n\n\n40\n1.4%\n73\n34.5% (33-36%)\n98.7% (97.4-99.7%)\n29.1% (27.6-30.7%)\n10.4% (10.1-10.6%)\n99.6% (99.2-99.9%)\n\n\n60\n11.4%\n22\n80.9% (79.7-82.1%)\n66.1% (60.9-71.3%)\n82.1% (80.9-83.3%)\n23.6% (21.7-25.5%)\n96.7% (96.2-97.2%)\n\n\n75\n40.7%\n2\n92.5% (92.1-93%)\n16.6% (12.4-20.8%)\n98.8% (98.5-99.2%)\n54.4% (45.1-63.9%)\n93.4% (93.1-93.8%)\n\n\n\n\n\n\nthe conversion table can also be visualized using an interactive figure.\n\n\nplot_predicted_risk(pred_score = pred_score, max_score = 100, \n                    final_variables = final_variables, \n                    scoring_table = scoring_table, point_size = 1)\n\n\n\n\n\n\nYou can also generate the pred_score_train based on training data for further analysis (e.g., conversion table based on the training data).\n\n\npred_score_train <-\n  AutoScore_testing(\n    test_set = train_set,\n    final_variables = final_variables,\n    cut_vec = cut_vec,\n    scoring_table = scoring_table,\n    threshold = \"best\",\n    with_label = TRUE\n  )\n\n***Performance using AutoScore:\nAUC:  0.8358   95% CI: 0.8245-0.8471 (DeLong)\nBest score threshold: >= 58 \nOther performance indicators based on this score threshold: \nSensitivity: 0.7371 95% CI: 0.7113-0.762\nSpecificity: 0.7771 95% CI: 0.7698-0.7841\nPPV:         0.2242 95% CI: 0.2161-0.2323\nNPV:         0.9713 95% CI: 0.9685-0.974\n\n\n\nGenerate conversion table based on the training data based on predictive risk (i.e., probability of mortality based on logistic regression estimation) cut-off (e.g., 0.01, 0.05, 0.1, 0.2, 0.5) using conversion_table()\n\n\nconversion_table(pred_score = pred_score_train, by =\"risk\", values = c(0.01,0.05,0.1,0.2,0.5))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredicted Risk [>=]\nScore cut-off [>=]\nPercentage of patients (%)\nAccuracy (95% CI)\nSensitivity (95% CI)\nSpecificity (95% CI)\nPPV (95% CI)\nNPV (95% CI)\n\n\n\n\n1%\n38\n80\n27.6% (26.9-28.2%)\n99.1% (98.5-99.6%)\n21.3% (20.6-22%)\n9.9% (9.8-10%)\n99.6% (99.4-99.9%)\n\n\n5%\n52\n43\n62.8% (62-63.6%)\n87.5% (85.4-89.3%)\n60.7% (59.8-61.5%)\n16.3% (15.9-16.7%)\n98.2% (97.9-98.5%)\n\n\n10%\n59\n24\n79.2% (78.5-79.9%)\n69.3% (66.4-71.8%)\n80.1% (79.4-80.8%)\n23.3% (22.4-24.3%)\n96.8% (96.5-97%)\n\n\n20%\n65\n12\n87.9% (87.4-88.4%)\n47.6% (44.8-50.4%)\n91.4% (90.9-91.9%)\n32.7% (30.9-34.6%)\n95.2% (95-95.5%)\n\n\n50%\n77\n2\n92.1% (91.9-92.3%)\n11.1% (9.2-13.1%)\n99.2% (99.1-99.4%)\n55% (48.5-61.5%)\n92.7% (92.6-92.9%)\n\n\n\n\n\n\nthe conversion table can also be visualized using an interactive figure.\n\n\nplot_predicted_risk(pred_score = pred_score_train, max_score = 100, \n                    final_variables = final_variables, \n                    scoring_table = scoring_table, point_size = 1)\n\n\n\n\n\n\nUsers could use the pred_score or pred_score_train for further analysis or export it as the CSV to other software (e.g., generating the calibration curve).\n\n\nwrite.csv(pred_score, file = \"D:/pred_score.csv\")\nwrite.csv(pred_score_train, file = \"D:/pred_score_train.csv\")"
  },
  {
    "objectID": "04-autoscore.html#development-on-small-dataset-sample-size-1000-with-cross-validation",
    "href": "04-autoscore.html#development-on-small-dataset-sample-size-1000-with-cross-validation",
    "title": "5  AutoScore for binary outcomes",
    "section": "5.3 development on Small dataset (sample size = 1000) with cross-validation**",
    "text": "5.3 development on Small dataset (sample size = 1000) with cross-validation**\nIn Demo #2, we demonstrate the use of AutoScore on a comparably small dataset where there are no sufficient samples to form a separate training and validation datasets. Thus, the cross validation is employed to generate the parsimony plot.\n\n5.3.1 Get small dataset with 1000 samples\n\ndata(\"sample_data_small\")\n\n\n\n5.3.2 Prepare training and test datasets\n\nOption 1: Prepare two separate datasets to train and test models.\nOption 2: Use demo codes below to randomly split your dataset into training and test datasets (70% and 30%, respectively). For cross-validation, train_set is equal to validation_set and the ratio of validation_set should be 0. Then cross-validation will be implemented in the STEP(ii) AutoScore_parsimony().\n\n\nset.seed(4)\nout_split <- split_data(data = sample_data_small, ratio = c(0.7, 0, 0.3), cross_validation = TRUE)\ntrain_set <- out_split$train_set\nvalidation_set <- out_split$validation_set\ntest_set <- out_split$test_set\n\n\n\n5.3.3 STEP(i): Generate variable ranking list (AutoScore Module 1)\n\nmethod: “rf” (default) or “auc”.\n\nmethod = auc: “auc” refers to the AUC-based ranking. For “auc”, univariate models will be built based on the train set, and the variable ranking is constructed via the AUC performance of corresponding univariate models on the validation set (validation_set).\n- validation_set: validation set required only if when method is “auc”.\n\nranking <- AutoScore_rank(train_set, validation_set = validation_set, ntree = 100)\n\nThe ranking based on variable importance was shown below for each variable: \n      Age     Lab_B     Lab_H   Vital_E     Lab_K   Vital_A     Lab_I     Lab_J \n37.406648 31.315285 25.564054 21.855069 20.907522 20.645694 16.788696 16.094679 \n  Vital_B     Lab_A     Lab_M   Vital_C   Vital_F     Lab_D     Lab_C     Lab_E \n15.574365 14.651987 14.297510 13.765633 12.932043 12.679113 12.295000 12.165724 \n    Lab_F   Vital_D     Lab_L     Lab_G   Vital_G \n11.649415 11.431833 10.108408  9.297786  7.680821 \n\n\n\n\n\nmethod = rf: “rf” refers to random forest-based ranking\n- ntree: Number of trees required only if when method is “rf” (Default: 100).\n\nranking <- AutoScore_rank(train_set, ntree = 100)\n\nThe ranking based on variable importance was shown below for each variable: \n      Age     Lab_B     Lab_H     Lab_K   Vital_E   Vital_A     Lab_I     Lab_A \n33.661071 30.937904 25.129836 23.696395 20.572556 19.405187 16.609492 15.938134 \n    Lab_J   Vital_C   Vital_B   Vital_F     Lab_M     Lab_C     Lab_D   Vital_D \n14.991216 14.839592 14.576851 14.465937 14.461607 13.750307 13.282037 13.245753 \n    Lab_F     Lab_E     Lab_L     Lab_G   Vital_G \n12.097335 10.795598 10.617640  8.980531  6.903336 \n\n\n\n\n\n\n\n5.3.4 STEP(ii): Select the best model with parsimony plot (AutoScore Modules 2+3+4)\n\nnmin: Minimum number of selected variables (Default: 1).\nnmax: Maximum number of selected variables (Default: 20).\ncategorize: Methods for categorize continuous variables. Options include \"quantile\" or \"kmeans\" (Default: \"quantile\").\nquantiles: Predefined quantiles to convert continuous variables to categorical ones. (Default: c(0, 0.05, 0.2, 0.8, 0.95, 1)) Available if categorize = \"quantile\".\nmax_cluster: The max number of cluster (Default: 5). Available if categorize = \"kmeans\".\nmax_score Maximum total score (Default: 100).\ncross_validation : TRUE if cross-validation is needed, especially for small datasets.\nfold The number of folds used in cross validation (Default: 10). Available if cross_validation = TRUE.\ndo_trace If set to TRUE, all results based on each fold of cross-validation would be printed out and plotted (Default: FALSE). Available if cross_validation = TRUE.\n\n\nAUC <- AutoScore_parsimony(\n    train_set,\n    validation_set,\n    rank = ranking,\n    max_score = 100,\n    n_min = 1,\n    n_max = 20,\n    cross_validation = TRUE,\n    categorize = \"quantile\",\n    fold = 10,\n    quantiles = c(0, 0.25, 0.5, 0.75, 1), #c(0, 0.05, 0.2, 0.8, 0.95, 1)\n    do_trace = FALSE\n  )\n\n***list of final mean AUC values through cross-validation are shown below \n   auc_set.sum\n1    0.6396675\n2    0.7231147\n3    0.7315859\n4    0.7457606\n5    0.7624567\n6    0.7669644\n7    0.7703138\n8    0.7642128\n9    0.7665648\n10   0.7713672\n11   0.7674212\n12   0.7664641\n13   0.7584156\n14   0.7558671\n15   0.7629152\n16   0.7636249\n17   0.7689182\n18   0.7656851\n19   0.7625896\n20   0.7593715\n\n\n\n\n\n\nUsers could use the AUC for further analysis or export it as the CSV to other software for plotting.\n\n\nwrite.csv(data.frame(AUC), file = \"D:/AUC.csv\")\n\n\nDetermine the optimal number of variables (num_var) based on the parsimony plot obtained in STEP(ii).\nThe final list of variables is the first num_var variables in the ranked list ranking obtained in STEP(i).\nOptional: User can adjust the finally included variables final_variables based on the clinical preferences and knowledge).\n\n\n# Example 1: Top 6 variables are selected\nnum_var <- 6\nfinal_variables <- names(ranking[1:num_var])\n\n# Example 2: Top 9 variables are selected\nnum_var <- 9\nfinal_variables <- names(ranking[1:num_var])\n\n# Example 3: Top 6 variables, the 9th and 10th variable are selected\nnum_var <- 6\nfinal_variables <- names(ranking[c(1:num_var, 9, 10)])\n\n\n\n5.3.5 STEP(iii): Generate initial scores with the final list of variables (Re-run AutoScore Modules 2+3)\n\nGenerate cut_vec with current cutoffs of continuous variables, which can be fine-tuned in STEP(iv).\n\n\ncut_vec <- AutoScore_weighting( \n    train_set,\n    validation_set,\n    final_variables,\n    max_score = 100,\n    categorize = \"quantile\",\n    quantiles = c(0, 0.05, 0.2, 0.8, 0.95, 1)\n  )\n\n****Included Variables: \n  variable_name\n1           Age\n2         Lab_B\n3         Lab_H\n4         Lab_K\n5       Vital_E\n****Initial Scores: \n\n\n========  ===========  =====\nvariable  interval     point\n========  ===========  =====\nAge       <35            0  \n          [35,49)        1  \n          [49,75)       13  \n          [75,88)       21  \n          >=88          18  \n                            \nLab_B     <8.6           0  \n          [8.6,11.3)     7  \n          [11.3,17.5)   14  \n          [17.5,19.8)   17  \n          >=19.8        14  \n                            \nLab_H     <1             0  \n          [1,3.1)        8  \n          [3.1,4.1)     14  \n          >=4.1         22  \n                            \nLab_K     <9             0  \n          [9,43.2)       1  \n          [43.2,59)      9  \n          >=59          13  \n                            \nVital_E   <12            0  \n          [12,15)        7  \n          [15,22)       11  \n          [22,25)       16  \n          >=25          28  \n========  ===========  =====\n\n\n\n\n\n***Performance (based on validation set):\nAUC:  0.7891   95% CI: 0.7558-0.8224 (DeLong)\nBest score threshold: >= 48 \nOther performance indicators based on this score threshold: \nSensitivity: 0.706\nSpecificity: 0.7589\nPPV:         0.7604\nNPV:         0.7044\n***The cutoffs of each variable generated by the AutoScore are saved in cut_vec. You can decide whether to revise or fine-tune them \n\n\n\n\n5.3.6 STEP(iv): Fine-tune the initial score generated in STEP(iii) (AutoScore Module 5 & Re-run AutoScore Modules 2+3)\n\nRevise cut_vec with domain knowledge to update the scoring table (AutoScore Module 5).\nRe-run AutoScore Modules 2+3 to generate the updated scores.\nUsers can choose any cutoff values and/or any number of categories, but are suggested to choose numbers close to the automatically determined values.\n\n\n## For example, we have current cutoffs of continuous variable:\n## ==============  ===========  =====\n## variable        interval     point\n## ==============  ===========  =====\n#> Lab_K           <9             0  \n#>                [9,43.2)       1  \n#>                [43.2,59)      9  \n#>                >=59          13  \n\n\nCurrent cutoffs: c(9, 43.2, 59). We can fine tune the cutoffs as follows:\nNote: It is just a demo using simulated data, and thus, the result might not be clinically meaningful.\n\n\n# Example 1: rounding up to a nice number\ncut_vec$Lab_K <- c(9, 45, 60)\n\n# Example 2: changing cutoffs according to clinical knowledge or preference \ncut_vec$Lab_K <- c(15, 45, 60)\n\n# Example 3: combining categories\ncut_vec$Lab_K <- c(45, 60)\n\n\nThen we do similar checks for other variables and update scoring table using new cutoffs if needed.\n\n\ncut_vec$Lab_H <- c(1, 2, 3)\ncut_vec$Age <- c(35, 50, 80)\ncut_vec$Lab_B <- c(8, 12, 18)\ncut_vec$Vital_E <- c(15, 22)\nscoring_table <- AutoScore_fine_tuning(train_set,\n                        validation_set,\n                        final_variables,\n                        cut_vec,\n                        max_score = 100)\n\n***Fine-tuned Scores: \n\n\n========  ========  =====\nvariable  interval  point\n========  ========  =====\nAge       <35         0  \n          [35,50)     2  \n          [50,80)    18  \n          >=80       23  \n                         \nLab_B     <8          0  \n          [8,12)      8  \n          [12,18)    15  \n          >=18       22  \n                         \nLab_H     <1          0  \n          [1,2)      12  \n          [2,3)      13  \n          >=3        18  \n                         \nLab_K     <45         0  \n          [45,60)    10  \n          >=60       17  \n                         \nVital_E   <15         0  \n          [15,22)    10  \n          >=22       20  \n========  ========  =====\n\n\n\n\n\n***Performance (based on validation set, after fine-tuning):\nAUC:  0.7623   95% CI: 0.7275-0.7971 (DeLong)\nBest score threshold: >= 60 \nOther performance indicators based on this score threshold: \nSensitivity: 0.5714\nSpecificity: 0.8214\nPPV:         0.7761\nNPV:         0.6389\n\n\n\n\n5.3.7 STEP(v): Evaluate final risk scores on test dataset (AutoScore Module 6)\n\nthreshold: Score threshold for the ROC analysis to generate sensitivity, specificity, etc. If set to \"best\", the optimal threshold will be calculated (Default: \"best\").\nwith_label: Set to TRUE if there are labels in the test_set and performance will be evaluated accordingly (Default: TRUE).\nSet the with_label to FALSE if there are not label in the test_set and the final predicted scores will be the output without performance evaluation.\n\n\npred_score <-\n  AutoScore_testing(\n    test_set,\n    final_variables,\n    cut_vec,\n    scoring_table,\n    threshold = \"best\",\n    with_label = TRUE\n  )\n\n\n\n\n***Performance using AutoScore:\nAUC:  0.7133   95% CI: 0.6556-0.7709 (DeLong)\nBest score threshold: >= 51 \nOther performance indicators based on this score threshold: \nSensitivity: 0.7421 95% CI: 0.673-0.805\nSpecificity: 0.5887 95% CI: 0.5106-0.6667\nPPV:         0.6707 95% CI: 0.621-0.7198\nNPV:         0.6694 95% CI: 0.6043-0.7373\n\nhead(pred_score)\n\n  pred_score Label\n1         53  TRUE\n2         56  TRUE\n3         49  TRUE\n4         38 FALSE\n5         51  TRUE\n6         40  TRUE\n\n\n\nUse print_roc_performance() to generate the performance under different score thresholds (e.g., 90).\nNote: It is just a demo using simulated data, and thus, the result might not be clinically meaningful.\n\n\nprint_roc_performance(pred_score$Label, pred_score$pred_score, threshold = 90)\n\nAUC:  0.7133   95% CI: 0.6556-0.7709 (DeLong)\nScore threshold: >= 90 \nOther performance indicators based on this score threshold: \nSensitivity: 0.0063\nSpecificity: 1\nPPV:         1\nNPV:         0.4716\n\n\n\n\n5.3.8 STEP(vi): Further analysis based on the final scoring systems (e.g., conversion table, model calibration, output the score)\n\nYou can also generate conversion table using conversion_table(). Please refer to our. But we skipped it here.\nUsers could use the pred_score for further analysis or export it as the CSV to other software.\n\n\nwrite.csv(pred_score, file = \"D:/pred_score.csv\")"
  },
  {
    "objectID": "04-autoscore.html#development-on-dataset-with-missing-values-informative-missing",
    "href": "04-autoscore.html#development-on-dataset-with-missing-values-informative-missing",
    "title": "5  AutoScore for binary outcomes",
    "section": "5.4 development on dataset with missing values (informative missing)**",
    "text": "5.4 development on dataset with missing values (informative missing)**\nIn Demo #3, we demonstrate the use of AutoScore on a dataset with missing values. AutoScore can automatically treat the missingness as a new category named Unknown. Please note that it is just a demo using simulated data, and thus, the result might not be clinically meaningful.\n\n5.4.1 load package and data\n\nlibrary(AutoScore)\ndata(\"sample_data\")\nnames(sample_data)[names(sample_data) == \"Mortality_inpatient\"] <- \"label\"\ncheck_data(sample_data)\n\n\nYour dataset doesn't have any missing values.\n\n\nData type check passed.\n\n\n\n\n5.4.2 Create informative missing\n\nsample_data <- AutoScore:::induce_informative_missing(sample_data, vars_to_induce = names(sample_data)[1:2], prop_missing = c(0.2, 0.6))\n\nThe following steps are similar to Demo #1.\nNOTE:\n\nHigh missing rate may cause the variable ranking less reliable, and thus, caution is needed for variable selection using parsimony plot.\n\n\nAUC <- AutoScore_parsimony(\n    train_set,\n    validation_set,\n    rank = ranking,\n    max_score = 100,\n    n_min = 1,\n    n_max = 20,\n    categorize = \"quantile\",\n    quantiles = c(0, 0.05, 0.2, 0.8, 0.95, 1),\n    auc_lim_min = 0.5,\n    auc_lim_max = \"adaptive\"\n  )\n\n\n\n\n\nThe Unknown category indicating the missingness will be displayed in the final scoring table.\n\n\n\n***Fine-tuned Scores: \n\n\n========  =========  =====\nvariable  interval   point\n========  =========  =====\nAge       <50          0  \n          [50,75)     11  \n          [75,90)     19  \n          >=90        23  \n                          \nLab_H     <0.2         0  \n          [0.2,1)      6  \n          [1,3)       10  \n          [3,4)       18  \n          >=4         22  \n                          \nLab_K     <10          0  \n          [10,40)      7  \n          >=40        14  \n                          \nLab_B     <10          0  \n          [10,17)      3  \n          >=17         8  \n                          \nVital_E   <12          0  \n          [12,15)      1  \n          [15,22)      8  \n          [22,25)     15  \n          >=25        18  \n                          \nVital_A   <59          2  \n          [59,70)      0  \n          [70,100)     8  \n          [100,112)   14  \n          >=113       16  \n          Unknown      7  \n========  =========  =====\n\n\n\n\n\n***Performance (based on validation set, after fine-tuning):\nAUC:  0.8185   95% CI: 0.7863-0.8507 (DeLong)\nBest score threshold: >= 54 \nOther performance indicators based on this score threshold: \nSensitivity: 0.8323\nSpecificity: 0.6575\nPPV:         0.1695\nNPV:         0.979"
  },
  {
    "objectID": "05-autoscore_survival.html",
    "href": "05-autoscore_survival.html",
    "title": "6  AutoScore for survival outcomes (AutoScore-Survival)",
    "section": "",
    "text": "(sample size = 20000)**\n\n\n\nlibrary(AutoScore)\ndata(\"sample_data_survival\")\ncheck_data_survival(sample_data_survival)\n\nWarning in check_data_survival(sample_data_survival): Please keep outcome status variable binary\n\n\n\nYour dataset doesn't have any missing values.\n\n\nData type check passed.\n\n\nIn Demo #1, we demonstrate the use of AutoScore on a comparably large dataset where separate training and validation sets are available. Please note that it is just a demo using simulated data, and thus, the result might not be clinically meaningful.\n\n\n\n\nOption 1: Prepare three separate datasets to train, validate, and test models.\nOption 2: Use demo codes below to randomly split your dataset into training, validation, and test datasets (70%, 10%, 20%, respectively).\n\n\nset.seed(4)\nout_split <- split_data(data = sample_data_survival, ratio = c(0.7, 0.1, 0.2))\ntrain_set <- out_split$train_set\nvalidation_set <- out_split$validation_set\ntest_set <- out_split$test_set\n\n\n\n\n\nntree: Number of trees required only if when method is “rf” (Default: 100).(More time is needed if you run a larger number of trees. For demostration purpose, I just use ntree = 10 in this guidebook)\n\n\nranking <- AutoScore_rank_Survival(train_set, ntree = 5)\n\n\nTrees Grown:       1,    Time Remaining (sec):       4 \nTrees Grown:       5,    Time Remaining (sec):       0 \n\n\nThe ranking based on variable importance was shown below for each variable: \n    Vital_E       Lab_H         Age       Lab_K     Vital_A       Lab_B \n0.165608652 0.160762453 0.128912855 0.128557962 0.045219867 0.040417391 \n      Lab_G     Vital_C       Lab_M     Vital_B       Lab_C     Vital_D \n0.024342208 0.022561527 0.020905397 0.018080011 0.016227841 0.014980410 \n      Lab_J       Lab_F     Vital_F       Lab_L     Vital_G       Lab_I \n0.013780700 0.012120477 0.011200870 0.008297275 0.006354309 0.005682440 \n      Lab_E       Lab_D       Lab_A \n0.005093323 0.004567277 0.004141324 \n\n\n\n\n\n\n\n\n\nnmin: Minimum number of selected variables (Default: 1).\nnmax: Maximum number of selected variables (Default: 20).\ncategorize: Methods for categorizing continuous variables. Options include \"quantile\" or \"kmeans\" (Default: \"quantile\").\nquantiles: Predefined quantiles to convert continuous variables to categorical ones. (Default: c(0, 0.05, 0.2, 0.8, 0.95, 1)) Available if categorize = \"quantile\".\nmax_cluster: The max number of cluster (Default: 5). Available if categorize = \"kmeans\".\nmax_score: Maximum total score (Default: 100).\nauc_lim_min: Min y_axis limit in the parsimony plot (Default: 0.5).\nauc_lim_max: Max y_axis limit in the parsimony plot (Default: “adaptive”).\n\n\niAUC <- AutoScore_parsimony_Survival(\n    train_set,\n    validation_set,\n    rank = ranking,\n    max_score = 100,\n    n_min = 1,\n    n_max = 20,\n    categorize = \"quantile\",\n    quantiles = c(0, 0.05, 0.2, 0.8, 0.95, 1),\n    auc_lim_min = 0.5,\n    auc_lim_max = \"adaptive\"\n  )\n\nSelect 1 Variable(s):  0.6651428 \nSelect 2 Variable(s):  0.7474401 \nSelect 3 Variable(s):  0.8111006 \nSelect 4 Variable(s):  0.8416833 \nSelect 5 Variable(s):  0.867012 \nSelect 6 Variable(s):  0.8779297 \nSelect 7 Variable(s):  0.8809597 \nSelect 8 Variable(s):  0.8830361 \nSelect 9 Variable(s):  0.8875346 \nSelect 10 Variable(s):  0.8905936 \nSelect 11 Variable(s):  0.8902906 \nSelect 12 Variable(s):  0.8906035 \nSelect 13 Variable(s):  0.8930109 \nSelect 14 Variable(s):  0.8927079 \nSelect 15 Variable(s):  0.8935644 \nSelect 16 Variable(s):  0.8922507 \nSelect 17 Variable(s):  0.8926088 \nSelect 18 Variable(s):  0.89691 \nSelect 19 Variable(s):  0.8981632 \nSelect 20 Variable(s):  0.8983681 \n\n\n\n\n\n\nUsers could use the iAUC for further analysis or export it as the CSV to other software for plotting.\n\n\nwrite.csv(data.frame(iAUC), file = \"D:/iAUC.csv\")\n\n\nDetermine the optimal number of variables (num_var) based on the parsimony plot obtained in STEP(ii).\nThe final list of variables is the first num_var variables in the ranked list ranking obtained in STEP(i).\nOptional: User can adjust the finally included variables final_variables based on the clinical preferences and knowledge.\n\n\n# Example 1: Top 6 variables are selected\nnum_var <- 6\nfinal_variables <- names(ranking[1:num_var])\n\n# Example 2: Top 9 variables are selected\nnum_var <- 9\nfinal_variables <- names(ranking[1:num_var])\n\n# Example 3: Top 6 variables, the 9th and 10th variable are selected\nnum_var <- 6\nfinal_variables <- names(ranking[c(1:num_var, 9, 10)])\n\n\n\n\n\nGenerate cut_vec with current cutoffs of continuous variables, which can be fine-tuned in STEP(iv).\ntime_point: The time points to be evaluated using time-dependent AUC(t).\n\n\ncut_vec <- AutoScore_weighting_Survival( \n    train_set,\n    validation_set,\n    final_variables,\n    max_score = 100,\n    categorize = \"quantile\",\n    quantiles = c(0, 0.05, 0.2, 0.8, 0.95, 1),\n    time_point = c(1,3,7,14,30,60,90)\n  )\n\n****Included Variables: \n  variable_name\n1       Vital_E\n2         Lab_H\n3           Age\n4         Lab_K\n5       Vital_A\n6         Lab_B\n****Initial Scores: \n\n\n========  ==========  =====\nvariable  interval    point\n========  ==========  =====\nVital_E   <12           0  \n          [12,15)       5  \n          [15,22)      11  \n          [22,25)      16  \n          >=25         18  \n                           \nLab_H     <0.2          0  \n          [0.2,1.1)     3  \n          [1.1,3.1)     8  \n          [3.1,4)      13  \n          >=4          18  \n                           \nAge       <35           0  \n          [35,49)       5  \n          [49,76)      13  \n          [76,89)      21  \n          >=89         24  \n                           \nLab_K     <8            0  \n          [8,42)        5  \n          [42,58)      11  \n          >=58         13  \n                           \nVital_A   <60           0  \n          [60,73)       3  \n          [73,98)       8  \n          [98,111)     11  \n          >=111        13  \n                           \nLab_B     <8.5          0  \n          [8.5,11.2)    3  \n          [11.2,17)     8  \n          [17,19.8)    11  \n          >=19.8       13  \n========  ==========  =====\nIntegrated AUC by all time points: 0.8779297\nC_index:  0.785738 \nThe AUC(t) are shown as bwlow:\n  time_point     AUC_t\n1          1 0.0002500\n2          3 1.0000000\n3          7 0.9927981\n4         14 0.9872613\n5         30 0.9433755\n6         60 0.8735564\n7         90 0.8599778\n***The cutoffs of each variable generated by the AutoScore are saved in cut_vec. You can decide whether to revise or fine-tune them \n\n\n\n\n\n\nRevise cut_vec with domain knowledge to update the scoring table (AutoScore Module 5).\nRe-run AutoScore Modules 2+3 to generate the updated scores.\nUsers can choose any cutoff values and/or any number of categories, but are suggested to choose numbers close to the automatically determined values.\ntime_point: The time points to be evaluated using time-dependent AUC(t).\n\n\n## For example, we have current cutoffs of continuous variable: Age \n## ==============  ===========  =====\n## variable        interval     point\n## ==============  ===========  =====\n## Age             <35            0  \n##                 [35,49)        7  \n##                 [49,76)       17  \n##                 [76,89)       23  \n##                 >=89          27  \n\n\nCurrent cutoffs:c(35, 49, 76, 89). We can fine tune the cutoffs as follows:\n\n\n# Example 1: rounding up to a nice number\ncut_vec$Age <- c(35, 50, 75, 90)\n\n# Example 2: changing cutoffs according to clinical knowledge or preference \ncut_vec$Age <- c(25, 50, 75, 90)\n\n# Example 3: combining categories\ncut_vec$Age <- c(50, 75, 90)\n\n\nThen we do similar checks for other variables and update scoring table using new cutoffs if needed.\n\n\ncut_vec$Lab_H <- c(0.2, 1, 3, 4)\ncut_vec$Lab_K <- c(10, 40)\ncut_vec$Lab_B <- c(10, 17)\ncut_vec$hVital_A<- c(70, 98)\nscoring_table <- AutoScore_fine_tuning_Survival(train_set,\n                        validation_set,\n                        final_variables,\n                        cut_vec,\n                        max_score = 100,\n                        time_point = c(1,3,7,14,30,60,90))\n\n***Fine-tuned Scores: \n\n\n========  ========  =====\nvariable  interval  point\n========  ========  =====\nVital_E   <12         0  \n          [12,15)     5  \n          [15,22)    10  \n          [22,25)    17  \n          >=25       21  \n                         \nLab_H     <0.2        0  \n          [0.2,1)     2  \n          [1,3)      10  \n          [3,4)      14  \n          >=4        21  \n                         \nAge       <50         0  \n          [50,75)    10  \n          [75,90)    17  \n          >=90       21  \n                         \nLab_K     <10         0  \n          [10,40)     5  \n          >=40       12  \n                         \nVital_A   <60         0  \n          [60,73)     2  \n          [73,98)     7  \n          [98,111)   12  \n          >=111      14  \n                         \nLab_B     <10         0  \n          [10,17)     5  \n          >=17       10  \n========  ========  =====\n***Performance (based on validation set, after fine-tuning):\nIntegrated AUC by all time points: 0.8743444\nC_index:  0.782904 \nThe AUC(t) are shown as bwlow:\n  time_point     AUC_t\n1          1 0.0005000\n2          3 1.0000000\n3          7 0.9912325\n4         14 0.9867337\n5         30 0.9422868\n6         60 0.8682391\n7         90 0.8594720\n\n\n\n\n\n\nthreshold: Score threshold for the ROC analysis to generate sensitivity, specificity, etc. If set to \"best\", the optimal threshold will be calculated (Default: \"best\").\nwith_label: Set to TRUE if there are labels in the test_set and performance will be evaluated accordingly (Default: TRUE).\nSet the with_label to FALSE if there are not label in the test_set and the final predicted scores will be the output without performance evaluation.\ntime_point: The time points to be evaluated using time-dependent AUC(t).\n\n\npred_score <-\n  AutoScore_testing_Survival(\n    test_set,\n    final_variables,\n    cut_vec,\n    scoring_table,\n    threshold = \"best\",\n    with_label = TRUE,\n    time_point = c(1,3,7,14,30,60,90)\n  )\n\n***Performance using AutoScore (based on unseen test Set):\nIntegrated AUC by all time points: 0.876 (0.867-0.883)\nC_index:  0.786 (0.776-0.793) \nThe AUC(t) are shown as bwlow:\n  time_point               AUC_t\n1          1     0.639 (0-0.999)\n2          3 0.937 (0.836-0.999)\n3          7   0.958 (0.9-0.995)\n4         14 0.951 (0.924-0.977)\n5         30  0.94 (0.925-0.953)\n6         60 0.866 (0.855-0.874)\n7         90 0.871 (0.855-0.882)\n\nhead(pred_score)\n\n  pred_score label_time label_status\n1         19         91        FALSE\n2         42         60         TRUE\n3         68         43         TRUE\n4         34         90         TRUE\n5         47         62         TRUE\n6         29         91        FALSE"
  },
  {
    "objectID": "05-autoscore_survival.html#further-evaluation-conversion-table-calibration-etc",
    "href": "05-autoscore_survival.html#further-evaluation-conversion-table-calibration-etc",
    "title": "6  AutoScore for survival outcomes (AutoScore-Survival)",
    "section": "6.2 Further evaluation (conversion table, calibration etc)",
    "text": "6.2 Further evaluation (conversion table, calibration etc)\nFurther analysis based on the final scoring systems (e.g., conversion table, model calibration, output the score)\n\nUse plot_survival_km() to generate Kaplan Meier Curve under different score thresholds (decided by users, e.g., 50).\n\n\nplot_survival_km(pred_score, score_cut = c(50))\n\n\n\n\n\nUser can also use several score thresholds to cut the score for generate Kaplan Meier Curve.\n\n\nplot_survival_km(pred_score, score_cut = c(40,50,60))\n\n\n\n\n\nUse conversion_table_survival() to generate the performance under different score_cut score cut-off (decided by users, e.g.,40,50,60).\ntime_point: The time points to be evaluated using time-dependent AUC(t).\n\n\ntable <- conversion_table_survival(pred_score, score_cut =c(40,50,60), time_point = c(7,14,30,60,90))\n\n\nYou can also generate the pred_score_train based on training data for further analysis (e.g., KM curve or conversion table based on the training data).\n\n\npred_score_train <-\n  AutoScore_testing_Survival(\n    train_set,\n    final_variables,\n    cut_vec,\n    scoring_table,\n    threshold = \"best\",\n    with_label = TRUE,\n    time_point = c(1,3,7,14,30,60,90)\n  )\n\n***Performance using AutoScore (based on unseen test Set):\nIntegrated AUC by all time points: 0.872 (0.868-0.875)\nC_index:  0.781 (0.777-0.784) \nThe AUC(t) are shown as bwlow:\n  time_point               AUC_t\n1          1  0.98 (0.966-0.995)\n2          3 0.978 (0.968-0.991)\n3          7 0.976 (0.961-0.986)\n4         14 0.966 (0.958-0.974)\n5         30 0.924 (0.916-0.933)\n6         60 0.866 (0.859-0.871)\n7         90  0.864 (0.858-0.87)\n\n\n\nGenerate conversion table based on the training data different score_cut score cut-off (decided by users, e.g.,40,50,60).\n\n\nconversion_table_survival(pred_score_train, score_cut =c(40,50,60), time_point = c(7,14,30,60,90))\n\n                       [0,40) [40,50) [50,60) [60,95]\nNumber of patients       3341    4407    3714    2538\nPercentage of patients 23.86%  31.48%  26.53%  18.13%\nt=7                        0%      0%      0%   1.02%\nt=14                       0%      0%   0.03%   3.51%\nt=30                       0%   0.16%   1.75%  16.08%\nt=60                    1.59%  13.77%  43.35%  77.38%\nt=90                   33.07%  72.52%  93.38%  98.86%\n\n\n\nUsers could use the pred_score or pred_score_train for further analysis or export it as the CSV to other software (e.g., generating other curves).\n\n\nwrite.csv(pred_score, file = \"D:/pred_score.csv\")\nwrite.csv(pred_score_train, file = \"D:/pred_score_train.csv\")"
  },
  {
    "objectID": "05-autoscore_survival.html#development-on-small-dataset-sample-size-1000-with-cross-validation-for-survival-outcomes",
    "href": "05-autoscore_survival.html#development-on-small-dataset-sample-size-1000-with-cross-validation-for-survival-outcomes",
    "title": "6  AutoScore for survival outcomes (AutoScore-Survival)",
    "section": "6.3 development on Small dataset (sample size = 1000) with cross-validation for survival outcomes**",
    "text": "6.3 development on Small dataset (sample size = 1000) with cross-validation for survival outcomes**\nIn Demo #2, we demonstrate the use of AutoScore on a comparably small dataset where there are no sufficient samples to form a separate training and validation datasets. Thus, the cross validation is employed to generate the parsimony plot.\n\n6.3.1 Get small dataset with 1000 samples\n\ndata(\"sample_data_survival_small\")\n\n\n\n6.3.2 Prepare training and test datasets\n\nOption 1: Prepare two separate datasets to train and test models.\nOption 2: Use demo codes below to randomly split your dataset into training and test datasets (70% and 30%, respectively). For cross-validation, train_set is equal to validation_set and the ratio of validation_set should be 0. Then cross-validation will be implemented in the STEP(ii) AutoScore_parsimony().\n\n\nset.seed(4)\nout_split <- split_data(data = sample_data_survival_small, ratio = c(0.7, 0, 0.3), cross_validation = TRUE)\ntrain_set <- out_split$train_set\nvalidation_set <- out_split$validation_set\ntest_set <- out_split$test_set"
  },
  {
    "objectID": "05-autoscore_survival.html#development-on-dataset-with-missing-values-informative-missing",
    "href": "05-autoscore_survival.html#development-on-dataset-with-missing-values-informative-missing",
    "title": "6  AutoScore for survival outcomes (AutoScore-Survival)",
    "section": "6.4 development on dataset with missing values (informative missing)**",
    "text": "6.4 development on dataset with missing values (informative missing)**\nIn Demo #3, we demonstrate the use of AutoScore on a dataset with missing values. AutoScore can automatically treat the missingness as a new category named Unknown. Please note that it is just a demo using simulated data, and thus, the result might not be clinically meaningful.\n\n6.4.1 load package and data\n\nlibrary(AutoScore)\ndata(\"sample_data_survival\")\ncheck_data_survival(sample_data_survival)\n\n\nYour dataset doesn't have any missing values.\n\n\nData type check passed.\n\n\n\n\n6.4.2 Create informative missing\n\nsample_data_survival <- AutoScore:::induce_informative_missing(sample_data_survival, vars_to_induce = names(sample_data_survival)[1:2], prop_missing = c(0.2, 0.6))\n\nThe following steps are similar to Demo #1.\nNOTE:\n\nHigh missing rate may cause the variable ranking less reliable, and thus, caution is needed for variable selection using parsimony plot.\n\n\niAUC <- AutoScore_parsimony_Survival(\n    train_set,\n    validation_set,\n    rank = ranking,\n    max_score = 100,\n    n_min = 1,\n    n_max = 20,\n    categorize = \"quantile\",\n    quantiles = c(0, 0.05, 0.2, 0.8, 0.95, 1),\n    auc_lim_min = 0.5,\n    auc_lim_max = \"adaptive\"\n  )\n\n\n\n\n\nThe Unknown category indicating the missingness will be displayed in the final scoring table.\n\n\n\n***Fine-tuned Scores: \n\n\n========  =========  =====\nvariable  interval   point\n========  =========  =====\nLab_H     <0.2         0  \n          [0.2,1)      3  \n          [1,3)       10  \n          [3,4)       16  \n          >=4         21  \n                          \nVital_E   <12          0  \n          [12,15)      5  \n          [15,22)     11  \n          [22,25)     16  \n          >=25        21  \n                          \nLab_K     <10          0  \n          [10,40)      6  \n          >=40        11  \n                          \nAge       <50          0  \n          [50,75)     10  \n          [75,90)     16  \n          >=90        22  \n                          \nVital_A   <59          0  \n          [59,70)      2  \n          [70,100)     6  \n          [100,112)   13  \n          >=113       16  \n          Unknown      8  \n                          \nLab_B     <10          0  \n          [10,17)      5  \n          >=17        10  \n========  =========  =====\n***Performance (based on validation set, after fine-tuning):\nIntegrated AUC by all time points: 0.8720738\nC_index:  0.7811834 \nThe AUC(t) are shown as bwlow:\n  time_point     AUC_t\n1          1 0.0002500\n2          3 1.0000000\n3          7 0.9914830\n4         14 0.9821608\n5         30 0.9427153\n6         60 0.8660457\n7         90 0.8599140"
  },
  {
    "objectID": "06-autoscore_ordinal.html",
    "href": "06-autoscore_ordinal.html",
    "title": "7  AutoScore for ordinal outcomes (AutoScore-Ordinal)",
    "section": "",
    "text": "AutoScore-Ordinal refers to the AutoScore framework for developing point-based scoring models for ordinal outcomes. Similar to the implementation described in Chapter 5 for binary outcomes, AutoScore-Ordinal is implemented by five functions: AutoScore_rank_Ordinal(), AutoScore_parsimony_Ordinal(), AutoScore_weighting_Ordinal(), AutoScore_fine_tuning_Ordinal() and AutoScore_testing_Ordinal().\nIn this chapter, we demonstrate the use of AutoScore-Ordinal to develop sparse risk scores for an ordinal outcome, adjust parameters to improve interpretability, assess the performance of the final model and generate a lookup table to easily predict risks for new data. To facilitate clinical applications, in the following sections we demonstrate AutoScore application with large and small datasets and with missing information.\nCite the following paper for AutoScore-Ordinal:\nSaffari SE, Ning Y, Feng X, Chakraborty B, Volovici V, Vaughan R, Ong ME, Liu N, AutoScore-Ordinal: An interpretable machine learning framework for generating scoring models for ordinal outcomes, arXiv:2202.08407 (https://doi.org/10.48550/arxiv.2202.08407)"
  },
  {
    "objectID": "06-autoscore_ordinal.html#demo1",
    "href": "06-autoscore_ordinal.html#demo1",
    "title": "7  AutoScore for ordinal outcomes (AutoScore-Ordinal)",
    "section": "7.1 Demo 1: Development with large sample",
    "text": "7.1 Demo 1: Development with large sample\nIn Demo 1, we demonstrate the use of AutoScore-Ordinal on a dataset with 20,000 observations using split-sample approach (i.e., to randomly divide the full dataset into training, validation and test sets) for model development.\n\n\n\n\n\n\nImportant\n\n\n\n\nBefore proceeding, follow the steps in Chapter 4 to ensure all data requirements are met.\nRefer to Chapter 3 for how to generate simple descriptive statistics before building prediction models.\n\n\n\n\n7.1.1 Prepare training, validation, and test datasets\n\nlibrary(AutoScore)\ndata(\"sample_data_ordinal\")\ndim(sample_data_ordinal)\n\n[1] 20000    21\n\nhead(sample_data_ordinal)\n\n  label Age Gender Util_A Util_B Util_C    Util_D Comorb_A Comorb_B Comorb_C\n1     1  63 FEMALE     P2      0   0.00 3.5933333        0        0        0\n2     1  41 FEMALE     P2      0   0.96 3.6288889        0        0        0\n3     1  86   MALE     P1      0   0.00 2.6502778        0        0        0\n4     1  51   MALE     P2      0   0.00 4.9711111        0        0        0\n5     1  23 FEMALE     P1      0   0.00 0.5352778        0        0        0\n6     1  32 FEMALE     P2      0   4.13 4.4008333        0        0        0\n  Comorb_D Comorb_E Lab_A Lab_B Lab_C Vital_A Vital_B Vital_C Vital_D Vital_E\n1        0        0   117   3.9   136      91      19     100      70     152\n2        1        0   500   3.6   114      91      16     100      70     147\n3        0        0    72   4.1   136     100      18      99      65     126\n4        0        0    67   5.0   122      73      17      97      46     100\n5        0        0  1036   4.1   138      74      18      98      89     114\n6        0        0   806   4.1   136      77      18      98      74     157\n  Vital_F\n1    25.7\n2    22.6\n3    25.7\n4    24.9\n5    25.7\n6    25.3\n\ncheck_data_ordinal(sample_data_ordinal)\n\n\nYour dataset doesn't have any missing values.\n\n\nData type check passed.\n\n\n\nOption 1: Prepare three separate datasets to train, validate, and test models.\nOption 2: Use demo codes below to randomly split your dataset into training, validation, and test datasets (70%, 10%, 20%, respectively), possibly stratified by outcome categories (strat_by_label = TRUE) to ensure they are well represented in all three datasets.\n\n\nset.seed(4)\nout_split <- split_data(data = sample_data_ordinal, ratio = c(0.7, 0.1, 0.2), \n                        strat_by_label = TRUE)\ntrain_set <- out_split$train_set\nvalidation_set <- out_split$validation_set\ntest_set <- out_split$test_set\n\n\n\n7.1.2 STEP(i): Generate variable ranking list\n\nAutoScore-Ordinal Module 1\n\n\nntree: Number of trees in the random forest algorithm (Default: 100).\n\n\nranking <- AutoScore_rank_Ordinal(train_set = train_set, ntree = 100)\n\nThe ranking based on variable importance was shown below for each variable: \n   Util_D     Lab_A   Vital_F   Vital_A       Age   Vital_E   Vital_D     Lab_B \n413.60631 379.51127 378.30195 372.84319 372.68880 364.51371 339.60643 296.86038 \n    Lab_C    Util_C    Util_B   Vital_C   Vital_B  Comorb_A    Util_A    Gender \n279.47643 244.28653 201.34337 186.47331 168.45639 115.28191  98.78811  51.88705 \n Comorb_B  Comorb_D  Comorb_C  Comorb_E \n 41.11154  32.31979  17.64803  11.87098 \n\n\n\n\n\n\n\n7.1.3 STEP(ii): Select model with parsimony plot\n\nAutoScore-Ordinal Modules 2+3+4\n\n\nn_min: Minimum number of selected variables (Default: 1).\nn_max: Maximum number of selected variables (Default: 20).\ncategorize: Methods for categorizing continuous variables. Options include \"quantile\" or \"kmeans\" (Default: \"quantile\").\nquantiles: Predefined quantiles to convert continuous variables to categorical ones (Default: c(0, 0.05, 0.2, 0.8, 0.95, 1)). Available if categorize = \"quantile\".\nmax_cluster: The max number of cluster (Default: 5). Available if categorize = \"kmeans\".\nmax_score: Maximum total score (Default: 100).\nauc_lim_min: y-axis limits (min) of the parsimony plot (Default: 0.5)\nauc_lim_max: y-axis limits (max) of the parsimony plot (Default: \"adaptive\")\nlink: link function in the ordinal regression, which affects predictive performance. Options include \"logit\" (for proportional odds model), \"cloglog\" (for proportional hazard model) and \"probit\" (Default: \"logit\"). Use the same link throughout descriptive analysis and model building steps.\n\n\nmAUC <- AutoScore_parsimony_Ordinal(\n  train_set = train_set, validation_set = validation_set, \n  rank = ranking, link = \"logit\", max_score = 100, n_min = 1, n_max = 20,\n  categorize = \"quantile\", quantiles = c(0, 0.05, 0.2, 0.8, 0.95, 1), \n  auc_lim_min = 0\n)\n\nSelect 1 variables:  Mean area under the curve: 0.4555607 \nSelect 2 variables:  Mean area under the curve: 0.5110174 \nSelect 3 variables:  Mean area under the curve: 0.5780548 \nSelect 4 variables:  Mean area under the curve: 0.5912554 \nSelect 5 variables:  Mean area under the curve: 0.6685143 \nSelect 6 variables:  Mean area under the curve: 0.672106 \nSelect 7 variables:  Mean area under the curve: 0.6690071 \nSelect 8 variables:  Mean area under the curve: 0.6710102 \nSelect 9 variables:  Mean area under the curve: 0.6706072 \nSelect 10 variables:  Mean area under the curve: 0.6721932 \nSelect 11 variables:  Mean area under the curve: 0.7003498 \nSelect 12 variables:  Mean area under the curve: 0.6995013 \nSelect 13 variables:  Mean area under the curve: 0.6994186 \nSelect 14 variables:  Mean area under the curve: 0.7476355 \nSelect 15 variables:  Mean area under the curve: 0.7489346 \nSelect 16 variables:  Mean area under the curve: 0.7448716 \nSelect 17 variables:  Mean area under the curve: 0.744752 \nSelect 18 variables:  Mean area under the curve: 0.744752 \nSelect 19 variables:  Mean area under the curve: 0.745261 \nSelect 20 variables:  Mean area under the curve: 0.7472124 \n\n\n\n\n\n\nDetermine the optimal number of variables (num_var) based on the parsimony plot obtained in STEP(ii).\nThe final list of variables is the first num_var variables in the ranked list ranking obtained in STEP(i).\nOptional: User can adjust the finally included variables final_variables based on the clinical preferences and knowledge.\n\n\n# Example 1: Top 5 variables are selected\nnum_var <- 5\nfinal_variables <- names(ranking[1:num_var])\n\n# Example 2: Top 14 variables are selected\nnum_var <- 14\nfinal_variables <- names(ranking[1:num_var])\n\n# Example 3: Top 5 variables, the 11th and 14th variable are selected\nfinal_variables <- names(ranking[c(1:5, 11, 14)])\n\n\n\n7.1.4 STEP(iii): Generate initial scores with final variables\n\nRe-run AutoScore-Ordinal Modules 2+3\n\n\nGenerate cut_vec with current cutoffs of continuous variables, which can be fine-tuned in STEP(iv).\nPerformance of resulting scores is evaluated using the mean AUC across dichotomous classifications (mAUC), with 95% CI computed using bootstrap (Default: n_boot = 100 bootstrap samples). Setting n_boot = 1 disables bootstrap and reports mAUC without CI.\n\n\ncut_vec <- AutoScore_weighting_Ordinal(\n  train_set = train_set, validation_set = validation_set, \n  final_variables = final_variables, link = \"logit\", max_score = 100,\n  categorize = \"quantile\", quantiles = c(0, 0.05, 0.2, 0.8, 0.95, 1), \n  n_boot = 100\n)\n\n****Included Variables: \n  variable_name\n1        Util_D\n2         Lab_A\n3       Vital_F\n4       Vital_A\n5           Age\n6        Util_B\n7      Comorb_A\n****Initial Scores: \n\n\n========  ============  =====\nvariable  interval      point\n========  ============  =====\nUtil_D    <0.652          7  \n          [0.652,1.32)    7  \n          [1.32,3.93)     2  \n          [3.93,5.93)     1  \n          >=5.93          0  \n                             \nLab_A     <46             6  \n          [46,61)         0  \n          [61,134)        1  \n          [134,584)       8  \n          >=584           6  \n                             \nVital_F   <16.7           8  \n          [16.7,20.5)     4  \n          [20.5,25.4)     0  \n          [25.4,28.1)     1  \n          >=28.1          4  \n                             \nVital_A   <58             2  \n          [58,68)         0  \n          [68,97)         3  \n          [97,113)        6  \n          >=113          13  \n                             \nAge       <27             0  \n          [27,46)         4  \n          [46,78)        14  \n          [78,87)        18  \n          >=87           21  \n                             \nUtil_B    <1              0  \n          [1,4)          10  \n          >=4            21  \n                             \nComorb_A  0               0  \n          1              22  \n========  ============  =====\n***Performance (based on validation set):\nmAUC: 0.7402     95% CI: 0.7159-0.7627 (from 100 bootstrap samples)\n***The cutoffs of each variables generated by the AutoScore-Ordinal are saved in cut_vec. You can decide whether to revise or fine-tune them \n\n\n\n\n7.1.5 STEP(iv): Fine-tune initial score from STEP(iii)\n\nAutoScore-Ordinal Module 5 & Re-run AutoScore-Ordinal Modules 2+3\n\n\nRevise cut_vec with domain knowledge to update the scoring table (AutoScore-Ordinal Module 5).\nRe-run AutoScore-Ordinal Modules 2+3 to generate the updated scores.\nUsers can choose any cutoff values and/or any number of categories, but are suggested to choose numbers close to the automatically determined values.\n\n\n## For example, we have current cutoffs of continuous variable: Age \n## ==============  ===========  =====\n## variable        interval     point\n## ==============  ===========  =====\n## Age                 <27          0  \n##                     [27,46)      2  \n##                     [46,78)     12  \n##                     [78,87)     16 \n##                     >=87        19 \n\n\nCurrent cutoffs:c(27, 46, 78, 87). We can fine tune the cutoffs as follows:\n\n\n# Example 1: rounding to a nice number\ncut_vec$Age <- c(25, 45, 75, 85)\n\n# Example 2: changing cutoffs according to clinical knowledge or preference \ncut_vec$Age <- c(25, 50, 75, 85)\n\n# Example 3: combining categories\ncut_vec$Age <- c(45, 75, 85)\n\n\nmAUC and 95% bootstrap CI (Default: n_boot = 100 bootstrap samples) are reported after fine-tuning.\n\n\ncut_vec$Util_D <- c(2 / 3, 4 / 3, 4, 6)\ncut_vec$Vital_F <- c(17, 20, 25, 28)\ncut_vec$Vital_A <- c(60, 70, 95, 115)\ncut_vec$Lab_A <- c(45, 60, 135, 595)\ncut_vec$Age <- c(25, 45, 75, 85)\nscoring_table <- AutoScore_fine_tuning_Ordinal(\n  train_set = train_set, validation_set = validation_set,\n  final_variables = final_variables, cut_vec = cut_vec,\n  max_score = 100, n_boot = 100\n)\n\n***Fine-tuned Scores: \n\n\n========  ============  =====\nvariable  interval      point\n========  ============  =====\nUtil_D    <0.667          7  \n          [0.667,1.33)    7  \n          [1.33,4)        2  \n          [4,6)           1  \n          >=6             0  \n                             \nLab_A     <45             7  \n          [45,60)         0  \n          [60,135)        1  \n          [135,595)       8  \n          >=595           6  \n                             \nVital_F   <17             8  \n          [17,20)         4  \n          [20,25)         0  \n          [25,28)         0  \n          >=28            4  \n                             \nVital_A   <60             0  \n          [60,70)         0  \n          [70,95)         2  \n          [95,115)        5  \n          >=115          13  \n                             \nAge       <25             0  \n          [25,45)         4  \n          [45,75)        14  \n          [75,85)        19  \n          >=85           22  \n                             \nUtil_B    <1              0  \n          [1,4)          10  \n          >=4            21  \n                             \nComorb_A  0               0  \n          1              22  \n========  ============  =====\n***Performance (based on Validation Set, after fine-tuning):\nmAUC: 0.7466     95% CI: 0.7230-0.7728 (from 100 bootstrap samples)\n\n\n\n\n7.1.6 STEP(v): Evaluate final risk scores on test dataset\n\nAutoScore-Ordinal Module 6\n\n\nmAUC and generalised c-index are reported for the test set, with 95% bootstrap CI (Default: n_boot = 100 bootstrap samples).\n\n\npred_score <- AutoScore_testing_Ordinal(\n  test_set = test_set, link = \"logit\", final_variables = final_variables, \n  cut_vec = cut_vec, scoring_table = scoring_table, \n  with_label = TRUE, n_boot = 100\n)\n\n***Performance using AutoScore-Ordinal (based on unseen test Set):\nmAUC: 0.7552     95% CI: 0.7346-0.7785 (from 100 bootstrap samples)\nGeneralised c-index: 0.7267      95% CI: 0.7097-0.7492 (from 100 bootstrap samples)\n\nhead(pred_score)\n\n  pred_score Label\n1         40     1\n2         35     1\n3         29     1\n4         26     1\n5         33     1\n6         22     1\n\n\n\nUsers can compute mAUC and generalised c-index (with 95% bootstrap CI) for previously saved pred_score.\n\n\nprint_performance_ordinal(\n  label = pred_score$Label, score = pred_score$pred_score, \n  n_boot = 100, report_cindex = TRUE\n)\n\nmAUC: 0.7552     95% CI: 0.7373-0.7791 (from 100 bootstrap samples)\nGeneralised c-index: 0.7267      95% CI: 0.7098-0.7421 (from 100 bootstrap samples)\n\n\n\nUsers can use pred_score for further analysis or save it as CSV to other software.\n\n\nwrite.csv(pred_score, file = \"pred_score.csv\")"
  },
  {
    "objectID": "06-autoscore_ordinal.html#prediction-lookup-table",
    "href": "06-autoscore_ordinal.html#prediction-lookup-table",
    "title": "7  AutoScore for ordinal outcomes (AutoScore-Ordinal)",
    "section": "7.2 Prediction: lookup table",
    "text": "7.2 Prediction: lookup table\n\nA lookup table maps scores to predicted risks, visualised using an interactive figure below.\npoint_size: Size of points indicating all attainable scores (Default: 0.5).\n\n\nplot_predicted_risk(pred_score = pred_score, max_score = 100, \n                    final_variables = final_variables, \n                    scoring_table = scoring_table, point_size = 1)\n\n\n\n\n\n\nGiven the proportion of subjects for each score value (see figure above), select reasonable score breaks (Default: 5, 10, 15, …, 70) to report the average predicted risk within each score interval, which can be used as lookup table to predict risk for a new subject.\nWhen selecting score breaks, avoid creating score intervals with too few observations.\n\n\nlookup_table_ordinal(pred_score = pred_score, \n                     score_breaks = seq(from = 5, to = 70, by = 5), \n                     digits = 4)\n\n\n\n\n\n\n\n\n\n\nScore\nPredicted risk, category 1\nPredicted risk, category 2\nPredicted risk, category 3\n\n\n\n\n[0,5]\n0.9742\n0.0192\n0.0065\n\n\n(5,10]\n0.9617\n0.0285\n0.0098\n\n\n(10,15]\n0.9454\n0.0404\n0.0142\n\n\n(15,20]\n0.9226\n0.0569\n0.0205\n\n\n(20,25]\n0.8914\n0.0791\n0.0295\n\n\n(25,30]\n0.8497\n0.1081\n0.0423\n\n\n(30,35]\n0.7956\n0.1441\n0.0602\n\n\n(35,40]\n0.7284\n0.1864\n0.0851\n\n\n(40,45]\n0.6489\n0.2321\n0.1190\n\n\n(45,50]\n0.5602\n0.2758\n0.1640\n\n\n(50,55]\n0.4675\n0.3109\n0.2216\n\n\n(55,60]\n0.3769\n0.3307\n0.2924\n\n\n(60,65]\n0.2942\n0.3309\n0.3749\n\n\n(65,70]\n0.2231\n0.3116\n0.4653\n\n\n(70,100]\n0.0840\n0.1718\n0.7442"
  },
  {
    "objectID": "06-autoscore_ordinal.html#demo2",
    "href": "06-autoscore_ordinal.html#demo2",
    "title": "7  AutoScore for ordinal outcomes (AutoScore-Ordinal)",
    "section": "7.3 Demo 2: Development with small sample",
    "text": "7.3 Demo 2: Development with small sample\nIn Demo 2, we demonstrate AutoScore-Ordinal application for a small dataset with 5000 samples using cross-validation.\n\n7.3.1 Get small dataset with 5000 samples\nFor demonstration purpose, randomly sample a small dataset with 5000 samples from original sample data, stratified by the outcome.\n\np <- 5000 / nrow(sample_data_ordinal)\nset.seed(4)\nsample_data_ordinal_small <- split_data(data = sample_data_ordinal, \n                                        ratio = c(p, 0, 1 - p), \n                                        strat_by_label = TRUE)[[1]]\n\n\n\n7.3.2 Prepare training, validation, and test datasets\n\nOption 1: Prepare two separate datasets to train and test models.\nOption 2: Use demo codes below to randomly split your dataset into training and test datasets (70% and 30%, respectively). For cross-validation, train_set is equal to validation_set and the ratio of validation_set should be 0. Then cross-validation will be implemented in the STEP(ii), AutoScore_parsimony_Ordinal().\n\n\nset.seed(4)\nout_split <- split_data(data = sample_data_ordinal_small, \n                        ratio = c(0.7, 0, 0.3), cross_validation = TRUE,\n                        strat_by_label = TRUE)\ntrain_set <- out_split$train_set\nvalidation_set <- out_split$validation_set\ntest_set <- out_split$test_set\n\n\n\n7.3.3 STEP(i): Generate variable ranking list\n\nAutoScore-Ordinal Module 1\n\n\nntree: Number of trees in the random forest algorithm (Default: 100).\n\n\nranking <- AutoScore_rank_Ordinal(train_set = train_set, ntree = 100)\n\nThe ranking based on variable importance was shown below for each variable: \n      Age     Lab_A    Util_D   Vital_A   Vital_F   Vital_E   Vital_D     Lab_B \n98.791839 98.073161 97.021662 93.282141 92.469166 91.677271 82.051893 72.462999 \n    Lab_C    Util_B    Util_C   Vital_C   Vital_B    Util_A  Comorb_A    Gender \n70.507041 57.778464 57.270171 45.468903 43.633031 26.120379 25.414958 13.405373 \n Comorb_B  Comorb_D  Comorb_C  Comorb_E \n 9.595971  7.320633  4.477519  2.993603 \n\n\n\n\n\n\n\n7.3.4 STEP(ii): Select the best model with parsimony plot\n\nAutoScore-Ordinal Modules 2+3+4\n\n\nn_min: Minimum number of selected variables (Default: 1).\nn_max: Maximum number of selected variables (Default: 20).\ncategorize: Methods for categorizing continuous variables. Options include \"quantile\" or \"kmeans\" (Default: \"quantile\").\nquantiles: Predefined quantiles to convert continuous variables to categorical ones (Default: c(0, 0.05, 0.2, 0.8, 0.95, 1)). Available if categorize = \"quantile\".\nmax_cluster: The max number of cluster (Default: 5). Available if categorize = \"kmeans\".\nmax_score: Maximum total score (Default: 100).\nauc_lim_min: y-axis limits (min) of the parsimony plot (Default: 0.5)\nauc_lim_max: y-axis limits (max) of the parsimony plot (Default: \"adaptive\")\nlink: link function in the ordinal regression, which affects predictive performance. Options include \"logit\" (for proportional odds model), \"cloglog\" (for proportional hazard model) and \"probit\" (Default: \"logit\"). Use the same link throughout descriptive analysis and model building steps.\n\n\nmAUC <- AutoScore_parsimony_Ordinal(\n  train_set = train_set, validation_set = validation_set, link = \"logit\",\n  rank = ranking, max_score = 100, n_min = 1, n_max = 20,\n  categorize = \"quantile\", quantiles = c(0, 0.05, 0.2, 0.8, 0.95, 1), \n  auc_lim_min = 0\n)\n\nSelect 1 variables:  Mean area under the curve: 0.4508142 \nSelect 2 variables:  Mean area under the curve: 0.5108712 \nSelect 3 variables:  Mean area under the curve: 0.6309579 \nSelect 4 variables:  Mean area under the curve: 0.6432916 \nSelect 5 variables:  Mean area under the curve: 0.6565442 \nSelect 6 variables:  Mean area under the curve: 0.6558072 \nSelect 7 variables:  Mean area under the curve: 0.6567401 \nSelect 8 variables:  Mean area under the curve: 0.6570538 \nSelect 9 variables:  Mean area under the curve: 0.6583098 \nSelect 10 variables:  Mean area under the curve: 0.7177025 \nSelect 11 variables:  Mean area under the curve: 0.7177396 \nSelect 12 variables:  Mean area under the curve: 0.7179181 \nSelect 13 variables:  Mean area under the curve: 0.7187799 \nSelect 14 variables:  Mean area under the curve: 0.7215945 \nSelect 15 variables:  Mean area under the curve: 0.7641033 \nSelect 16 variables:  Mean area under the curve: 0.7646344 \nSelect 17 variables:  Mean area under the curve: 0.7643153 \nSelect 18 variables:  Mean area under the curve: 0.7641064 \nSelect 19 variables:  Mean area under the curve: 0.7643961 \nSelect 20 variables:  Mean area under the curve: 0.7647775 \n\n\n\n\n\n\nDetermine the optimal number of variables (num_var) based on the parsimony plot obtained in STEP(ii).\nThe final list of variables is the first num_var variables in the ranked list ranking obtained in STEP(i).\nOptional: User can adjust the finally included variables final_variables based on the clinical preferences and knowledge.\n\n\n# Example 1: Top 6 variables are selected\nnum_var <- 6\nfinal_variables <- names(ranking[1:num_var])\n\n# Example 2: Top 14 variables are selected\nnum_var <- 14\nfinal_variables <- names(ranking[1:num_var])\n\n# Example 3: Top 3 variables, the 6th, 11th and 14th variable are selected\nfinal_variables <- names(ranking[c(1:3, 6, 11, 14)])\n\n\n\n7.3.5 STEP(iii): Generate initial scores with final variables\n\nRe-run AutoScore-Ordinal Modules 2+3\n\n\nGenerate cut_vec with current cutoffs of continuous variables, which can be fine-tuned in STEP(iv).\nPerformance of resulting scores is evaluated using the mean AUC across dichotomous classifications (mAUC), with 95% CI computed using bootstrap (Default: n_boot = 100 bootstrap samples). Setting n_boot = 1 disables bootstrap and reports mAUC without CI.\n\n\ncut_vec <- AutoScore_weighting_Ordinal(\n  train_set = train_set, validation_set = validation_set, \n  final_variables = final_variables, \n  max_score = 100,\n  categorize = \"quantile\",\n  quantiles = c(0, 0.05, 0.2, 0.8, 0.95, 1), n_boot = 100\n)\n\n****Included Variables: \n  variable_name\n1           Age\n2         Lab_A\n3        Util_D\n4       Vital_E\n5        Util_C\n6        Util_A\n****Initial Scores: \n\n\n========  ===========  =====\nvariable  interval     point\n========  ===========  =====\nAge       <27            0  \n          [27,46)       19  \n          [46,78)       30  \n          [78,87)       39  \n          >=87          44  \n                            \nLab_A     <46           17  \n          [46,61)        0  \n          [61,136)       1  \n          [136,608)     13  \n          >=608         13  \n                            \nUtil_D    <0.64          9  \n          [0.64,1.3)    11  \n          [1.3,3.83)     6  \n          [3.83,5.74)    7  \n          >=5.74         0  \n                            \nVital_E   <98           14  \n          [98,111)       7  \n          [111,153)      4  \n          [153,179)      0  \n          >=179          1  \n                            \nUtil_C    <4.95          4  \n          [4.95,13.6)    7  \n          >=13.6         0  \n                            \nUtil_A    P1             6  \n          P2             0  \n          P3 and P4      3  \n========  ===========  =====\n***Performance (based on validation set):\nmAUC: 0.6400     95% CI: 0.6087-0.6623 (from 100 bootstrap samples)\n***The cutoffs of each variables generated by the AutoScore-Ordinal are saved in cut_vec. You can decide whether to revise or fine-tune them \n\n\n\n\n7.3.6 STEP(iv): Fine-tune initial score from STEP(iii)\n\nAutoScore-Ordinal Module 5 & Re-run AutoScore-Ordinal Modules 2+3\n\nSimilar to the large sample scenario, users can fine-tune cutoff values for continuous variables, rerun AutoScore-Ordinal Modules 2+3 to generate the updated scores and assess model performance of the updated model using mAUC.\n\ncut_vec$Util_D <- c(2 / 3, 4 / 3, 4, 6)\ncut_vec$Lab_A <- c(45, 60, 135, 595)\ncut_vec$Age <- c(25, 45, 75, 85)\ncut_vec$Vital_A <- c(60, 70, 95, 115)\nscoring_table <- AutoScore_fine_tuning_Ordinal(\n  train_set = train_set, validation_set = validation_set, link = \"logit\",\n  final_variables = final_variables, cut_vec = cut_vec, max_score = 100, \n  n_boot = 100\n)\n\n***Fine-tuned Scores: \n\n\n========  ============  =====\nvariable  interval      point\n========  ============  =====\nAge       <25             0  \n          [25,45)        16  \n          [45,75)        28  \n          [75,85)        37  \n          >=85           42  \n                             \nLab_A     <45            18  \n          [45,60)         0  \n          [60,135)        1  \n          [135,595)      12  \n          >=595          12  \n                             \nUtil_D    <0.667         10  \n          [0.667,1.33)   13  \n          [1.33,4)        7  \n          [4,6)          10  \n          >=6             0  \n                             \nVital_E   <98            15  \n          [98,111)        7  \n          [111,153)       4  \n          [153,179)       0  \n          >=179           1  \n                             \nUtil_C    <4.95           4  \n          [4.95,13.6)     7  \n          >=13.6          0  \n                             \nUtil_A    P1              4  \n          P2              0  \n          P3 and P4       3  \n========  ============  =====\n***Performance (based on Validation Set, after fine-tuning):\nmAUC: 0.6447     95% CI: 0.6146-0.6648 (from 100 bootstrap samples)\n\n\n\n\n7.3.7 STEP(v): Evaluate final risk scores on test dataset\n\nAutoScore-Ordinal Module 6\n\n\nmAUC and generalised c-index are reported for the test set, with 95% bootstrap CI (Default: n_boot = 100 bootstrap samples).\n\n\npred_score <- AutoScore_testing_Ordinal(\n  test_set = test_set, link = \"logit\",\n  final_variables = final_variables, cut_vec = cut_vec, \n  scoring_table = scoring_table, \n  with_label = TRUE, n_boot = 100\n)\n\n***Performance using AutoScore-Ordinal (based on unseen test Set):\nmAUC: 0.6409     95% CI: 0.6060-0.6837 (from 100 bootstrap samples)\nGeneralised c-index: 0.6098      95% CI: 0.5730-0.6494 (from 100 bootstrap samples)\n\nhead(pred_score)\n\n  pred_score Label\n1         43     1\n2         59     1\n3         60     1\n4         55     1\n5         37     1\n6         15     1\n\n\n\nUsers can use pred_score for further analysis or save it as CSV to other software.\n\n\nwrite.csv(pred_score, file = \"pred_score.csv\")"
  },
  {
    "objectID": "06-autoscore_ordinal.html#development-on-dataset-with-missing-values-informative-missing",
    "href": "06-autoscore_ordinal.html#development-on-dataset-with-missing-values-informative-missing",
    "title": "7  AutoScore for ordinal outcomes (AutoScore-Ordinal)",
    "section": "7.4 Development on dataset with missing values (informative missing)",
    "text": "7.4 Development on dataset with missing values (informative missing)\n\n\n\n\n\nFor demonstration, we manually induce missing to two variables in the sample data (20% in Lab_A and 60% in Vital_A.), assuming values close to normal range (i.e., median value in the complete data) are more likely to be missing.\nSuch missing pattern is more common in laboratory tests and vital signs, where doctors may not choose to order a test if there is no reason to suspect deviation from normal range.\n\n\nsample_data_ordinal_missing <- AutoScore:::induce_informative_missing(\n  df = sample_data_ordinal, vars_to_induce = c(\"Lab_A\", \"Vital_A\"), \n  prop_missing = c(0.2, 0.6)\n)\n\n\nAs demonstrated below, such missingness pattern has little impact on variable ranking and cutoff values.\nMissing is treated as category ‘Unknown’ in the scoring table.\n\n\nset.seed(4)\nout_split <- split_data(data = sample_data_ordinal_missing, \n                        ratio = c(0.7, 0.1, 0.2), strat_by_label = TRUE)\ntrain_set <- out_split$train_set\nvalidation_set <- out_split$validation_set\ntest_set <- out_split$test_set\nmAUC <- AutoScore_parsimony_Ordinal(\n  train_set = train_set, validation_set = validation_set, link = \"logit\",\n  rank = ranking, max_score = 100, n_min = 1, n_max = 20,\n  categorize = \"quantile\", quantiles = c(0, 0.05, 0.2, 0.8, 0.95, 1), \n  auc_lim_min = 0\n)\n\nSelect 1 variables:  Mean area under the curve: 0.3788152 \nSelect 2 variables:  Mean area under the curve: 0.5190094 \nSelect 3 variables:  Mean area under the curve: 0.6440789 \nSelect 4 variables:  Mean area under the curve: 0.6512936 \nSelect 5 variables:  Mean area under the curve: 0.6585614 \nSelect 6 variables:  Mean area under the curve: 0.6606368 \nSelect 7 variables:  Mean area under the curve: 0.6590194 \nSelect 8 variables:  Mean area under the curve: 0.6604489 \nSelect 9 variables:  Mean area under the curve: 0.6622922 \nSelect 10 variables:  Mean area under the curve: 0.6922552 \nSelect 11 variables:  Mean area under the curve: 0.691542 \nSelect 12 variables:  Mean area under the curve: 0.6906316 \nSelect 13 variables:  Mean area under the curve: 0.691181 \nSelect 14 variables:  Mean area under the curve: 0.6913957 \nSelect 15 variables:  Mean area under the curve: 0.7372037 \nSelect 16 variables:  Mean area under the curve: 0.7373817 \nSelect 17 variables:  Mean area under the curve: 0.7371645 \nSelect 18 variables:  Mean area under the curve: 0.737344 \nSelect 19 variables:  Mean area under the curve: 0.7370222 \nSelect 20 variables:  Mean area under the curve: 0.7370222 \n\n\n\n\nfinal_variables <- names(ranking[c(1:3, 6, 11, 14)])\ncut_vec <- AutoScore_weighting_Ordinal(\n  train_set = train_set, validation_set = validation_set, \n  final_variables = final_variables, \n  max_score = 100,\n  categorize = \"quantile\",\n  quantiles = c(0, 0.05, 0.2, 0.8, 0.95, 1), n_boot = 100\n)\n\n****Included Variables: \n  variable_name\n1           Age\n2         Lab_A\n3        Util_D\n4       Vital_E\n5        Util_C\n6        Util_A\n****Initial Scores: \n\n\n========  ============  =====\nvariable  interval      point\n========  ============  =====\nAge       <27             0  \n          [27,46)         6  \n          [46,78)        28  \n          [78,87)        36  \n          >=87           43  \n                             \nLab_A     <44            13  \n          [44,59)         1  \n          [59,164)        2  \n          [164,677)      16  \n          >=678          11  \n          Unknown         0  \n                             \nUtil_D    <0.652         16  \n          [0.652,1.32)   15  \n          [1.32,3.93)     5  \n          [3.93,5.93)     2  \n          >=5.93          0  \n                             \nVital_E   <99            21  \n          [99,112)       16  \n          [112,153)       7  \n          [153,179)       4  \n          >=179           0  \n                             \nUtil_C    <5.01           2  \n          [5.01,14.7)     0  \n          >=14.7          3  \n                             \nUtil_A    P1              0  \n          P2              1  \n          P3 and P4       0  \n========  ============  =====\n***Performance (based on validation set):\nmAUC: 0.6500     95% CI: 0.6029-0.6831 (from 100 bootstrap samples)\n***The cutoffs of each variables generated by the AutoScore-Ordinal are saved in cut_vec. You can decide whether to revise or fine-tune them"
  },
  {
    "objectID": "07-autoscore_shapleyvic.html",
    "href": "07-autoscore_shapleyvic.html",
    "title": "8  AutoScore-ShapleyVIC for binary outcomes",
    "section": "",
    "text": "Chapter 5 described the AutoScore framework for binary outcomes, using the random forest or AUC-based approach to rank and select variables. This chapter describes an integrated AutoScore-ShapleyVIC framework that uses Shapley variable importance cloud (ShapleyVIC), a recently developed interpretable machine learning approach, for interpretable and robust variable ranking, and the subsequent AutoScore pipelines (AutoScore_parsimony(), AutoScore_weighting(), AutoScore_fine_tuning() and AutoScore_testing()) to develop sparse clinical risk scores.\nIn this chapter, we demonstrate the use of AutoScore-ShapleyVIC using the same simulated dataset as in Chapter 5, Demo 1, focusing on the variable ranking step using ShapleyVIC and the difference in the resulting parsimony plot.\nCite the following papers for AutoScore-ShapleyVIC:\nNing Y, Ong ME, Chakraborty B, Goldstein BA, Ting DS, Vaughan R, Liu N. Shapley variable importance cloud for interpretable machine learning. Patterns 2022 (https://doi.org/10.1016/j.patter.2022.100452)\nNing Y, Li S, Ong ME, Xie F, Chakraborty B, Ting DS, Liu N. A novel interpretable machine learning system to generate clinical risk scores: An application for predicting early mortality or unplanned readmission in a retrospective cohort study. PLOS Digit Health 1(6): e0000062. (https://doi.org/10.1371/journal.pdig.0000062)"
  },
  {
    "objectID": "07-autoscore_shapleyvic.html#step-1-prepare-training-validation-and-test-datasets",
    "href": "07-autoscore_shapleyvic.html#step-1-prepare-training-validation-and-test-datasets",
    "title": "8  AutoScore-ShapleyVIC for binary outcomes",
    "section": "8.1 Step 1: Prepare training, validation, and test datasets",
    "text": "8.1 Step 1: Prepare training, validation, and test datasets\n\nSame split-sample set-up as in Chapter 5, Demo 1.\n\n\nlibrary(AutoScore)\ndata(\"sample_data\")\nnames(sample_data)[names(sample_data) == \"Mortality_inpatient\"] <- \"label\"\ncheck_data(sample_data)\n\n\nYour dataset doesn't have any missing values.\n\n\nData type check passed.\n\n\n\nShapleyVIC requires binary outcomes to be coded as 0 and 1 or -1 and 1.\n\n\nsample_data$label <- as.numeric(sample_data$label == \"TRUE\")\n\n\nset.seed(4)\nout_split <- split_data(data = sample_data, ratio = c(0.7, 0.1, 0.2))\ntrain_set <- out_split$train_set\nvalidation_set <- out_split$validation_set\ntest_set <- out_split$test_set"
  },
  {
    "objectID": "07-autoscore_shapleyvic.html#step-2-variable-ranking-from-shapleyvic-analysis-of-nearly-optimal-models",
    "href": "07-autoscore_shapleyvic.html#step-2-variable-ranking-from-shapleyvic-analysis-of-nearly-optimal-models",
    "title": "8  AutoScore-ShapleyVIC for binary outcomes",
    "section": "8.2 Step 2: Variable ranking from ShapleyVIC analysis of nearly optimal models",
    "text": "8.2 Step 2: Variable ranking from ShapleyVIC analysis of nearly optimal models\n\n8.2.1 Install ShapleyVIC package from GitHub\n\ninstall.packages(\"devtools\")\nlibrary(devtools)\ninstall_github(repo = \"nliulab/ShapleyVIC\")\n\n\n\n8.2.2 Load R packages\n\nlibrary(car) # For VIF analysis of optimal model (needed for ShapleyVIC analysis)\nlibrary(ShapleyVIC)\nlibrary(tidyverse) # For convenient data manipulation and visualization\nlibrary(magrittr) # For convenient data manipulation\nlibrary(knitr)\n\n\n\n8.2.3 Train the optimal logistic regression model and check for colinearity\n\nm_optim_r <- glm(label ~ ., data = train_set, family = \"binomial\")\nvar_vif <- vif(m_optim_r)\n\n\n\n8.2.4 Generate a random set of 350 nearly optimal models\n\nset.seed(4)\nmodels <- draw_models(\n  coef_optim = coef(m_optim_r), coef_optim_var = vcov(m_optim_r),\n  x = train_set[, 1:21], y = train_set$label,\n  M = 800, u1 = 0.5, u2 = 18, epsilon = 0.05, n_final = 350\n)\n\n\n\n\nParameters u1 and u2 need tuning:\n\nu1 usually takes a small value around 0.5.\nu2 can take a large value for large data.\n\nAs detailed in the Methods section of the ShapleyVIC paper, these two values can be selected via grid search, using a smaller size of initial sample (e.g., M = 100) to reduce run time. Select values of u1 and u2 such that:\n\nAround 75% of initial samples were eligible.\nThe range of 1-1.05 times of minimum loss is well represented in the eligible models (see histogram above, generated by draw_models()).\n\n\n\n8.2.5 Compute ShapleyVIC values from nearly optimal models\nUse the first 3500 observations in the validation set to compute ShapleyVIC values. This step will be time consuming, and users are recommended to use parallel computing if the computer used has multiple cores (using the n_cores parameter).\nTo check the number of cores available, use the following code:\n\nlibrary(parallel)\nn_cores_total = detectCores(logical = TRUE)\n\nThe appropriate number of cores to use depends on the total memory available and the total number of variables. Users may start with n_cores = floor(n_cores_total / 2), inspect the total % of CPU used, and adjust n_cores to avoid 100% CPU usage (which may freeze the computation process). This example used 6 out of 8 cores on a Mac mini (M1, 2020) and took 50 hours. Users are recommended to save outputs from individual models to a folder for backup purpose, by using the output_folder parameter.\n\n# Create a python version of optimal model:\nm_optim <- logit_model_python(\n  x_train = train_set[, 1:21], y_train = train_set$label\n)\nset.seed(4)\n# Use the python version of optimal model to compute ShapleyVIC values:\ndf_shapley_vic <- compute_shapley_vic(\n  model_py = m_optim, var_vif = var_vif, var_vif_threshold = 2,\n  coef_mat = df_models[, setdiff(names(df_models), \"perf_metric\")], \n  perf_metric = df_models$perf_metric, \n  x_test = Out_split$validation_set[1:3500, x_names_display], \n  y_test = Out_split$validation_set$label[1:3500], \n  output_folder = \"shapley_vic_output\", # Results for each model is saved to this folder\n  n_cores = 6 \n)\n\n\n\n8.2.6 Variable important analysis using ShapleyVIC values\nBefore proceeding to develop scoring models, users can summarize and visualize the ShapleyVIC values to assess the contribution of the 20 candidate variables to the outcome.\n\ndf_shapley_vic_bar <- df_shapley_vic %$% summarise_shapley_vic(\n  val = shapley_vic_val, val_sd = sage_sd, var_names = var_names\n)\ndf_shapley_vic_bar %$% draw_bars(\n  val = val, val_lower = val_lower, val_upper = val_upper, var_names = Variable, \n  title = \"Overall variable importance across 350 nearly optimal models\"\n) \n\n\n\n\n\ndf_shapley_vic %$% draw_violins(\n  var_names = var_names, \n  var_ordering = levels(df_shapley_vic_bar$Variable),\n  val = shapley_vic_val, perf_metric = perf_metric, \n  title = \"Variable importance from 350 nearly optimal models\"\n)\n\n\n\n\n\n\n8.2.7 Generate ensemble variable ranking from ShapleyVIC\n\nUse summarise = TRUE to generate ensemble variable ranking by averaging variable ranks across the 350 nearly optimal models.\nExclude variables with non-significant overall importance to simplify subsequent model development, based on the 95% prediction interval of average ShapleyVIC values.\n\n\nval_ranks <- df_shapley_vic %$% rank_variables(\n  val = shapley_vic_val, val_sd = sage_sd, model_id = model_id, var_names = var_names, \n  summarise = TRUE, ties.method = \"min\"\n) %>% arrange(mean_rank) %>% \n  filter(Variable %in% df_shapley_vic_bar$Variable[which(df_shapley_vic_bar$val_lower > 0)])\nkable(val_ranks, digits = 1)\n\n\n\n\nVariable\nmean_rank\n\n\n\n\nAge\n1.0\n\n\nLab_H\n2.0\n\n\nVital_E\n3.3\n\n\nLab_K\n4.0\n\n\nVital_A\n5.4\n\n\nLab_B\n5.8\n\n\nVital_C\n8.7\n\n\nLab_C\n9.4\n\n\nLab_M\n9.9\n\n\nLab_E\n10.1\n\n\nLab_D\n10.5\n\n\nLab_L\n11.1\n\n\nVital_D\n13.0\n\n\nLab_J\n14.2\n\n\nVital_B\n14.3\n\n\nVital_F\n16.5\n\n\n\n\n\nSmaller value of mean_rank indicates higher importance."
  },
  {
    "objectID": "07-autoscore_shapleyvic.html#step-3-model-development-using-shapleyvic-based-ranking-and-autoscore-workflow",
    "href": "07-autoscore_shapleyvic.html#step-3-model-development-using-shapleyvic-based-ranking-and-autoscore-workflow",
    "title": "8  AutoScore-ShapleyVIC for binary outcomes",
    "section": "8.3 Step 3: Model development using ShapleyVIC-based ranking and AutoScore workflow",
    "text": "8.3 Step 3: Model development using ShapleyVIC-based ranking and AutoScore workflow\n\n8.3.1 Prepare variable ranking list for AutoScore\n\nranking <- val_ranks$mean_rank\nnames(ranking) <- val_ranks$Variable\n\n\n\n8.3.2 Select the best model with parsimony plot (AutoScore Modules 2+3+4)\n\nParsimony plot from ShapleyVIC-based variable ranking is smoother than that based on random forest.\n\n\nAUC <- AutoScore_parsimony(\n  train_set = train_set, validation_set = validation_set, \n  rank = ranking, max_score = 100, n_min = 1, n_max = length(ranking)\n)\n\nSelect 1 Variable(s):  Area under the curve: 0.6649\nSelect 2 Variable(s):  Area under the curve: 0.7466\nSelect 3 Variable(s):  Area under the curve: 0.7881\nSelect 4 Variable(s):  Area under the curve: 0.8009\nSelect 5 Variable(s):  Area under the curve: 0.8137\nSelect 6 Variable(s):  Area under the curve: 0.8268\nSelect 7 Variable(s):  Area under the curve: 0.8221\nSelect 8 Variable(s):  Area under the curve: 0.8227\nSelect 9 Variable(s):  Area under the curve: 0.8215\nSelect 10 Variable(s):  Area under the curve: 0.8185\nSelect 11 Variable(s):  Area under the curve: 0.8274\nSelect 12 Variable(s):  Area under the curve: 0.8294\nSelect 13 Variable(s):  Area under the curve: 0.8281\nSelect 14 Variable(s):  Area under the curve: 0.8261\nSelect 15 Variable(s):  Area under the curve: 0.8256\nSelect 16 Variable(s):  Area under the curve: 0.8322\n\n\n\n\n\nA feasible choice is to select the top 6 variables, Age, Lab_H, Vital_E, Lab_K, Vital_A, Lab_B, resulting in the same model as developed in Chapter 5.\n\ncut_vec <- AutoScore_weighting(\n  train_set = train_set, validation_set = validation_set, \n  final_variables = names(ranking)[1:6], max_score = 100\n)\n\n****Included Variables: \n  variable_name\n1           Age\n2         Lab_H\n3       Vital_E\n4         Lab_K\n5       Vital_A\n6         Lab_B\n****Initial Scores: \n\n\n========  ==========  =====\nvariable  interval    point\n========  ==========  =====\nAge       <35           0  \n          [35,49)       7  \n          [49,76)      17  \n          [76,89)      23  \n          >=89         27  \n                           \nLab_H     <0.2          0  \n          [0.2,1.1)     4  \n          [1.1,3.1)     9  \n          [3.1,4)      15  \n          >=4          18  \n                           \nVital_E   <12           0  \n          [12,15)       2  \n          [15,22)       7  \n          [22,25)      12  \n          >=25         15  \n                           \nLab_K     <8            0  \n          [8,42)        6  \n          [42,58)      11  \n          >=58         14  \n                           \nVital_A   <60           0  \n          [60,73)       1  \n          [73,98)       6  \n          [98,111)     10  \n          >=111        13  \n                           \nLab_B     <8.5          0  \n          [8.5,11.2)    4  \n          [11.2,17)     7  \n          [17,19.8)    10  \n          >=19.8       12  \n========  ==========  =====\n\n\n\n\n\n***Performance (based on validation set):\nAUC:  0.8268   95% CI: 0.7953-0.8583 (DeLong)\nBest score threshold: >= 57 \nOther performance indicators based on this score threshold: \nSensitivity: 0.8065\nSpecificity: 0.6775\nPPV:         0.1736\nNPV:         0.9766\n***The cutoffs of each variable generated by the AutoScore are saved in cut_vec. You can decide whether to revise or fine-tune them \n\n\nUsers can follow detailed steps in Chapter 5 for subsequent model fine-tuning and evaluation."
  }
]