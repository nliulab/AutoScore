############################################
## Four main peplines for AutoScore
AutoScore_rank <- function(TrainSet, ntree = 100) {
  set.seed(4)
  TrainSet$label <- as.factor(TrainSet$label)
  model <- randomForest(label ~ ., data = TrainSet, ntree = ntree, preProcess = "scale")

  # estimate variable importance
  importance <- importance(model, scale = F)

  # summarize importance
  b <- importance
  names(b) <- rownames(importance)
  b <- sort(b, decreasing = T)
  print(b)
  return(b)
}


AutoScore_parsimony <- function(TrainSet, ValidationSet, rank, nmin = 1, nmax = 20, probs = c(0, 0.05, 0.2, 0.8, 0.95, 1)) {
  s <- names(rank)
  AUC <- c()

  for (i in nmin:nmax) {
    print("Select the number of Variables")
    print(i)
    SD <- TrainSet[, c(s[1:i], "label")]
    ValidationSet1 <- ValidationSet[, c(s[1:i], "label")]

    # AutoScore Module 2 : cut numeric and transfer categories
    SDlist <- Dftransform(SD, ValidationSet1, probs = probs)
    SD2 <- SDlist[[1]]
    ValidationSet2 <- SDlist[[2]]
    # str(SD2) str(testSet2)

    # multivariable analysis after
    model <- glm(label ~ ., family = binomial(link = "logit"), data = SD2)
    y_validation <- ValidationSet2$label

    # AutoScore Module 3 : cut numeric and transfer categories
    coefVec <- coef(model)
    SD2 <- ChangeRef(SD2, coefVec)
    model <- glm(label ~ ., family = binomial(link = "logit"), data = SD2)
    # print(model) summary(model)
    coefVec <- coef(model)
    a <- round(coefVec/min(coefVec[-1]))
    myvec <- AddBaseline(SD2, a)

    ValidationSet3 <- AutoTest(ValidationSet2, myvec)
    ValidationSet3$TotalScore <- rowSums(subset(ValidationSet3, select = -label))
    y_validation <- ValidationSet3$label
    # PlotROCCurve(ValidationSet3$TotalScore,as.numeric(y_validation)-1)

    Modelroc <- roc(y_validation, ValidationSet3$TotalScore, quiet = T)
    print(auc(Modelroc))
    AUC <- c(AUC, auc(Modelroc))
  }

  names(AUC) <- nmin:nmax
  print("list of AUC values are shown below")
  print(data.frame(AUC))
  plot(AUC, main = "Parsimony plot on the Validation Set", xlab = "Number of Variables", ylab = "Area Under the Curve", col = "red",
       lwd = 2, type = "o")

  return(AUC)

}



AutoScore_weighting <- function(TrainSet, ValidationSet, FinalVariable, MaxScore = 100, probs = c(0, 0.05, 0.2, 0.8, 0.95, 1)) {
  SD <- TrainSet[, c(FinalVariable, "label")]
  ValidationSet1 <- ValidationSet[, c(FinalVariable, "label")]

  # AutoScore Module 2 : cut numeric and transfer categories
  SDlist <- Dftransform(SD, ValidationSet1, probs = probs, Print_categories = TRUE)
  SD2 <- SDlist[[1]]
  ValidationSet2 <- SDlist[[2]]
  CutVec1 <- SDlist[[3]]
  CutVec <- CutVec1
  for (i in 1:length(CutVec)) CutVec[[i]] <- CutVec[[i]][2:(length(CutVec[[i]]) - 1)]

  # AutoScore Module 3 : Score weighting
  model <- glm(label ~ ., family = binomial(link = "logit"), data = SD2)
  y_validation <- ValidationSet2$label
  coefVec <- coef(model)
  SD2 <- ChangeRef(SD2, coefVec)
  model <- glm(label ~ ., family = binomial(link = "logit"), data = SD2)
  # print(model)
  coefVec <- coef(model)
  a <- round(coefVec/min(coefVec[-1]))
  myvec <- AddBaseline(SD2, a)
  # print('The generated Scores are shown below') print(as.data.frame(myvec))

  total_max <- MaxScore
  total <- 0
  for (i in 1:length(FinalVariable)) total <- total + max(myvec[grepl(FinalVariable[i], names(myvec))])
  myvec <- round(myvec/(total/total_max))
  print("The generated Scores are shown below")
  print(as.data.frame(myvec))

  ## intermediate evaluation based on Validation Set
  ValidationSet3 <- AutoTest(ValidationSet2, myvec)
  ValidationSet3$TotalScore <- rowSums(subset(ValidationSet3, select = -label))
  y_validation <- ValidationSet3$label
  PlotROCCurve(ValidationSet3$TotalScore, as.numeric(y_validation) - 1)

  print("Performance using AutoScore (based on internal validation):")
  Modelroc <- roc(y_validation, ValidationSet3$TotalScore, quiet = T)
  print("AUC:")
  print(ci(Modelroc))
  print(auc(Modelroc))
  print("The best cutoff of using this score")
  print(coords(Modelroc, "best", ret = "threshold", transpose = TRUE))
  print("Other Performance indicators based on this cutoff: ")
  print(coords(Modelroc, "best", ret = c("specificity", "sensitivity", "accuracy", "npv", "ppv", "precision"), transpose = TRUE))
  print("the cut-offs generated by the AutoScore are shown as below. You can decide whether to revise or fine-tune them")
  print(CutVec)
  return(CutVec)
}


AutoScore_fine_tuning <- function(TrainSet, ValidationSet, FinalVariable, CutVec, MaxScore = 100) {
  SD <- TrainSet[, c(FinalVariable, "label")]
  ValidationSet1 <- ValidationSet[, c(FinalVariable, "label")]

  # AutoScore Module 2 : cut numeric and transfer categories(fix)
  SD2 <- Dftransform_fixed(SD, CutVec = CutVec)
  ValidationSet2 <- Dftransform_fixed(ValidationSet1, CutVec = CutVec)

  # AutoScore Module 3 : Score weighting
  model <- glm(label ~ ., family = binomial(link = "logit"), data = SD2)
  y_validation <- ValidationSet2$label
  coefVec <- coef(model)
  SD2 <- ChangeRef(SD2, coefVec)
  model <- glm(label ~ ., family = binomial(link = "logit"), data = SD2)
  # print(model) summary(model)
  coefVec <- coef(model)
  a <- round(coefVec/min(coefVec[-1]))
  myvec <- AddBaseline(SD2, a)
  # print('The generated Scores are shown below') print(as.data.frame(myvec)) Revising Score values
  total_max <- MaxScore
  total <- 0
  for (i in 1:length(FinalVariable)) total <- total + max(myvec[grepl(FinalVariable[i], names(myvec))])  #update
  myvec <- round(myvec/(total/total_max))
  print("The generated Scores are shown below")
  print(as.data.frame(myvec))

  ValidationSet3 <- AutoTest(ValidationSet2, myvec)
  ValidationSet3$TotalScore <- rowSums(subset(ValidationSet3, select = -label))
  y_validation <- ValidationSet3$label
  PlotROCCurve(ValidationSet3$TotalScore, as.numeric(y_validation) - 1)
  print("Performance using AutoScore ( based on Validation Set (After fine-tuning)):")
  Modelroc <- roc(y_validation, ValidationSet3$TotalScore, quiet = T)
  print("AUC:")
  print(ci(Modelroc))
  print(auc(Modelroc))
  print("The best cutoff of using this score")
  print(coords(Modelroc, "best", ret = "threshold", transpose = TRUE))
  print("Other Performance indicators based on this cutoff: ")
  print(coords(Modelroc, "best", ret = c("specificity", "sensitivity", "accuracy", "npv", "ppv", "precision"), transpose = TRUE))
  return(myvec)
}




AutoScore_testing <- function(TestSet, FinalVariable, CutVec, ScoringTable) {
  TestSet1 <- TestSet[, c(FinalVariable, "label")]
  TestSet2 <- Dftransform_fixed(TestSet1, CutVec = CutVec)
  TestSet3 <- AutoTest(TestSet2, ScoringTable)
  TestSet3$TotalScore <- rowSums(subset(TestSet3, select = -label))
  y_test <- TestSet3$label
  PlotROCCurve(TestSet3$TotalScore, as.numeric(y_test) - 1)

  print("Performance using AutoScore ( based on unseen test Set):")
  Modelroc <- roc(y_test, TestSet3$TotalScore, quiet = T)
  print("AUC:")
  print(ci(Modelroc))
  print(auc(Modelroc))
  print("The best cutoff of using this score")
  print(coords(Modelroc, "best", ret = "threshold", transpose = TRUE))
  print("Other Performance indicators based on this cutoff: ")
  print(coords(Modelroc, "best", ret = c("specificity", "sensitivity", "accuracy", "npv", "ppv", "precision"), transpose = TRUE))
}



