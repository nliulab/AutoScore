# AutoScore-ShapleyVIC for binary outcomes {#top}

```{r, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, 
                      fig.width = 6, fig.height = 6)
```

[Chapter 5](04-autoscore.qmd#top) described the AutoScore framework for binary
outcomes, using the random forest or AUC-based approach to rank and select
variables. This chapter describes an integrated AutoScore-ShapleyVIC framework
that uses Shapley variable importance cloud (ShapleyVIC), a recently developed
interpretable machine learning approach, for interpretable and robust variable
ranking, and the subsequent AutoScore pipelines (`AutoScore_parsimony()`, 
`AutoScore_weighting()`, `AutoScore_fine_tuning()` and `AutoScore_testing()`) to 
develop sparse clinical risk scores.

In this chapter, we demonstrate the use of AutoScore-ShapleyVIC using the same
simulated dataset as in [Chapter 5, Demo 1](04-autoscore.qmd#top), focusing on 
the variable ranking step using ShapleyVIC and the difference in the resulting
parsimony plot.

Cite the following papers for AutoScore-ShapleyVIC:

Ning Y, Ong ME, Chakraborty B, Goldstein BA, Ting DS, Vaughan R, Liu N. Shapley
variable importance cloud for interpretable machine learning. *Patterns* 2022
(<https://doi.org/10.1016/j.patter.2022.100452>)

Ning Y, Li S, Ong ME, Xie F, Chakraborty B, Ting DS, Liu N. A novel
interpretable machine learning system to generate clinical risk scores: An
application for predicting early mortality or unplanned readmission in a
retrospective cohort study. *PLOS Digit Health* 1(6): e0000062.
(<https://doi.org/10.1371/journal.pdig.0000062>)

## Step 1: Prepare training, validation, and test datasets

- Same split-sample set-up as in [Chapter 5, Demo 1](04-autoscore.qmd#top).

```{r, warning=TRUE, message=TRUE}
library(AutoScore)
data("sample_data")
names(sample_data)[names(sample_data) == "Mortality_inpatient"] <- "label"
check_data(sample_data)
```

- ShapleyVIC requires binary outcomes to be coded as 0 and 1 or -1 and 1.

```{r}
sample_data$label <- as.numeric(sample_data$label == "TRUE")
```

```{r}
set.seed(4)
out_split <- split_data(data = sample_data, ratio = c(0.7, 0.1, 0.2))
train_set <- out_split$train_set
validation_set <- out_split$validation_set
test_set <- out_split$test_set
```

## Step 2: Variable ranking from ShapleyVIC analysis of nearly optimal models

### Install ShapleyVIC package from GitHub

```{r, eval=FALSE}
install.packages("devtools")
library(devtools)
install_github(repo = "nliulab/ShapleyVIC")
```

### Load R packages

```{r, message=FALSE}
library(car) # For VIF analysis of optimal model (needed for ShapleyVIC analysis)
library(ShapleyVIC)
library(tidyverse) # For convenient data manipulation and visualization
library(magrittr) # For convenient data manipulation
library(knitr)
```

### Train the optimal logistic regression model and check for colinearity

```{r}
m_optim_r <- glm(label ~ ., data = train_set, family = "binomial")
var_vif <- vif(m_optim_r)
```

### Generate a random set of 350 nearly optimal models

```{r, fig.width = 8, fig.height = 4}
set.seed(4)
models <- draw_models(
  coef_optim = coef(m_optim_r), coef_optim_var = vcov(m_optim_r),
  x = train_set[, 1:21], y = train_set$label,
  M = 800, u1 = 0.5, u2 = 18, epsilon = 0.05, n_final = 350
)
```

Parameters `u1` and `u2` need tuning:

- `u1` usually takes a small value around 0.5.
- `u2` can take a large value for large data.

As detailed in the Methods section of 
[the ShapleyVIC paper](https://doi.org/10.1016/j.patter.2022.100452), 
these two values can be selected via grid search, using a smaller size of 
initial sample (e.g., `M = 100`) to reduce run time. Select values of `u1` and 
`u2` such that:

1. Around 75% of initial samples were eligible.
2. The range of 1-1.05 times of minimum loss is well represented in the
eligible models (see histogram above, generated by `draw_models()`).

### Compute ShapleyVIC values from nearly optimal models

Use the first 3500 observations in the validation set to compute ShapleyVIC
values. This step will be time consuming, and users are recommended to use
parallel computing if the computer used has multiple cores (using the `n_cores`
parameter).

To check the number of cores available, use the following code:

```{r, eval=FALSE}
library(parallel)
n_cores_total = detectCores(logical = TRUE)
```

The appropriate number of cores to use depends on the total memory available and
the total number of variables. Users may start with 
`n_cores = floor(n_cores_total / 2)`, inspect the total % of CPU used, and
adjust `n_cores` to avoid 100% CPU usage (which may freeze the computation
process). This example used 6 out of 8 cores on a Mac mini (M1, 2020) and took 
50 hours. Users are recommended to save outputs from individual
models to a folder for backup purpose, by using the `output_folder` parameter.

```{r, eval=FALSE}
# Create a python version of optimal model:
m_optim <- logit_model_python(
  x_train = train_set[, 1:21], y_train = train_set$label
)
set.seed(4)
# Use the python version of optimal model to compute ShapleyVIC values:
df_shapley_vic <- compute_shapley_vic(
  model_py = m_optim, var_vif = var_vif, var_vif_threshold = 2,
  coef_mat = df_models[, setdiff(names(df_models), "perf_metric")], 
  perf_metric = df_models$perf_metric, 
  x_test = Out_split$validation_set[1:3500, x_names_display], 
  y_test = Out_split$validation_set$label[1:3500], 
  output_folder = "shapley_vic_output", # Results for each model is saved to this folder
  n_cores = 6 
)
```

```{r, include=FALSE}
df_shapley_vic <- readRDS("data/df_shapley_vic.RDS")
```

### Variable important analysis using ShapleyVIC values

Before proceeding to develop scoring models, users can summarize and visualize
the ShapleyVIC values to assess the contribution of the 20 candidate variables
to the outcome.

```{r}
df_shapley_vic_bar <- df_shapley_vic %$% summarise_shapley_vic(
  val = shapley_vic_val, val_sd = sage_sd, var_names = var_names
)
df_shapley_vic_bar %$% draw_bars(
  val = val, val_lower = val_lower, val_upper = val_upper, var_names = Variable, 
  title = "Overall variable importance across 350 nearly optimal models"
) 
```

```{r}
df_shapley_vic %$% draw_violins(
  var_names = var_names, 
  var_ordering = levels(df_shapley_vic_bar$Variable),
  val = shapley_vic_val, perf_metric = perf_metric, 
  title = "Variable importance from 350 nearly optimal models"
)
```

### Generate ensemble variable ranking from ShapleyVIC

- Use `summarise = TRUE` to generate ensemble variable ranking by averaging
variable ranks across the 350 nearly optimal models.
- Exclude variables with non-significant overall importance to simplify
subsequent model development, based on the 95% prediction interval of average
ShapleyVIC values.

```{r}
val_ranks <- df_shapley_vic %$% rank_variables(
  val = shapley_vic_val, val_sd = sage_sd, model_id = model_id, var_names = var_names, 
  summarise = TRUE, ties.method = "min"
) %>% arrange(mean_rank) %>% 
  filter(Variable %in% df_shapley_vic_bar$Variable[which(df_shapley_vic_bar$val_lower > 0)])
kable(val_ranks, digits = 1)
```

Smaller value of `mean_rank` indicates higher importance.

## Step 3: Model development using ShapleyVIC-based ranking and AutoScore workflow

### Prepare variable ranking list for AutoScore

```{r}
ranking <- val_ranks$mean_rank
names(ranking) <- val_ranks$Variable
```

### Select the best model with parsimony plot (AutoScore Modules 2+3+4)

- Parsimony plot from ShapleyVIC-based variable ranking is smoother than that
[based on random forest](04-autoscore.qmd#rf-pars). 

```{r}
AUC <- AutoScore_parsimony(
  train_set = train_set, validation_set = validation_set, 
  rank = ranking, max_score = 100, n_min = 1, n_max = length(ranking)
)
```

A feasible choice is to select the top 6 variables, 
`r toString(names(ranking)[1:6])`, resulting in the same model as developed in 
[Chapter 5](04-autoscore.qmd#model). 

```{r, fig.width=5, fig.height=5}
cut_vec <- AutoScore_weighting(
  train_set = train_set, validation_set = validation_set, 
  final_variables = names(ranking)[1:6], max_score = 100
)
```

Users can follow detailed steps in [Chapter 5](04-autoscore.qmd) for subsequent
model fine-tuning and evaluation.
