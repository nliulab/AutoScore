

# Pipeline_function --------------------------------------------------------

#' @title AutoScore STEP (1): Generate variable ranking List by machine learning (AutoScore Module 1)
#' @param train_set A processed \code{data.frame} that contains data to be analyzed, for training.
#'    MIssing values and outliers should be cleaned first. The outcome's name should be "label".
#'     Going through \code{Preprocess} for preprocessing data and \code{split_data} for splitting data,
#'     before AutoScore implementation, if needed.
#' @param ntree Number of trees in random forest algorithm for variable ranking, default:100
#'
#' @details The first step in the AutoScore framework is variable ranking. We use random forest (RF),
#' an ensemble machine learning algorithm, to identify the top-ranking predictors for subsequent score generation.
#' This step correspond to Module 1 in the AutoScore paper.
#'
#' @return Returns a vector containing the list of variables and its ranking generated by machine learning (random forest)
#'
#' @examples
#' # see AutoScore Guidebook for the whole 5-step workflow
#' \dontrun{
#' ranking <- AutoScore_rank(train_set, ntree=100)
#' }
#'
#' @references
#' \itemize{
#'  \item{Breiman, L. (2001), Random Forests, Machine Learning 45(1), 5-32}
#'  \item{Xie F, Chakraborty B, Ong MEH, Goldstein BA, Liu N, AutoScore: A Machine Learning?CBased Automatic Clinical
#'   Score Generator and Its Application to Mortality Prediction Using Electronic Health Records,
#'   JMIR Med Inform 2020;8(10):e21798, doi: 10.2196/21798}
#' }
#' @export
#' @importFrom randomForest randomForest importance
#'
AutoScore_rank <- function(train_set, ntree = 100) {
  #set.seed(4)
  train_set$label <- as.factor(train_set$label)
  model <- randomForest::randomForest(label ~ ., data = train_set, ntree = ntree, preProcess = "scale")

  # estimate variable importance
  importance <- randomForest::importance(model, scale = F)

  # summarize importance
  names(importance) <- rownames(importance)
  importance <- sort(importance, decreasing = T)
  cat("The ranking based on variable importance was shown below for each variable: \n")
  print(importance)
  return(importance)
}


#' @title AutoScore STEP (2): Selecting best models with parsimony plot ((AutoScore Modules 2+3+4))
#' @param train_set A processed \code{data.frame} that contains data to be analyzed, for training.
#'    Missing values and outliers should be cleaned first. The outcome's name should be "label".
#'     Going through \code{Preprocess} for preprocessing data and \code{split_data} for splitting data,
#'     before AutoScore implementation, if needed.
#' @param validation_set A processed \code{data.frame} that contains data for validation purpose.
#' @param rank the ramking result generated from AutoScore STEP (1) \code{AutoScore_rank}
#' @param n_min Minimum number of selected variables, default: 1
#' @param n_max Maximum number of selected variables, default: 20
#' @param max_score Maximum total score limit, used for normalization. Default: 100
#' @param cross_validation If set to \code{TRUE}, cross-validation would be used for generating parsimony plot, which is
#'   suitable for small-size data. Default to \code{FALSE}
#' @param fold Fold used in cross validation if \code{cross_validation=TRUE}. Default to 10
#' @param categorize  Methods for categorize continous variables. Options include "quantile", "kmeans" or "decision_tree"
#' @param quantiles It works only if \code{categorize="quantile"}. Predefined quantiles to convert continuous variables to categorical ones, default:(0, 0.05, 0.2, 0.8, 0.95, 1)
#' @param categorize It works only if \code{categorize="kmeans"}. continuous variable discretization by kmeans clustering, default # of cluster = 5
#' @param categorize It works only if \code{categorize="decision_tree"}. continuous variable discretization by decision tree(rpart),
#'    Splitting points generated from the decision tree constructed using continuous variable to be discretized (e.g. SpO2) as x,
#'    and label of the dataset(e.g. survival outcome) as y.
#' @param max_cluster the max number of cluster, default value = 5. works only if \code{categorize="kmeans"} or \code{categorize="decision_tree"}
#' @param do_trace If set to \code{TRUE}, all results based on each fold of cross-validation would be printed out and plotted.Default to \code{FALSE}
#'
#' @details This is the second step of the general AutoScore workflow, to generate the parsimony plot to help select parsimonious model.
#'  In this step, it goes through AutoScore Module 2,3 and 4 multiple times and to evaluate the performance under different variable list.
#'  The generated parsimony plot would give researcher an intuitive figure to choose the best models.
#'  If data size is small (ie <5000), an independent validation set may not be a wise choice. Then we will use cross-validation
#'  to maximize the utility of data. Set \code{cross_validation=TRUE}.
#'
#' @return List of AUC value for different number of variables
#'
#' @examples
#' # see AutoScore Guidebook for the whole 5-step workflow
#' \dontrun{
#' ranking <- AutoScore_rank(train_set, ntree=100)
#' }
#'
#' @references
#' \itemize{
#'  \item{Xie F, Chakraborty B, Ong MEH, Goldstein BA, Liu N, AutoScore: A Machine Learning?CBased Automatic Clinical
#'   Score Generator and Its Application to Mortality Prediction Using Electronic Health Records,
#'   JMIR Med Inform 2020;8(10):e21798, doi: 10.2196/21798}
#' }
#' @export
#' @import rpart pROC
#'
AutoScore_parsimony <- function(train_set, validation_set, rank, max_score = 100, n_min = 1, n_max = 20, cross_validation = FALSE, fold = 10,
                                categorize = "quantile", quantiles = c(0, 0.05, 0.2, 0.8, 0.95, 1), max_cluster = 5, do_trace = FALSE) {

  if(n_max > length(rank)){
    warning("WARNING: the n_max (",n_max,") is larger the number of all variables (",length(rank),"). We Automatically revise the n_max to ",length(rank))
    n_max <- length(rank)
  }
  # Cross Validation scenario
  if(cross_validation == TRUE){

  # Divide the data equally into n fold, record its index number
  set.seed(4)
  index<-list()
  all<-1:length(train_set[,1])
  for(i in 1:(fold-1)){
    a<-sample(all,trunc(length(train_set[,1])/fold))
    index<-append(index,list(a))
    all<-all[!(all %in% a)]
  }
  index<-c(index,list(all))

  # Create a new variable auc_set to store all AUC value during the corss-validation
  auc_set<-data.frame(rep(0,n_max-n_min+1))

  # for each fold, generate train_set and validation_set
  for(j in 1:fold){
    validation_set_temp <- train_set[index[[j]],]
    train_set_tmp <- train_set[-index[[j]],]

    #variable_list <- names(rank)
    AUC <- c()

    # Go through AUtoScore Module 2/3/4 in the loop
    for (i in n_min:n_max) {
      variable_list<-names(rank)[1:i]
      train_set_1 <- train_set_tmp[, c(variable_list, "label")]
      validation_set_1 <- validation_set_temp[, c(variable_list, "label")]

      model_roc<-compute_auc_val(train_set_1, validation_set_1, variable_list, categorize, quantiles, max_score)
      #print(auc(model_roc))
      AUC <- c(AUC, auc(model_roc))
    }

    # plot parsimony plot for each fold
    names(AUC) <- n_min:n_max

    # only print and plot when do_trace = TRUE
    if(do_trace){
    print(paste("list of AUC values for fold",j))
    print(data.frame(AUC))
    plot(AUC, main = paste("Parsimony plot (cross validation) for fold",j), xlab = "Number of Variables", ylab = "Area Under the Curve", col = "red",
         lwd = 2, type = "o")}

    # store AUC result from each fold into "auc_set"
    auc_set<-cbind(auc_set,data.frame(AUC))

  }

  # finish loop and then output final results averaged by all folds
  auc_set$rep.0..n_max...n_min...1.<-NULL
  auc_set$sum<-rowSums(auc_set)/fold
  cat("***list of final mean AUC values through cross-validation are shown below \n")
  print(data.frame(auc_set$sum))
  plot(auc_set$sum, main = paste("Final Parsimony Plot based on ", fold, "-fold Cross Validation",sep = ""), xlab = "Number of Variables", ylab = "Area Under the Curve", col = "red",
       lwd = 2, type = "o")
  return(auc_set)}




  # if Cross validation is FALSE
  else{
  AUC <- c()

  # Go through AutoScore Module 2/3/4 in the loop
  for (i in n_min:n_max) {
    cat(paste("Select",i,"Variable(s):  "))

    variable_list<-names(rank)[1:i]
    train_set_1 <- train_set[, c(variable_list, "label")]
    validation_set_1 <- validation_set[, c(variable_list, "label")]
    model_roc<-compute_auc_val(train_set_1, validation_set_1,variable_list, categorize, quantiles, max_score)
    print(auc(model_roc))
    AUC <- c(AUC, auc(model_roc))
  }

  # output final results and plot parsimony plot
  names(AUC) <- n_min:n_max
  #cat("list of AUC values are shown below")
  #print(data.frame(AUC))
  plot(AUC, main = "Parsimony Plot on the Validation Set", xlab = "Number of Variables", ylab = "Area Under the Curve", col = "red",
       lwd = 2, type = "o")

  return(AUC)
}
}


#' @title AutoScore STEP (3): Generate initial score with selected Variables
#' (Rerun AutoScore Module 2+3)
#' @param train_set A processed \code{data.frame} that contains data to be analyzed, for training.
#'    Missing values and outliers should be cleaned first. The outcome's name should be "label".
#'     Going through \code{Preprocess} for preprocessing data and \code{split_data} for splitting data,
#'     before AutoScore implementation, if needed.
#' @param validation_set A processed \code{data.frame} that contains data for validation purpose.
#' @param final_variables A vector containing the list of selected variables, selected from Step(2). See Guidebook
#' @param max_score Maximum total score limit, used for normalization. Default: 100
#' @param categorize  Methods for categorize continous variables. Options include "quantile", "kmeans" or "decision_tree"
#' @param quantiles It works only if \code{categorize="quantile"}. Predefined quantiles to convert continuous variables to categorical ones, default:(0, 0.05, 0.2, 0.8, 0.95, 1)
#' @param categorize It works only if \code{categorize="kmeans"}. continuous variable discretization by kmeans clustering, default # of cluster = 5
#' @param categorize It works only if \code{categorize="decision_tree"}. continuous variable discretization by decision tree(rpart),
#'    Splitting points generated from the decision tree constructed using continuous variable to be discretized (e.g. SpO2) as x,
#'    and label of the dataset(e.g. survival outcome) as y.
#' @param max_cluster the max number of cluster, default value = 5. works only if \code{categorize="kmeans"} or \code{categorize="decision_tree"}
#' @details This is the third step of AutoScore workflow, to generate the initial score based on selected variables.
#'  In this step, it goes through AutoScore Module 2,3 to generate initial performance. Generated \code{cut_vec} will be used forStep(4): fine-tuning
#' @return Generated \code{cut_vec} for downstream fine-tuning process [STEP (4)]
#' @examples
#' \dontrun{
#' cut_vec <- AutoScore_weighting(train_set, validation_set, final_variables, max_score=100, quantiles=c(0, 0.05, 0.2, 0.8, 0.95, 1))
#' }
#'
#' @export
#' @import rpart pROC ggplot2
AutoScore_weighting <- function(train_set, validation_set, final_variables, max_score = 100, categorize = "quantile", max_cluster = 5, quantiles = c(0, 0.05, 0.2, 0.8, 0.95, 1)) {
  # prepare train_set and VadalitionSet
  cat("****Included Variables: \n")
  print(data.frame(variable_name = final_variables))
  train_set_1 <- train_set[, c(final_variables, "label")]
  validation_set_1 <- validation_set[, c(final_variables, "label")]

  # AutoScore Module 2 : cut numeric and transfer categories and generate "cut_vec"
  df_transformed <- transform_df(train_set_1, validation_set_1, categorize = categorize, quantiles = quantiles, max_cluster = max_cluster, print_categories = TRUE)
  train_set_2 <- df_transformed[[1]]
  validation_set_2 <- df_transformed[[2]]
  cut_vec_tmp <- df_transformed[[3]]
  cut_vec <- cut_vec_tmp
  for (i in 1:length(cut_vec)) cut_vec[[i]] <- cut_vec[[i]][2:(length(cut_vec[[i]]) - 1)]

  # AutoScore Module 3 : Score weighting
  score_table<-compute_score_table(train_set_2,validation_set_2,max_score,final_variables)
  # Revised scoring table representation if possible @YuanHan
  cat("****Initial Scores: \n")
  #print(as.data.frame(score_table))
  print_score_table(scoring_table = score_table, final_variable = final_variables)

  # Using "assign_score" to generate score based on new dataset and Scoring table "score_table"
  validation_set_3 <- assign_score(validation_set_2, score_table)
  validation_set_3$total_score <- rowSums(subset(validation_set_3, select = -label))
  y_validation <- validation_set_3$label

  # Intermediate evaluation based on Validation Set
  plot_roc_curve(validation_set_3$total_score, as.numeric(y_validation) - 1)
  cat("***Performance (based on validation set):\n")
  print_roc_performance(y_validation, validation_set_3$total_score, threshold = "best")
  cat("***The cutoffs of each variable generated by the AutoScore are saved in cut_vec. You can decide whether to revise or fine-tune them \n")
  #print(cut_vec)
  return(cut_vec)
}




#' @title AutoScore function: STEP (4): Fine-tune the score (AutoScore Module 5)
#' @description Domain knowledge is essential in guiding risk model development.
#'  For continuous variables, the variable transformation is a data-driven process (based on "quantile", "kmeans" or "decision_tree).
#'  In this step, the automatically generated cutoff values for each continuous variable can be fine-tuned
#'  by combining, rounding, and adjusting according to the standard clinical norm.  Revised \code{cut_vec} will be input  with domain knowledge to
#' update scoring table. User can choose any cut-off values/any number of categories. Then final Scoring table will be generated. See Guidebook.
#'
#' @param train_set A processed \code{data.frame} that contains data to be analyzed, for training.
#'    Missing values and outliers should be cleaned first. The outcome's name should be "label".
#'     Going through \code{Preprocess} for preprocessing data and \code{split_data} for splitting data,
#'     before AutoScore implementation, if needed.
#' @param validation_set A processed \code{data.frame} that contains data for validation purpose.
#' @param final_variables A vector containing the list of selected variables, selected from Step(2). See Guidebook
#' @param max_score Maximum total score limit, used for normalization. Default: 100
#' @param cut_vec Generated from STEP(3) \code{AutoScore_weighting()}.Please
#' follow the guidebook
#'
#' @return Generated final table of scoring model for downstream testing
#' @examples
#' \dontrun{
#' scoring_table <- AutoScore_fine_tuning(train_set, validation_set, final_variables, cut_vec, max_score=100)}
#' @export
#' @import pROC ggplot2
AutoScore_fine_tuning <- function(train_set, validation_set, final_variables, cut_vec, max_score = 100) {
  # Prepare train_set and VadalitionSet
  train_set_1 <- train_set[, c(final_variables, "label")]
  validation_set_1 <- validation_set[, c(final_variables, "label")]

  # AutoScore Module 2 : cut numeric and transfer categories (based on fix "cut_vec" vector)
  train_set_2 <- transform_df_fixed(train_set_1, cut_vec = cut_vec)
  validation_set_2 <- transform_df_fixed(validation_set_1, cut_vec = cut_vec)

  # AutoScore Module 3 : Score weighting
  score_table<-compute_score_table(train_set_2,validation_set_2,max_score,final_variables)
  # Revised scoring table representation if possible @YuanHan
  cat("***Fine-tuned Scores: \n")
  #print(as.data.frame(score_table))
  print_score_table(scoring_table = score_table, final_variable = final_variables)

  # Using "assign_score" to generate score based on new dataset and Scoring table "score_table"
  validation_set_3 <- assign_score(validation_set_2, score_table)
  validation_set_3$total_score <- rowSums(subset(validation_set_3, select = -label))
  y_validation <- validation_set_3$label

  # Intermediate evaluation based on Validation Set after fine-tuning
  plot_roc_curve(validation_set_3$total_score, as.numeric(y_validation) - 1)
  cat("***Performance (based on validation set, after fine-tuning):\n")
  print_roc_performance(y_validation, validation_set_3$total_score, threshold = "best")
  return(score_table)
}


#' @title AutoScore STEP (5): Final score evaluation (AutoScore Module 6)
#' @description Domain knowledge is essential in guiding risk model development.
#'  For continuous variables, the variable transformation is a data-driven process (based on "quantile", "kmeans" or "decision_tree).
#'  In this step, the automatically generated cutoff values for each continuous variable can be fine-tuned
#'  by combining, rounding, and adjusting according to the standard clinical norm.  Revised \code{cut_vec} will be input  with domain knowledge to
#' update scoring table. User can choose any cut-off values/any number of categories. Then final Scoring table will be generated. See Guidebook.
#'
#' @param test_set A processed \code{data.frame} that contains data for testing purpose. This \code{data.frame} should have same format as
#'        \code{train_set} (same variable names and outcomes)
#' @param final_variables A vector containing the list of selected variables, selected from Step(2). See Guidebook
#' @param scoring_table The final scoring table after fine-tuning, generated from STEP(4) \code{AutoScore_fine_tuning}.Please follow the guidebook
#' @param cut_vec Generated from STEP(3) \code{AutoScore_weighting()}.Please follow the guidebook
#'
#' @return List of performance metrics including "AUROC", "specificity", "sensitivity", "accuracy", "npv", "ppv", "??AUPRC??.
#' #' @examples
#' \dontrun{
#' performance <- AutoScore_testing(test_set, final_variables, cut_vec, scoring_table)}
#' @export
#' @import pROC ggplot2
AutoScore_testing <- function(test_set, final_variables, cut_vec, scoring_table, threshold = "best", with_label = TRUE) {
  if(with_label){
  # prepare testset: categorization and "assign_score"
  test_set_1 <- test_set[, c(final_variables, "label")]
  test_set_2 <- transform_df_fixed(test_set_1, cut_vec = cut_vec)
  test_set_3 <- assign_score(test_set_2, scoring_table)
  test_set_3$total_score <- rowSums(subset(test_set_3, select = -label))
  test_set_3$total_score[which(is.na(test_set_3$total_score))]<-0
  y_test <- test_set_3$label

  # Final evaluation based on testing set
  plot_roc_curve(test_set_3$total_score, as.numeric(y_test) - 1)
  cat("***Performance using AutoScore (based on unseen test Set):\n")
  model_roc <- roc(y_test, test_set_3$total_score, quiet = T)
  print_roc_performance(y_test, test_set_3$total_score, threshold = threshold)
  #Modelprc <- pr.curve(test_set_3$total_score[which(y_test == 1)],test_set_3$total_score[which(y_test == 0)],curve = TRUE)
  #values<-coords(model_roc, "best", ret = c("specificity", "sensitivity", "accuracy", "npv", "ppv", "precision"), transpose = TRUE)
  pred_score <- data.frame(pred_score = test_set_3$total_score, Label = y_test)
  return(pred_score)


  } else { test_set_1 <- test_set[, c(final_variables)]
  test_set_2 <- transform_df_fixed(test_set_1, cut_vec = cut_vec)
  test_set_3 <- assign_score(test_set_2, scoring_table)
  test_set_3$total_score <- rowSums(subset(test_set_3, select = -label))
  test_set_3$total_score[which(is.na(test_set_3$total_score))]<-0
  pred_score <- data.frame(pred_score = test_set_3$total_score, Label = NA)
  return(pred_score)}
}


# Direct_function ---------------------------------------------------------

# preprocess
# preprocessing: requirement of data finish preselection first 1.data type(only numeric or factor),all character will be transformed
# to factor automatically 2.point out the outcome(should be factor and binary outcome) 3.The number of categories for each factor
# should be less than 9
# Preprocess(data,outcome) outcome: character class object wich point to the name of the outcome in the dataset data: should be a
# dataframe


#' @title AutoScore Function: Check Dataset
#' @param data The data which will be the source for the dataset
#' @examples checkData(testdata1_mimic)
#' @export
check_data<-function(data){
  #1. check label and binary
  if(is.null(data$label)) stop("ERROR: for this dataset: These is no dependent variable -lable- to indicate the outcome. Please add one first")
  if(length(levels(factor(data$label)))!=2) warning("Please keep outcome lable variable binary")

  #2. check each variable
  non_num_fac<-c()
  fac_large<-c()
  special_case<-c()


  for (i in names(data)) {
    if ((class(data[[i]]) != "factor") && (class(data[[i]]) != "numeric"))  non_num_fac<-c(non_num_fac,i)
    if ((length(levels(data[[i]])) > 10) && (class(data[[i]]) == "factor"))   fac_large<-c(fac_large,i)
    if (grepl(",",i)) warning(paste("WARNING: the dataset has variable names",i, "with character , please change it"))
    if (grepl(")",i)) warning(paste("WARNING: the dataset has variable names",i, "with character ) please change it"))
    if (grepl(" ",i)) {
      warning(paste("WARNING: the dataset has variable names",i, "with character sapce symbol, procedure automatically changes it to _"))
      gsub(" ", "_", names(data)[i])
      }
    }

  if(!is.null(non_num_fac)) warning(paste("WARNING: the dataset has variable of character and they will be transformed to factor:",non_num_fac))
  if(!is.null(fac_large)) warning(paste("WARNING: The number of categories for some variables is too many :larger than:",fac_large))


  #3. check missing values
  missing_rate<-colSums(is.na(data))
  if(sum(missing_rate)) {
    warning("WARNING: Your dataset contains NA. Please handle them before AutoScore. The variables with missing values are shown below:")
    print(missing_rate[missing_rate!=0])
  }
  else cat("missing value check passed.\n")
  #cat("Please fixed the problem of your dataset before AutoScore if you see any Wanings below.\n")




}
# check 1. missing value or not label is there and binary or not 3.only factor and numeric: 4. factor larger than 10?



#' @title AutoScore Function: Automatically splitting dataset to train, validation and test set
#' @param data The data which will be the source for the dataset
#' @param ratio To divede train, validation and test set
#' @param cross_validation If set to \code{TRUE}, cross-validation would be used for generating parsimony plot, which is
#'   suitable for small-size data. Default to \code{FALSE}
#' @return Returns a list containing training, validation and testing set
#' @examples #large sample size
#' out_split <- split_data(data = df_AutoScore, ratio = c(0.7, 0.1, 0.2), cross_validation = FALSE)
#' #small sample size
#' out_split <- split_data(data = df_AutoScore, ratio = c(0.7, 0, 0.3), cross_validation = TRUE)
#' @export
split_data <- function(data, ratio, cross_validation = FALSE) {

  # non cross validation: default
  if (cross_validation == FALSE) {
    n <- length(data[, 1])
    test_ratio <- ratio[3] / sum(ratio)
    validation_ratio <- ratio[2] / sum(ratio)
    set.seed(4)
    test_index <- sample((1:n), test_ratio * n)
    validate_index <-
      sample((1:n)[!(1:n) %in% test_index], validation_ratio * n)
    train_set <- data[-c(validate_index, test_index), ]
    test_set <- data[test_index, ]
    validation_set <- data[validate_index, ]

    return(list(
      train_set = train_set,
      validation_set = validation_set,
      test_set = test_set
    ))
  }

  #  cross validation: train = validation
  else{
    n <- length(data[, 1])
    test_ratio <- ratio[3] / sum(ratio)
    validation_ratio <- ratio[2] / sum(ratio)
    set.seed(4)
    test_index <- sample((1:n), test_ratio * n)
    validate_index <-
      sample((1:n)[!(1:n) %in% test_index], validation_ratio * n)
    train_set <- data[-c(test_index),]
    test_set <- data[test_index,]
    validation_set <- train_set

    return(list(
      train_set = train_set,
      validation_set = validation_set,
      test_set = test_set
    ))
  }

}




#' @title AutoScore Function: Descriptive Analysis
#' @description print Descriptive table for your data, straified by your label(outcome)
#' @param df dataset after preprocessing
#' @examples Descriptive(data)
#' @export
#' @importFrom tableone CreateTableOne print.TableOne
# Generate table one based on stratified outcomes
compute_descriptive_table<- function(df) {
  descriptive_table <- CreateTableOne(vars = names(df), strata = "label", data = df)
  descriptive_table_overall <- CreateTableOne(vars = names(df), data = df)
  print(descriptive_table)
  print(descriptive_table_overall)
}



#' @title AutoScore Function: Univariable Analysis
#' @description Univariable Analysis
#' @param df dataset after preprocessing
#' @return dataframe of univariate analysis
#' @examples UniVariable(data)
#' @export
# Univariable analysis
compute_uni_variable_table <- function(df) {
  uni_table <- data.frame()
  for (i in names(df)[names(df) != "label"]) {
    model <- glm(label ~ ., data = subset(df, select = c("label", i)), family = binomial, na.action = na.omit)
    a <- cbind(exp(cbind(OR = coef(model), confint.default(model))), summary(model)$coef[, "Pr(>|z|)"])
    uni_table <- rbind(uni_table, a)
  }
  uni_table <- uni_table[!grepl("Intercept", row.names(uni_table), ignore.case = T), ]
  uni_table <- round(uni_table, digits = 3)
  uni_table$V4[uni_table$V4<0.001]<-"<0.001"
  uni_table$OR <- paste(uni_table$OR, "(", uni_table$`2.5 %`, "-", uni_table$`97.5 %`, ")", sep = "")
  uni_table$`2.5 %` <- NULL
  uni_table$`97.5 %` <- NULL
  names(uni_table)[names(uni_table) == "V4"] <- "p value"
  return(uni_table)
}



#' @title AutoScore Function: Multivariable Analysis
#' @description Generate tables for Multivariable Analysis
#' @param df dataset after preprocessing
#' @return dataframe of Multivariable Analysis
#' @examples MultiVariable(data)
#' @export
# full logistic model-multivariable analysis
compute_multi_variable_table <- function(df) {
  model <- glm(label ~ ., data = df, family = binomial, na.action = na.omit)
  multi_table <- cbind(exp(cbind(OR = coef(model), confint.default(model))), summary(model)$coef[, "Pr(>|z|)"])
  multi_table <- multi_table[!grepl("Intercept", row.names(multi_table), ignore.case = T), ]
  multi_table <- round(multi_table, digits = 3)
  multi_table <- as.data.frame(multi_table)
  multi_table$V4[multi_table$V4<0.001]<-"<0.001"
  multi_table$OR <- paste(multi_table$OR, "(", multi_table$`2.5 %`, "-", multi_table$`97.5 %`, ")", sep = "")
  multi_table$`2.5 %` <- NULL
  multi_table$`97.5 %` <- NULL
  names(multi_table)[names(multi_table) == "V4"] <- "p value"
  return(multi_table)
}

#' @title AutoScore Function: print_score_table
#' @description Print scoring tables for visulization
#' @param ScoringTable Raw scoring table generated by procedure
#' @param FinalVariable Final included variables by predictor_rank
#' @return dataframe of formatted scoring table
#' @examples GenerateCSV(ScoringTable = myvec, FinalVariable = final_variable)
#' @export
#' @importFrom knitr kable
print_score_table<-function(scoring_table,final_variable){
  #library(knitr)
  table_tmp<-data.frame()
  for (i in 1:length(final_variable)) {
    var_tmp<-final_variable[i]
    var_name<-names(scoring_table)
    num<-grepl(var_tmp,var_name)
    table_1<-data.frame(name=var_name[num],value=unname(scoring_table[num]))
    rank_indicator<-gsub(".*,","",table_1$name)
    rank_indicator<-gsub(")","",rank_indicator) #>>> (Xie Feng) bugs here. not compatible for only two categories(E.g.,mwhen quantiles = c(0, 0.5, 1) )
    # rank_indicator[which(rank_indicator=="")]<-max(as.numeric(rank_indicator[-which(rank_indicator=="")]))+1
    {
    if (grepl(",",table_1$name[1])!=TRUE){
      table_1$rank_indicator<-c(seq(1:nrow(table_1)))
      interval<-c(gsub(pattern = var_tmp, replacement = "", table_1$name))
      table_1$interval<-interval
      table_2<-table_1[order(table_1$interval),]
      table_2$variable<-c(var_tmp,rep("",(nrow(table_2)-1)))
      table_3<-rbind(table_2,rep("",ncol(table_2)))
      table_tmp<-rbind(table_tmp,table_3)
    }
    else
    {
    rank_indicator[which(rank_indicator=="")]<-max(as.numeric(rank_indicator[-which(rank_indicator=="")]))+1
    rank_indicator<-as.numeric(rank_indicator)
    {
    if (length(rank_indicator)==2){
      table_1$rank_indicator<-rank_indicator
      table_2<-table_1[order(table_1$rank_indicator),]
      interval<-c(paste0("<",table_2$rank_indicator[1]))
      interval<-c(interval,paste0(">=",table_2$rank_indicator[length(rank_indicator)-1]))
      table_2$interval<-interval
      table_2$variable<-c(var_tmp,rep("",(nrow(table_2)-1)))
      table_3<-rbind(table_2,rep("",ncol(table_2)))
      table_tmp<-rbind(table_tmp,table_3)
    }
    else{
      table_1$rank_indicator<-rank_indicator
      table_2<-table_1[order(table_1$rank_indicator),]
      interval<-c(paste0("<",table_2$rank_indicator[1]))
      for (j in 1:(length(table_2$rank_indicator)-2)) {
        interval<-c(interval,paste0("[",table_2$rank_indicator[j],",",table_2$rank_indicator[j+1],")"))
      }
      interval<-c(interval,paste0(">=",table_2$rank_indicator[length(rank_indicator)-1]))
      table_2$interval<-interval
      table_2$variable<-c(var_tmp,rep("",(nrow(table_2)-1)))
      table_3<-rbind(table_2,rep("",ncol(table_2)))
      table_tmp<-rbind(table_tmp,table_3)
    }
    }
    }
    }
  }
  table_tmp<-table_tmp[1:(nrow(table_tmp)-1),]
  table_final<-data.frame(variable = table_tmp$variable,interval = table_tmp$interval, point = table_tmp$value)
  table_kable_format<-kable(table_final,align = "llc",caption = "AutoScore-created scoring model",format = "rst")
  print(table_kable_format)
  invisible(table_final)
}




# Build_in_function -------------------------------------------------------
## built-in function for AutoScore below are functions
## Those functions are cited by pepline functions

# description -
# R-oxygen
# default value.
# function description
# comments what u are trying to do.

#print(paste("Select the number of Variables",i))


delete_min_max<-function(vec){
  levels(vec)[1] <- gsub(".*,", "(,", levels(vec)[1])
  levels(vec)[length(levels(vec))] <- gsub(",.*", ",)", levels(vec)[length(levels(vec))])
  return(vec)
}



print_roc_performance<-function(label, score, threshold = "best"){
  if(sum(is.na(score))>0) warning("NA in the score: ",sum(is.na(score)))
  model_roc <- roc(label, score, quiet = T)
  cat("AUC: ", round(auc(model_roc),4),"  ")
  print(ci(model_roc))

  if(threshold == "best") {
    threshold <- ceiling(coords(model_roc, "best", ret = "threshold", transpose = TRUE))
    cat("Best score threshold: >=", threshold, "\n")
  } else {cat("Score threshold: >=",threshold,"\n")}
  cat("Other performance indicators based on this score threshold: \n")
  roc <- ci.coords(model_roc, threshold , ret = c("specificity", "sensitivity", "npv", "ppv"), transpose = TRUE)
  cat("Sensitivity: ",round(roc$sensitivity[2],4)," 95% CI: ",round(roc$sensitivity[1],4),"-",round(roc$sensitivity[3],4),"\n",sep = "")
  cat("Specificity: ",round(roc$specificity[2],4)," 95% CI: ",round(roc$specificity[1],4),"-",round(roc$specificity[3],4),"\n",sep = "")
  cat("PPV:         ",round(roc$ppv[2],4)," 95% CI: ",round(roc$ppv[1],4),"-",round(roc$ppv[3],4),"\n",sep = "")
  cat("NPV:         ",round(roc$npv[2],4)," 95% CI: ",round(roc$npv[1],4),"-",round(roc$npv[3],4),"\n",sep = "")
}

compute_score_table<-function(train_set_2,validation_set_2,max_score,variable_list){

  #AutoScore Module 3 : Score weighting
  # First-step logistic regression
  model <- glm(label ~ ., family = binomial(link = "logit"), data = train_set_2)
  y_validation <- validation_set_2$label
  coef_vec <- coef(model)
  if (length(which(is.na(coef_vec)))>0) {warning(" WARNING: GLM output contains NULL, Replace NULL with 1")
    coef_vec[which(is.na(coef_vec))]<-1}
  train_set_2 <- change_reference(train_set_2, coef_vec)

  # Second-step logistic regression
  model <- glm(label ~ ., family = binomial(link = "logit"), data = train_set_2)
  coef_vec <- coef(model)
  if (length(which(is.na(coef_vec)))>0) {warning(" WARNING: GLM output contains NULL, Replace NULL with 1")
    coef_vec[which(is.na(coef_vec))]<-1}

  # rounding for final scoring table "score_table"
  coef_vec_tmp <- round(coef_vec/min(coef_vec[-1]))
  score_table <- add_baseline(train_set_2, coef_vec_tmp)

  # normalization according to "max_score" and regenerate score_table
  total_max <- max_score
  total <- 0
  for (i in 1:length(variable_list)) total <- total + max(score_table[grepl(variable_list[i], names(score_table))])
  score_table <- round(score_table/(total/total_max))
  return(score_table)}



compute_auc_val <-function(train_set_1, validation_set_1, variable_list, categorize, quantiles, max_score){

  # AutoScore Module 2 : cut numeric and transfer categories
  df_transformed <- transform_df(train_set_1, validation_set_1, categorize = categorize ,quantiles = quantiles)
  train_set_2 <- df_transformed[[1]]
  validation_set_2 <- df_transformed[[2]]

  # AutoScore Module 3 : Variable Weighting
  score_table<-compute_score_table(train_set_2,validation_set_2,max_score,variable_list)

  # Using "assign_score" to generate score based on new dataset and Scoring table "score_table"
  validation_set_3 <- assign_score(validation_set_2, score_table)
  validation_set_3$total_score <- rowSums(subset(validation_set_3, select = -label))
  y_validation <- validation_set_3$label
  # plot_roc_curve(validation_set_3$total_score,as.numeric(y_validation)-1)

  # calculate AUC value
  model_roc <- roc(y_validation, validation_set_3$total_score, quiet = T)

  return(model_roc) }





#' @title Build-in Function: Categorizing continuous variables (AutoScore Module 2)
#' @param df dataset
#' @param df_new new dataset
#' @param ntree Number of trees in random forest algorithm for variable ranking, default:100
#' @param categorize  Methods for categorize continous variables. Options include
#'   "quantile", "kmeans" or "decision_tree"
#' @param quantiles It works only if \code{categorize="quantile"}. Predefined quantiles
#' to convert continuous variables to categorical ones, default:(0, 0.05, 0.2, 0.8, 0.95, 1)
#' @param max_cluster the max number of cluster, default value = 5. works only if \code{categorize="kmeans"} or \code{categorize="decision_tree"}
#' @param print_categories whether to print all cateroties, used in Step(3) generating the \code{cut_vec}
#' @return Returns a vector containing the list of variables and its ranking generated by machine learning (random forest)
#' @import rpart
# @Linxuan see how we can add more check point function:
# transform_df

transform_df <-
  function(df,
           df_new,
           quantiles = c(0, 0.05, 0.2, 0.8, 0.95, 1), #by default
           print_categories = FALSE,
           max_cluster = 5,
           categorize = "quantile") {

    # Generate cut_vec for downstream usage
    cut_vec <- list()

    for (i in 1:(length(df) - 1)) {
      # for factor variable
      if (class(df[, i]) == "factor") {
        if (length(levels(df[, i])) < 10)
          #(next)() else stop("ERROR: The number of categories should be less than 10")
          (next)()
        else
          warning("WARNING: The number of categories should be less than 10",
                names(df)[i])
      }

      # for continuous variable: variable transformation
      # select discretization method, default mode = 1

      ## mode 1 - quantiles
      if (categorize == "quantile") {
        # options(scipen = 20)
        #print("in quantile")
        cut_off_tmp <- quantile(df[, i], quantiles = quantiles)
        cut_off_tmp <- unique(cut_off_tmp)
        cut_off <- signif(cut_off_tmp, 3)  # remain 3 digits
        #print(cut_off)

        ## mode 2 k-means clustering
      } else if (categorize == "k_means") {
        #print("using k-means")
        clusters <- kmeans(df[, i], max_cluster)
        cut_off_tmp <- c()
        for (j in unique(clusters$cluster)) {
          #print(min(df[,i][clusters$cluster==j]))
          #print(length(df[,i][clusters$cluster==j]))
          cut_off_tmp <- append(cut_off_tmp, min(df[, i][clusters$cluster == j]))
          #print(cut_off_tmp)
        }
        cut_off_tmp <- append(cut_off_tmp, max(df[, i]))
        cut_off_tmp <- sort(cut_off_tmp)
        #print(names(df)[i])
        #assert (length(cut_off_tmp) == 6)
        cut_off_tmp <- unique(cut_off_tmp)
        cut_off <- signif(cut_off_tmp, 3)
        cut_off <- unique(cut_off)
        #print (cut_off)

        ## mode 3 decision_tree-rpart
      } else if (categorize == "decision_tree") {
        ## assign weights to address the umbalanced dataset
        # hardcode, might need to change
        w_positive <-
          nrow(df) / (length(unique(df$label)) * sum(df$label == 1))
        w_negative <-
          nrow(df) / (length(unique(df$label)) * sum(df$label == 0))
        #print(w_positive)
        #print(w_negative)
        w <-
          (as.numeric(df$label) - 1) * w_positive + (2 - as.numeric(df$label)) * w_negative

        m <- rpart(df$label ~ df[, i], method = 'class', weights = w)
        cut_off_tmp <- unname(m$splits[, max_cluster-1])
        cut_off_tmp <- head(cut_off_tmp, max_cluster-1)
        cut_off_tmp <- append(cut_off_tmp, max(df[, i]))
        cut_off_tmp <- append(cut_off_tmp, min(df[, i]))
        cut_off_tmp <- sort(cut_off_tmp)
        #print(names(df)[i])

        #assert (length(cut_off_tmp) == 6)
        cut_off_tmp <- unique(cut_off_tmp)
        #print(cut_off_tmp)
        cut_off <- signif(cut_off_tmp, 3)
        cut_off <- unique(cut_off)
        #print (cut_off)

      } else {
        stop('ERROR: please specify cut_off_tmp correct method for splitting:  quantile, k_means or decision_tree. input invalid!')
      }



      # Generate cut_vec for downstream usage if (print_categories == TRUE)
      if (print_categories == TRUE)
      {
        #print(names(df)[i])
        # print(cut_off)
        l <- list(cut_off)
        #print("*****************************l***************************")
        #print(l)
        names(l)[1] <- names(df)[i]
        cut_vec <- append(cut_vec, l)
        #print("****************************cut_vec*************************")
        #print(cut_vec)
      }  #update



      # further processing for cut_off
      if (length(cut_off) <= 2) {
        df[, i] <- as.factor(df[, i])
        df_new[, i] <- as.factor(df_new[, i])
      } else {
        #avoid produce NaN value at cut due to round down
        cut_off <- c(cut_off[cut_off < max(cut_off)], max(cut_off)*1.2)
        cut_off <- c(cut_off[cut_off > min(cut_off)], min(cut_off)*0.8)
        cut_off <- sort(cut_off)
        df[, i] <-
          cut(
            df[, i],
            breaks = cut_off,
            right = F,
            include.lowest = T,
            dig.lab = 3
          )

        ## delete min and max for the Interval after discretion: train_set
        # xmin<-unlist(strsplit(levels(df[,i])[1],','))[1] xmax<-unlist(strsplit(levels(df[,i])[length(levels(df[,i]))],','))[2]
        df[, i]<-delete_min_max(df[, i])


        ## make conresponding cutvec for validation_set: cut_off_newdata
        cut_off_newdata <- cut_off
        cut_off_newdata[1] <- min(cut_off_newdata[1], floor(min(df_new[, i])))
        cut_off_newdata[length(cut_off_newdata)] <- max(cut_off_newdata[length(cut_off_newdata)], ceiling(max(df_new[, i])))
        cut_off_newdata_1 <- signif(cut_off_newdata, 3)
        cut_off_newdata_1 <- unique(cut_off_newdata_1)  ###revised update##
        #print(cut_off_newdata_1)

        ## Cut make conresponding cutvec for validation_set: cut_off_newdata
        df_new[, i] <-
          cut(
            df_new[, i],
            breaks = cut_off_newdata_1,
            right = F,
            include.lowest = T,
            dig.lab = 3
          )
        # xmin<-as.character(min(cut_off_newdata_1)) xmax<-as.character(max(cut_off_newdata_1))

        ## delete min and max for the Interval after discretion: validation_set
        df_new[, i]<-delete_min_max(df_new[, i])


      }
      # print(summary(df[,i]))update print(summary(df_new[,i]))update


    }

    if (print_categories == TRUE)
      return(list(df = df, df_new = df_new, cut_vec = cut_vec))
    else
      return(list(df = df, df_new = df_new))
  }


#' @title Build-in Function: Categorizing continuous variables based on fixed cut_vec(AutoScore Module 2)
#' @param df dataset to be processed
#' @return  Processed \code{data.frame} after categorizing based on fixed cut_vec
transform_df_fixed <- function(df, cut_vec) {
  j <- 1

  # for loop going through all variables
  for (i in 1:(length(df) - 1)) {

    if (class(df[, i]) == "factor") {
      if (length(levels(df[, i])) < 10)
        (next)() else stop("ERROR: The number of categories should be less than 9")
    }

    ## make conresponding cutvec for validation_set: cut_vec_new
    cut_vec_new <- c(floor(min(df[, i])), cut_vec[[j]], ceiling(max(df[, i])))
    cut_vec_new_tmp <- signif(cut_vec_new, 3)
    cut_vec_new_tmp <- unique(cut_vec_new_tmp)  ###revised update##
    df[, i] <- cut(df[, i], breaks = cut_vec_new_tmp, right = F, include.lowest = T, dig.lab = 3)
    # xmin<-as.character(min(cut_vec_new_tmp)) xmax<-as.character(max(cut_vec_new_tmp))

    ## delete min and max for the Interval after discretion: validation_set
    df[, i]<-delete_min_max(df[, i])
    j <- j + 1
  }
  return(df)
}









#' @title Build-in Function: Plotting ROC curve
#' @param prob Predictve probability
#' @param labels Actual outcome(binary)
#' @param quiet if set to TRUE, there will be no trace printing
#' @import pROC
plot_roc_curve <- function(prob, labels, quiet = TRUE) {
  #library(pROC)
  # prob<-predict(model.glm,newdata=X_test, type = 'response')
  model_roc <- roc(labels, prob, quiet = quiet)
  auc <- auc(model_roc)

  roc.data <- data.frame(fpr = as.vector(coords(model_roc, "local maximas", ret = "1-specificity", transpose = TRUE)), tpr = as.vector(coords(model_roc,
                                                                                                                                             "local maximas", ret = "sensitivity", transpose = TRUE)))
  p <- ggplot(roc.data, aes(x = fpr, ymin = 0, ymax = tpr)) + geom_ribbon(alpha = 0.2) + geom_line(aes(y = tpr)) + xlab("1-Specificity") +
    ylab("Sensitivity") + ggtitle(paste0("Receiver Operating Characteristic (ROC) Curve \nAUC=", round(auc, digits = 4)))
  print(p)
}

#' @title Build-in Function: Change Reference category after first-step logistic regression
#' @param df A \code{data.frame} used for logistic regression
#' @param coef_vec Generated from logistic regression
#' @return Processed \code{data.frame} after changing reference category
change_reference <- function(df, coef_vec) {
  # delete label first
  df_tmp <- subset(df, select = -label)

  # one loops to go through all variable
  for (i in (1:length(df_tmp))) {
    char_tmp <- paste("^", names(df_tmp)[i], sep = "")
    coef_tmp <- coef_vec[grepl(char_tmp, names(coef_vec))]
    coef_tmp<- coef_tmp[!is.na(coef_tmp)]

    # if min(coef_tmp)<0, the current lowest one will be used for reference
    if (min(coef_tmp) < 0) {
      ref <- gsub(names(df_tmp)[i], "", names(coef_tmp)[which.min(coef_tmp)])
      df_tmp[, i] <- relevel(df_tmp[, i], ref = ref)
    }
  }

  # add lable again
  df_tmp$label <- df$label#df_tmp
  return(df_tmp)
}

#' @title Build-in Function: Add baselines after second-step logistic regression
#' @param df A \code{data.frame} used for logistic regression
#' @param coef_vec Generated from logistic regression
#' @return Processed \code{vector} for generating the scoring table
add_baseline <- function(df, coef_vec) { # Proposed new version
  df <- subset(df, select = -label)
  coef_names_all <- unlist(lapply(names(df), function(var_name) {
    paste0(var_name, levels(df[, var_name]))
  }))
  coef_vec_all <- numeric(length(coef_names_all))
  names(coef_vec_all) <- coef_names_all
  # Remove items in coef_vec that are not meant to be in coef_vec_all
  # (i.e., the intercept)
  coef_vec_core <- coef_vec[which(names(coef_vec) %in% names(coef_vec_all))]
  i_coef <- match(x = names(coef_vec_core), table = names(coef_vec_all))
  coef_vec_all[i_coef] <- coef_vec_core
  coef_vec_all
}



#' @title Build-in Function: Automatically assign scores to each subjects given Test Set and scoring table.
#' @param df A \code{data.frame} used for testing, where variables keep before categorization
#' @param score_table A \code{vector} containing the scoring table
#' @return Processed \code{data.frame} with assigned scores for each variables
assign_score <- function(df, score_table) {
  for (i in 1:(length(names(df))-1)) {
    score_table_tmp <- score_table[grepl(names(df)[i], names(score_table))]
    df[, i] <- as.character(df[, i])
    for (j in 1:length(names(score_table_tmp))) {
      df[, i][df[, i] %in% gsub(names(df)[i], "", names(score_table_tmp)[j])] <- score_table_tmp[j]
    }

    df[, i] <- as.numeric(df[, i])
  }

  return(df)
}




#' Inpatient blood glucose data for 1200 patients
#'
#' @description A simulated dataset containing the variability of inpatient
#'   point-of-care blood glucose (BG) measurements from 1200 non-critical care
#'   adult patients in medical ward. BG variability is measured as the standard
#'   deviation of the BG readings within a day. Data was simulated based on real
#'   data.
#'
#' @format A data frame with 1200 rows and 7 variables:
#' \describe{
#'   \item{subject_id}{Subject ID of each patient.}
#'   \item{case_id}{Case ID, with \code{1} and \code{2} referring to the first
#'   and second follow-up respectively.}
#'   \item{y}{BG variability of the first and second follow-up.}
#'   \item{t}{Binary indicator for the second follow-up.}
#'   \item{sd0}{Baseline BG variability.}
#'   \item{age}{Patients' age.}
#'   \item{female}{Binary indicator for being female.}
#' }
"sample_data"




