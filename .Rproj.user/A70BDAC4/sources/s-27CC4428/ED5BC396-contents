# Pipeline_function --------------------------------------------------------

#' @title AutoScore STEP(i): Rank variables with machine learning (AutoScore Module 1)
#' @param train_set A processed \code{data.frame} that contains data to be analyzed, for training.
#' @param ntree Number of trees in the random forest algorithm (Default: 100).
#'
#' @details The first step in the AutoScore framework is variable ranking. We use random forest (RF),
#' an ensemble machine learning algorithm, to identify the top-ranking predictors for subsequent score generation.
#' This step correspond to Module 1 in the AutoScore paper.
#' @return Returns a vector containing the list of variables and its ranking generated by machine learning (random forest)
#' @examples
#' # see AutoScore Guidebook for the whole 5-step workflow
#' data("sample_data")
#' names(sample_data)[names(sample_data) == "Mortality_inpatient"] <- "label"
#' ranking <- AutoScore_rank(sample_data, ntree=100)
#' @references
#' \itemize{
#'  \item{Breiman, L. (2001), Random Forests, Machine Learning 45(1), 5-32}
#'  \item{Xie F, Chakraborty B, Ong MEH, Goldstein BA, Liu N. AutoScore: A Machine Learning-Based Automatic Clinical Score Generator and
#'   Its Application to Mortality Prediction Using Electronic Health Records. JMIR Medical Informatics 2020;8(10):e21798}
#' }
#' @seealso AutoScore_parsimony, AutoScore_weighting, AutoScore_fine_tuning, AutoScore_testing
#' @export
#' @importFrom randomForest randomForest importance
#'
AutoScore_rank <- function(train_set, ntree = 100) {
  #set.seed(4)
  train_set$label <- as.factor(train_set$label)
  model <- randomForest::randomForest(label ~ ., data = train_set, ntree = ntree, preProcess = "scale")

  # estimate variable importance
  importance <- randomForest::importance(model, scale = F)

  # summarize importance
  names(importance) <- rownames(importance)
  importance <- sort(importance, decreasing = T)
  cat("The ranking based on variable importance was shown below for each variable: \n")
  print(importance)
  return(importance)
}


#' @title AutoScore STEP(ii): Select the best model with parsimony plot (AutoScore Modules 2+3+4)
#' @param train_set A processed \code{data.frame} that contains data to be analyzed, for training.
#' @param validation_set A processed \code{data.frame} that contains data for validation purpose.
#' @param rank the raking result generated from AutoScore STEP(i) \code{AutoScore_rank}
#' @param n_min Minimum number of selected variables (Default: 1).
#' @param n_max Maximum number of selected variables (Default: 20).
#' @param max_score Maximum total score (Default: 100).
#' @param cross_validation If set to \code{TRUE}, cross-validation would be used for generating parsimony plot, which is
#'   suitable for small-size data. Default to \code{FALSE}
#' @param fold The number of folds used in cross validation (Default: 10). Available if cross_validation = TRUE.
#' @param categorize  Methods for categorize continuous variables. Options include "quantile" or "kmeans" (Default: "quantile").
#' @param quantiles Predefined quantiles to convert continuous variables to categorical ones. (Default: c(0, 0.05, 0.2, 0.8, 0.95, 1)) Available if \code{categorize = "quantile"}.
#' @param max_cluster The max number of cluster (Default: 5). Available if categorize = "kmeans".
#' @param do_trace If set to TRUE, all results based on each fold of cross-validation would be printed out and plotted (Default: FALSE). Available if cross_validation = TRUE.
#' @details This is the second step of the general AutoScore workflow, to generate the parsimony plot to help select parsimonious model.
#'  In this step, it goes through AutoScore Module 2,3 and 4 multiple times and to evaluate the performance under different variable list.
#'  The generated parsimony plot would give researcher an intuitive figure to choose the best models.
#'  If data size is small (ie <5000), an independent validation set may not be a wise choice. Then we will use cross-validation
#'  to maximize the utility of data. Set \code{cross_validation=TRUE}.
#' @return List of AUC value for different number of variables
#' @examples
#' # see AutoScore Guidebook for the whole 5-step workflow
#' data("sample_data")
#' names(sample_data)[names(sample_data) == "Mortality_inpatient"] <- "label"
#' out_split <- split_data(data = sample_data, ratio = c(0.7, 0.1, 0.2))
#' train_set <- out_split$train_set
#' validation_set <- out_split$validation_set
#' ranking <- AutoScore_rank(train_set, ntree=100)
#' AUC <- AutoScore_parsimony(
#' train_set,
#' validation_set,
#' rank = ranking,
#' max_score = 100,
#' n_min = 1,
#' n_max = 20,
#' categorize = "quantile",
#' quantiles = c(0, 0.05, 0.2, 0.8, 0.95, 1)
#' )
#' @references
#' \itemize{
#'  \item{Xie F, Chakraborty B, Ong MEH, Goldstein BA, Liu N, AutoScore: A Machine Learning-Based Automatic Clinical
#'   Score Generator and Its Application to Mortality Prediction Using Electronic Health Records,
#'   JMIR Med Inform 2020;8(10):e21798, doi: 10.2196/21798}
#' }
#' @seealso AutoScore_rank, AutoScore_parsimony, AutoScore_weighting, AutoScore_fine_tuning, AutoScore_testing
#' @export
#' @import  pROC
AutoScore_parsimony <- function(train_set, validation_set, rank, max_score = 100, n_min = 1, n_max = 20, cross_validation = FALSE, fold = 10,
                                categorize = "quantile", quantiles = c(0, 0.05, 0.2, 0.8, 0.95, 1), max_cluster = 5, do_trace = FALSE) {
  if(n_max > length(rank)){
    warning("WARNING: the n_max (",n_max,") is larger the number of all variables (",length(rank),"). We Automatically revise the n_max to ",length(rank))
    n_max <- length(rank)
  }
  # Cross Validation scenario
  if(cross_validation == TRUE){

  # Divide the data equally into n fold, record its index number
  #set.seed(4)
  index<-list()
  all<-1:length(train_set[,1])
  for(i in 1:(fold-1)){
    a<-sample(all,trunc(length(train_set[,1])/fold))
    index<-append(index,list(a))
    all<-all[!(all %in% a)]
  }
  index<-c(index,list(all))

  # Create a new variable auc_set to store all AUC value during the cross-validation
  auc_set<-data.frame(rep(0,n_max-n_min+1))

  # for each fold, generate train_set and validation_set
  for(j in 1:fold){
    validation_set_temp <- train_set[index[[j]],]
    train_set_tmp <- train_set[-index[[j]],]

    #variable_list <- names(rank)
    AUC <- c()

    # Go through AUtoScore Module 2/3/4 in the loop
    for (i in n_min:n_max) {
      variable_list<-names(rank)[1:i]
      train_set_1 <- train_set_tmp[, c(variable_list, "label")]
      validation_set_1 <- validation_set_temp[, c(variable_list, "label")]

      model_roc<-compute_auc_val(train_set_1, validation_set_1, variable_list, categorize, quantiles, max_cluster, max_score)
      #print(auc(model_roc))
      AUC <- c(AUC, auc(model_roc))
    }

    # plot parsimony plot for each fold
    names(AUC) <- n_min:n_max

    # only print and plot when do_trace = TRUE
    if(do_trace){
    print(paste("list of AUC values for fold",j))
    print(data.frame(AUC))
    plot(AUC, main = paste("Parsimony plot (cross validation) for fold",j), xlab = "Number of Variables", ylab = "Area Under the Curve", col = "cadetblue1",
         lwd = 2, type = "o")}

    # store AUC result from each fold into "auc_set"
    auc_set<-cbind(auc_set,data.frame(AUC))
  }

  # finish loop and then output final results averaged by all folds
  auc_set$rep.0..n_max...n_min...1.<-NULL
  auc_set$sum<-rowSums(auc_set)/fold
  cat("***list of final mean AUC values through cross-validation are shown below \n")
  print(data.frame(auc_set$sum))
  plot(auc_set$sum, main = paste("Final Parsimony Plot based on ", fold, "-fold Cross Validation",sep = ""), xlab = "Number of Variables", ylab = "Area Under the Curve", col = "cadetblue1",
       lwd = 2, type = "o")
  return(auc_set)}


  # if Cross validation is FALSE
  else{
  AUC <- c()

  # Go through AutoScore Module 2/3/4 in the loop
  for (i in n_min:n_max) {
    cat(paste("Select",i,"Variable(s):  "))

    variable_list<-names(rank)[1:i]
    train_set_1 <- train_set[, c(variable_list, "label")]
    validation_set_1 <- validation_set[, c(variable_list, "label")]
    model_roc<-compute_auc_val(train_set_1, validation_set_1,variable_list, categorize, quantiles, max_cluster, max_score)
    print(auc(model_roc))
    AUC <- c(AUC, auc(model_roc))
  }

  # output final results and plot parsimony plot
  names(AUC) <- n_min:n_max
  #cat("list of AUC values are shown below")
  #print(data.frame(AUC))
  plot(AUC, main = "Parsimony Plot on the Validation Set", xlab = "Number of Variables", ylab = "Area Under the Curve", col = "red",
       lwd = 2, type = "o")

  return(AUC)
}
}


#' @title AutoScore STEP(iii): Generate the initial score with the final list of variables (Re-run AutoScore Modules 2+3)
#' @param train_set A processed \code{data.frame} that contains data to be analyzed, for training.
#' @param validation_set A processed \code{data.frame} that contains data for validation purpose.
#' @param final_variables A vector containing the list of selected variables, selected from Step(2). See Guidebook
#' @param max_score Maximum total score (Default: 100).
#' @param categorize  Methods for categorize continuous variables. Options include "quantile" or "kmeans" (Default: "quantile").
#' @param quantiles Predefined quantiles to convert continuous variables to categorical ones. (Default: c(0, 0.05, 0.2, 0.8, 0.95, 1)) Available if \code{categorize = "quantile"}.
#' @param max_cluster The max number of cluster (Default: 5). Available if categorize = "kmeans".
#' @return Generated \code{cut_vec} for downstream fine-tuning process [STEP(iv)]
#' @examples
#' data("sample_data")
#' names(sample_data)[names(sample_data) == "Mortality_inpatient"] <- "label"
#' out_split <- split_data(data = sample_data, ratio = c(0.7, 0.1, 0.2))
#' train_set <- out_split$train_set
#' validation_set <- out_split$validation_set
#' ranking <- AutoScore_rank(train_set, ntree=100)
#' num_var <- 6
#' final_variables <- names(ranking[1:num_var])
#' cut_vec <- AutoScore_weighting(
#' train_set,
#' validation_set,
#' final_variables,
#' max_score = 100,
#' categorize = "quantile",
#' quantiles = c(0, 0.05, 0.2, 0.8, 0.95, 1)
#' )
#' @references
#' \itemize{
#'  \item{Xie F, Chakraborty B, Ong MEH, Goldstein BA, Liu N. AutoScore: A Machine Learning-Based Automatic Clinical Score Generator and
#'   Its Application to Mortality Prediction Using Electronic Health Records. JMIR Medical Informatics 2020;8(10):e21798}
#' }
#' @export
#' @import pROC ggplot2
AutoScore_weighting <- function(train_set, validation_set, final_variables, max_score = 100, categorize = "quantile", max_cluster = 5, quantiles = c(0, 0.05, 0.2, 0.8, 0.95, 1)) {
  # prepare train_set and Validation Set
  cat("****Included Variables: \n")
  print(data.frame(variable_name = final_variables))
  train_set_1 <- train_set[, c(final_variables, "label")]
  validation_set_1 <- validation_set[, c(final_variables, "label")]

  # AutoScore Module 2 : cut numeric and transfer categories and generate "cut_vec"
  cut_vec <- get_cut_vec(train_set_1, categorize = categorize, quantiles = quantiles, max_cluster = max_cluster)
  train_set_2 <- transform_df_fixed(train_set_1, cut_vec)
  validation_set_2 <- transform_df_fixed(validation_set_1, cut_vec)

  # AutoScore Module 3 : Score weighting
  score_table<-compute_score_table(train_set_2,max_score,final_variables)
  cat("****Initial Scores: \n")
  #print(as.data.frame(score_table))
  print_scoring_table(scoring_table = score_table, final_variable = final_variables)

  # Using "assign_score" to generate score based on new dataset and Scoring table "score_table"
  validation_set_3 <- assign_score(validation_set_2, score_table)
  validation_set_3$total_score <- rowSums(subset(validation_set_3, select = names(validation_set_3)[names(validation_set_3)!="label"]))
  y_validation <- validation_set_3$label

  # Intermediate evaluation based on Validation Set
  plot_roc_curve(validation_set_3$total_score, as.numeric(y_validation) - 1)
  cat("***Performance (based on validation set):\n")
  print_roc_performance(y_validation, validation_set_3$total_score, threshold = "best")
  cat("***The cutoffs of each variable generated by the AutoScore are saved in cut_vec. You can decide whether to revise or fine-tune them \n")
  #print(cut_vec)
  return(cut_vec)
}


#' @title AutoScore STEP(iv): Fine-tune the score by revising cut_vec with domain knowledge (AutoScore Module 5)
#' @description Domain knowledge is essential in guiding risk model development.
#'  For continuous variables, the variable transformation is a data-driven process (based on "quantile" or "kmeans" ).
#'  In this step, the automatically generated cutoff values for each continuous variable can be fine-tuned
#'  by combining, rounding, and adjusting according to the standard clinical norm.  Revised \code{cut_vec} will be input  with domain knowledge to
#' update scoring table. User can choose any cut-off values/any number of categories. Then final Scoring table will be generated. See Guidebook.
#' @param train_set A processed \code{data.frame} that contains data to be analyzed, for training.
#' @param validation_set A processed \code{data.frame} that contains data for validation purpose.
#' @param final_variables A vector containing the list of selected variables, selected from Step(2). See Guidebook
#' @param max_score Maximum total score (Default: 100).
#' @param cut_vec Generated from STEP(iii) \code{AutoScore_weighting()}.Please follow the guidebook
#' @return Generated final table of scoring model for downstream testing
#' @examples
#' \dontrun{
#' scoring_table <- AutoScore_fine_tuning(train_set, validation_set,
#'  final_variables, cut_vec, max_score = 100)}
#' @references
#' \itemize{
#'  \item{Xie F, Chakraborty B, Ong MEH, Goldstein BA, Liu N. AutoScore: A Machine Learning-Based Automatic Clinical Score Generator and
#'   Its Application to Mortality Prediction Using Electronic Health Records. JMIR Medical Informatics 2020;8(10):e21798}
#' }
#' @export
#' @import pROC ggplot2
AutoScore_fine_tuning <- function(train_set, validation_set, final_variables, cut_vec, max_score = 100) {
  # Prepare train_set and Validation Set
  train_set_1 <- train_set[, c(final_variables, "label")]
  validation_set_1 <- validation_set[, c(final_variables, "label")]

  # AutoScore Module 2 : cut numeric and transfer categories (based on fix "cut_vec" vector)
  train_set_2 <- transform_df_fixed(train_set_1, cut_vec = cut_vec)
  validation_set_2 <- transform_df_fixed(validation_set_1, cut_vec = cut_vec)

  # AutoScore Module 3 : Score weighting
  score_table<-compute_score_table(train_set_2,max_score,final_variables)
  cat("***Fine-tuned Scores: \n")
  #print(as.data.frame(score_table))
  print_scoring_table(scoring_table = score_table, final_variable = final_variables)

  # Using "assign_score" to generate score based on new dataset and Scoring table "score_table"
  validation_set_3 <- assign_score(validation_set_2, score_table)
  validation_set_3$total_score <- rowSums(subset(validation_set_3, select = names(validation_set_3)[names(validation_set_3)!="label"])) ## which name ="label"
  y_validation <- validation_set_3$label

  # Intermediate evaluation based on Validation Set after fine-tuning
  plot_roc_curve(validation_set_3$total_score, as.numeric(y_validation) - 1)
  cat("***Performance (based on validation set, after fine-tuning):\n")
  print_roc_performance(y_validation, validation_set_3$total_score, threshold = "best")
  return(score_table)
}


#' @title AutoScore STEP(v): Evaluate the final score with ROC analysis (AutoScore Module 6)
#' @description Domain knowledge is essential in guiding risk model development.
#'  For continuous variables, the variable transformation is a data-driven process (based on "quantile", "kmeans" or "decision_tree).
#'  In this step, the automatically generated cutoff values for each continuous variable can be fine-tuned
#'  by combining, rounding, and adjusting according to the standard clinical norm.  Revised \code{cut_vec} will be input  with domain knowledge to
#' update scoring table. User can choose any cut-off values/any number of categories. Then final Scoring table will be generated. See Guidebook.
#' @param test_set A processed \code{data.frame} that contains data for testing purpose. This \code{data.frame} should have same format as
#'        \code{train_set} (same variable names and outcomes)
#' @param final_variables A vector containing the list of selected variables, selected from Step(ii). See Guidebook
#' @param scoring_table The final scoring table after fine-tuning, generated from STEP(iv) \code{AutoScore_fine_tuning}.Please follow the guidebook
#' @param cut_vec Generated from STEP(iii) \code{AutoScore_weighting()}.Please follow the guidebook
#' @param threshold Score threshold for the ROC analysis to generate sensitivity, specificity, etc. If set to "best", the optimal threshold will be calculated (Default:"best").
#' @param with_label Set to TRUE if there are labels in the test_set and performance will be evaluated accordingly (Default:TRUE).
#' Set it to "FALSE" if there are not "label" in the "test_set" and the final predicted scores will be the output without performance evaluation.
#' @return A data frame with predicted score and the outcome for downstream visualization.
#' @examples
#' \dontrun{
#' pred_score <- AutoScore_testing(test_set, final_variables, cut_vec,
#' scoring_table, threshold = "best", with_label = TRUE)
#' }
#' @references
#' \itemize{
#'  \item{Xie F, Chakraborty B, Ong MEH, Goldstein BA, Liu N. AutoScore: A Machine Learning-Based Automatic Clinical Score Generator and
#'   Its Application to Mortality Prediction Using Electronic Health Records. JMIR Medical Informatics 2020;8(10):e21798}
#' }
#' @export
#' @import pROC ggplot2
AutoScore_testing <- function(test_set, final_variables, cut_vec, scoring_table, threshold = "best", with_label = TRUE) {
  if(with_label){
  # prepare test set: categorization and "assign_score"
  test_set_1 <- test_set[, c(final_variables, "label")]
  test_set_2 <- transform_df_fixed(test_set_1, cut_vec = cut_vec)
  test_set_3 <- assign_score(test_set_2, scoring_table)
  test_set_3$total_score <- rowSums(subset(test_set_3, select = names(test_set_3)[names(test_set_3)!="label"]))
  test_set_3$total_score[which(is.na(test_set_3$total_score))]<-0
  y_test <- test_set_3$label

  # Final evaluation based on testing set
  plot_roc_curve(test_set_3$total_score, as.numeric(y_test) - 1)
  cat("***Performance using AutoScore (based on unseen test Set):\n")
  model_roc <- roc(y_test, test_set_3$total_score, quiet = T)
  print_roc_performance(y_test, test_set_3$total_score, threshold = threshold)
  #Modelprc <- pr.curve(test_set_3$total_score[which(y_test == 1)],test_set_3$total_score[which(y_test == 0)],curve = TRUE)
  #values<-coords(model_roc, "best", ret = c("specificity", "sensitivity", "accuracy", "npv", "ppv", "precision"), transpose = TRUE)
  pred_score <- data.frame(pred_score = test_set_3$total_score, Label = y_test)
  return(pred_score)

  } else { test_set_1 <- test_set[, c(final_variables)]
  test_set_2 <- transform_df_fixed(test_set_1, cut_vec = cut_vec)
  test_set_3 <- assign_score(test_set_2, scoring_table)
  test_set_3$total_score <- rowSums(subset(test_set_3, select = names(test_set_3)[names(test_set_3)!="label"]))
  test_set_3$total_score[which(is.na(test_set_3$total_score))]<-0
  pred_score <- data.frame(pred_score = test_set_3$total_score, Label = NA)
  return(pred_score)}
}


# Direct_function ---------------------------------------------------------

# preprocess
# preprocessing: requirement of data finish preselection first 1.data type(only numeric or factor),all character will be transformed
# to factor automatically 2.point out the outcome(should be factor and binary outcome) 3.The number of categories for each factor
# should be less than 9
# Preprocess(data,outcome) outcome: character class object wich point to the name of the outcome in the dataset data: should be a
# dataframe

#' @title AutoScore function: Check whether the input dataset fulfill the requirement of the AutoScore
#' @param data The data to be checked
#' @examples
#' data("sample_data")
#' names(sample_data)[names(sample_data) == "Mortality_inpatient"] <- "label"
#' check_data(sample_data)
#' @export
check_data<-function(data){
  #1. check label and binary
  if(is.null(data$label)) stop("ERROR: for this dataset: These is no dependent variable -lable- to indicate the outcome. Please add one first")
  if(length(levels(factor(data$label)))!=2) warning("Please keep outcome lable variable binary")

  #2. check each variable
  non_num_fac<-c()
  fac_large<-c()
  special_case<-c()

  for (i in names(data)) {
    if ((class(data[[i]]) != "factor") && (class(data[[i]]) != "numeric"))  non_num_fac<-c(non_num_fac,i)
    if ((length(levels(data[[i]])) > 10) && (class(data[[i]]) == "factor"))   fac_large<-c(fac_large,i)
    if (grepl(",",i)) warning(paste("WARNING: the dataset has variable names",i, "with character , please change it"))
    if (grepl(")",i)) warning(paste("WARNING: the dataset has variable names",i, "with character ) please change it"))
       }

  if(!is.null(non_num_fac)) warning(paste("WARNING: the dataset has variable of character and they will be transformed to factor:",non_num_fac))
  if(!is.null(fac_large)) warning(paste("WARNING: The number of categories for some variables is too many :larger than:",fac_large))

  #3. check missing values
  missing_rate<-colSums(is.na(data))
  if(sum(missing_rate)) {
    warning("WARNING: Your dataset contains NA. Please handle them before AutoScore. The variables with missing values are shown below:")
    print(missing_rate[missing_rate!=0])
  }
  else cat("missing value check passed.\n")
  #cat("Please fixed the problem of your dataset before AutoScore if you see any Wanings below.\n")
}
# check 1. missing value or not label is there and binary or not 3.only factor and numeric: 4. factor larger than 10?


#' @title AutoScore function: Automatically splitting dataset to train, validation and test set
#' @param data The dataset ro be splitted
#' @param ratio The ratio to divide dataset into train, validation and test set.(Default: c(0.7, 0.1, 0.2))
#' @param cross_validation If set to \code{TRUE}, cross-validation would be used for generating parsimony plot, which is
#'   suitable for small-size data. Default to \code{FALSE}
#' @return Returns a list containing training, validation and testing set
#' @examples
#' data("sample_data")
#' set.seed(4)
#' #large sample size
#' out_split <- split_data(data = sample_data, ratio = c(0.7, 0.1, 0.2))
#' #small sample size
#' out_split <- split_data(data = sample_data, ratio = c(0.7, 0, 0.3), cross_validation = TRUE)
#' @export
split_data <- function(data, ratio, cross_validation = FALSE) {
  # non cross validation: default
  if (cross_validation == FALSE) {
    n <- length(data[, 1])
    test_ratio <- ratio[3] / sum(ratio)
    validation_ratio <- ratio[2] / sum(ratio)
    #set.seed(4)
    test_index <- sample((1:n), test_ratio * n)
    validate_index <-
      sample((1:n)[!(1:n) %in% test_index], validation_ratio * n)
    train_set <- data[-c(validate_index, test_index), ]
    test_set <- data[test_index, ]
    validation_set <- data[validate_index, ]

    return(list(
      train_set = train_set,
      validation_set = validation_set,
      test_set = test_set
    ))
  }

  #  cross validation: train = validation
  else{
    n <- length(data[, 1])
    test_ratio <- ratio[3] / sum(ratio)
    validation_ratio <- ratio[2] / sum(ratio)
    #set.seed(4)
    test_index <- sample((1:n), test_ratio * n)
    validate_index <-
      sample((1:n)[!(1:n) %in% test_index], validation_ratio * n)
    train_set <- data[-c(test_index),]
    test_set <- data[test_index,]
    validation_set <- train_set

    return(list(
      train_set = train_set,
      validation_set = validation_set,
      test_set = test_set
    ))
  }

}


#' @title AutoScore function: Descriptive Analysis
#' @description Compute descriptive table (usually Table 1 in medical literature) for the dataset.
#' @param df data frame after checking
#' @examples
#' data("sample_data")
#' names(sample_data)[names(sample_data) == "Mortality_inpatient"] <- "label"
#' compute_descriptive_table(sample_data)
#' @export
#' @import tableone
# Generate table one based on stratified outcomes
compute_descriptive_table<- function(df) {
  descriptive_table <- CreateTableOne(vars = names(df), strata = "label", data = df)
  descriptive_table_overall <- CreateTableOne(vars = names(df), data = df)
  print(descriptive_table)
  print(descriptive_table_overall)
}


#' @title AutoScore function: Univariable Analysis
#' @description Perform univariable analysis and generate the result table with odd ratios.
#' @param df data frame after checking
#' @return result of univariate analysis
#' @examples
#' data("sample_data")
#' names(sample_data)[names(sample_data) == "Mortality_inpatient"] <- "label"
#' uni_table<-compute_uni_variable_table(sample_data)
#' @export
# Univariable analysis
compute_uni_variable_table <- function(df) {
  uni_table <- data.frame()
  for (i in names(df)[names(df) != "label"]) {
    model <- glm(as.formula("label ~ ."), data = subset(df, select = c("label", i)), family = binomial, na.action = na.omit)
    a <- cbind(exp(cbind(OR = coef(model), confint.default(model))), summary(model)$coef[, "Pr(>|z|)"])
    uni_table <- rbind(uni_table, a)
  }
  uni_table <- uni_table[!grepl("Intercept", row.names(uni_table), ignore.case = T), ]
  uni_table <- round(uni_table, digits = 3)
  uni_table$V4[uni_table$V4<0.001]<-"<0.001"
  uni_table$OR <- paste(uni_table$OR, "(", uni_table$`2.5 %`, "-", uni_table$`97.5 %`, ")", sep = "")
  uni_table$`2.5 %` <- NULL
  uni_table$`97.5 %` <- NULL
  names(uni_table)[names(uni_table) == "V4"] <- "p value"
  return(uni_table)
}


#' @title AutoScore function: Multivariate Analysis
#' @description Generate tables for Multivariate Analysis
#' @param df data frame after checking
#' @return result of Multivariate Analysis
#' @examples
#' data("sample_data")
#' names(sample_data)[names(sample_data) == "Mortality_inpatient"] <- "label"
#' multi_table<-compute_multi_variable_table(sample_data)
#' @export
# full logistic model-multivariable analysis
compute_multi_variable_table <- function(df) {
  model <- glm(label ~ ., data = df, family = binomial, na.action = na.omit)
  multi_table <- cbind(exp(cbind(adjusted_OR = coef(model), confint.default(model))), summary(model)$coef[, "Pr(>|z|)"])
  multi_table <- multi_table[!grepl("Intercept", row.names(multi_table), ignore.case = T), ]
  multi_table <- round(multi_table, digits = 3)
  multi_table <- as.data.frame(multi_table)
  multi_table$V4[multi_table$V4<0.001]<-"<0.001"
  multi_table$adjusted_OR <- paste(multi_table$adjusted_OR, "(", multi_table$`2.5 %`, "-", multi_table$`97.5 %`, ")", sep = "")
  multi_table$`2.5 %` <- NULL
  multi_table$`97.5 %` <- NULL
  names(multi_table)[names(multi_table) == "V4"] <- "p value"
  return(multi_table)
}


#' @title AutoScore Function: print_scoring_table
#' @description Print scoring tables for visulization
#' @param scoring_table Raw scoring table generated by procedure
#' @param final_variable Final included variables by predictor_rank
#' @return Data frame of formatted scoring table
#' @examples
#' \dontrun{
#' print_scoring_table(scoring_table,final_variable)}
#' @export
#' @importFrom knitr kable
print_scoring_table<-function(scoring_table,final_variable){
  #library(knitr)
  table_tmp<-data.frame()
  for (i in 1:length(final_variable)) {
    var_tmp<-final_variable[i]
    var_name<-names(scoring_table)
    num<-grepl(var_tmp,var_name)
    table_1<-data.frame(name=var_name[num],value=unname(scoring_table[num]))
    rank_indicator<-gsub(".*,","",table_1$name)
    rank_indicator<-gsub(")","",rank_indicator) #>>> (Xie Feng) bugs here. not compatible for only two categories(E.g.,mwhen quantiles = c(0, 0.5, 1) )
    # rank_indicator[which(rank_indicator=="")]<-max(as.numeric(rank_indicator[-which(rank_indicator=="")]))+1
    {
      if (grepl(",",table_1$name[1])!=TRUE){
        table_1$rank_indicator<-c(seq(1:nrow(table_1)))
        interval<-c(gsub(pattern = var_tmp, replacement = "", table_1$name))
        table_1$interval<-interval
        table_2<-table_1[order(table_1$interval),]
        table_2$variable<-c(var_tmp,rep("",(nrow(table_2)-1)))
        table_3<-rbind(table_2,rep("",ncol(table_2)))
        table_tmp<-rbind(table_tmp,table_3)
      }
      else
      {
        rank_indicator[which(rank_indicator=="")]<-max(as.numeric(rank_indicator[-which(rank_indicator=="")]))+1
        rank_indicator<-as.numeric(rank_indicator)
        {
          if (length(rank_indicator)==2){
            table_1$rank_indicator<-rank_indicator
            table_2<-table_1[order(table_1$rank_indicator),]
            interval<-c(paste0("<",table_2$rank_indicator[1]))
            interval<-c(interval,paste0(">=",table_2$rank_indicator[length(rank_indicator)-1]))
            table_2$interval<-interval
            table_2$variable<-c(var_tmp,rep("",(nrow(table_2)-1)))
            table_3<-rbind(table_2,rep("",ncol(table_2)))
            table_tmp<-rbind(table_tmp,table_3)
          }
          else{
            table_1$rank_indicator<-rank_indicator
            table_2<-table_1[order(table_1$rank_indicator),]
            interval<-c(paste0("<",table_2$rank_indicator[1]))
            for (j in 1:(length(table_2$rank_indicator)-2)) {
              interval<-c(interval,paste0("[",table_2$rank_indicator[j],",",table_2$rank_indicator[j+1],")"))
            }
            interval<-c(interval,paste0(">=",table_2$rank_indicator[length(rank_indicator)-1]))
            table_2$interval<-interval
            table_2$variable<-c(var_tmp,rep("",(nrow(table_2)-1)))
            table_3<-rbind(table_2,rep("",ncol(table_2)))
            table_tmp<-rbind(table_tmp,table_3)
          }
        }
      }
    }
  }
  table_tmp<-table_tmp[1:(nrow(table_tmp)-1),]
  table_final<-data.frame(variable = table_tmp$variable,interval = table_tmp$interval, point = table_tmp$value)
  table_kable_format<-kable(table_final,align = "llc",caption = "AutoScore-created scoring model",format = "rst")
  print(table_kable_format)
  invisible(table_final)
}


# Build_in_function -------------------------------------------------------
## built-in function for AutoScore below are functions
## Those functions are cited by pepline functions

# description -
# R-oxygen
# default value.
# function description
# comments what u are trying to do.
# print(paste("Select the number of Variables",i))

#' @title Internal function: Print receiver operating characteristic (ROC) performance
#' @description Print receiver operating characteristic (ROC) performance
#' @param label outcome
#' @param score predicted score
#' @param threshold Threshold for analyze sensitivity, specificity and other metrics. Default to "best"
#' @examples
#' \dontrun{
#' print_roc_performance<-function(label, score, threshold = "best")}
#' @export
#' @import pROC
print_roc_performance<-function(label, score, threshold = "best"){
  if(sum(is.na(score))>0) warning("NA in the score: ",sum(is.na(score)))
  model_roc <- roc(label, score, quiet = T)
  cat("AUC: ", round(auc(model_roc),4),"  ")
  print(ci(model_roc))

  if(threshold == "best") {
    threshold <- ceiling(coords(model_roc, "best", ret = "threshold", transpose = TRUE))
    cat("Best score threshold: >=", threshold, "\n")
    } else {cat("Score threshold: >=",threshold,"\n")}
  cat("Other performance indicators based on this score threshold: \n")
  roc <- ci.coords(model_roc, threshold , ret = c("specificity", "sensitivity", "npv", "ppv"), transpose = TRUE)
  cat("Sensitivity: ",round(roc$sensitivity[2],4)," 95% CI: ",round(roc$sensitivity[1],4),"-",round(roc$sensitivity[3],4),"\n",sep = "")
  cat("Specificity: ",round(roc$specificity[2],4)," 95% CI: ",round(roc$specificity[1],4),"-",round(roc$specificity[3],4),"\n",sep = "")
  cat("PPV:         ",round(roc$ppv[2],4)," 95% CI: ",round(roc$ppv[1],4),"-",round(roc$ppv[3],4),"\n",sep = "")
  cat("NPV:         ",round(roc$npv[2],4)," 95% CI: ",round(roc$npv[1],4),"-",round(roc$npv[3],4),"\n",sep = "")
  }


compute_score_table<-function(train_set_2,max_score,variable_list){
  #AutoScore Module 3 : Score weighting
  # First-step logistic regression
  model <- glm(label ~ ., family = binomial(link = "logit"), data = train_set_2)
  coef_vec <- coef(model)
  if (length(which(is.na(coef_vec)))>0) {warning(" WARNING: GLM output contains NULL, Replace NULL with 1")
    coef_vec[which(is.na(coef_vec))]<-1}
  train_set_2 <- change_reference(train_set_2, coef_vec)

  # Second-step logistic regression
  model <- glm(label ~ ., family = binomial(link = "logit"), data = train_set_2)
  coef_vec <- coef(model)
  if (length(which(is.na(coef_vec)))>0) {warning(" WARNING: GLM output contains NULL, Replace NULL with 1")
    coef_vec[which(is.na(coef_vec))]<-1}

  # rounding for final scoring table "score_table"
  coef_vec_tmp <- round(coef_vec/min(coef_vec[-1]))
  score_table <- add_baseline(train_set_2, coef_vec_tmp)

  # normalization according to "max_score" and regenerate score_table
  total_max <- max_score
  total <- 0
  for (i in 1:length(variable_list)) total <- total + max(score_table[grepl(variable_list[i], names(score_table))])
  score_table <- round(score_table/(total/total_max))
  return(score_table)}


compute_auc_val <-function(train_set_1, validation_set_1, variable_list, categorize, quantiles, max_cluster, max_score){
  # AutoScore Module 2 : cut numeric and transfer categories
  cut_vec <- get_cut_vec(train_set_1, categorize = categorize, quantiles = quantiles, max_cluster = max_cluster)
  train_set_2 <- transform_df_fixed(train_set_1, cut_vec)
  validation_set_2 <- transform_df_fixed(validation_set_1, cut_vec)
  if(sum(is.na(validation_set_2))>0) warning("NA in the validation_set_2: ",sum(is.na(validation_set_2)))
  if(sum(is.na(train_set_2))>0) warning("NA in the train_set_2: ",sum(is.na(train_set_2)))

  # AutoScore Module 3 : Variable Weighting
  score_table<-compute_score_table(train_set_2, max_score,variable_list)
  if(sum(is.na(score_table))>0) warning("NA in the score_table: ",sum(is.na(score_table)))

  # Using "assign_score" to generate score based on new dataset and Scoring table "score_table"
  validation_set_3 <- assign_score(validation_set_2, score_table)
  if(sum(is.na(validation_set_3))>0) warning("NA in the validation_set_3: ",sum(is.na(validation_set_3)))

  validation_set_3$total_score <- rowSums(subset(validation_set_3, select = names(validation_set_3)[names(validation_set_3)!="label"]))
  y_validation <- validation_set_3$label
  # plot_roc_curve(validation_set_3$total_score,as.numeric(y_validation)-1)

  # calculate AUC value
  model_roc <- roc(y_validation, validation_set_3$total_score, quiet = T)

  return(model_roc) }


get_cut_vec <-
  function(df,
           quantiles = c(0, 0.05, 0.2, 0.8, 0.95, 1), #by default
           max_cluster = 5,
           categorize = "quantile") {

    # Generate cut_vec for downstream usage
    cut_vec <- list()

    for (i in 1:(length(df) - 1)) {
      # for factor variable
      if (class(df[, i]) == "factor") {
        if (length(levels(df[, i])) < 10)
          #(next)() else stop("ERROR: The number of categories should be less than 10")
          (next)()
        else
          warning("WARNING: The number of categories should be less than 10",
                names(df)[i])
      }

      # for continuous variable: variable transformation
      # select discretization method, default mode = 1

      ## mode 1 - quantiles
      if (categorize == "quantile") {
        # options(scipen = 20)
        #print("in quantile")
        cut_off_tmp <- quantile(df[, i], quantiles)
        cut_off_tmp <- unique(cut_off_tmp)
        cut_off <- signif(cut_off_tmp, 3)  # remain 3 digits
        #print(cut_off)

        ## mode 2 k-means clustering
      } else if (categorize == "k_means") {
        #print("using k-means")
        clusters <- kmeans(df[, i], max_cluster)
        cut_off_tmp <- c()
        for (j in unique(clusters$cluster)) {
          #print(min(df[,i][clusters$cluster==j]))
          #print(length(df[,i][clusters$cluster==j]))
          cut_off_tmp <- append(cut_off_tmp, min(df[, i][clusters$cluster == j]))
          #print(cut_off_tmp)
        }
        cut_off_tmp <- append(cut_off_tmp, max(df[, i]))
        cut_off_tmp <- sort(cut_off_tmp)
        #print(names(df)[i])
        #assert (length(cut_off_tmp) == 6)
        cut_off_tmp <- unique(cut_off_tmp)
        cut_off <- signif(cut_off_tmp, 3)
        cut_off <- unique(cut_off)
        #print (cut_off)

      } else {
        stop('ERROR: please specify correct method for categorizing:  "quantile" or "k_means".')
      }

      l <- list(cut_off)
      names(l)[1] <- names(df)[i]
      cut_vec <- append(cut_vec, l)
      #print("****************************cut_vec*************************")
      #print(cut_vec)
    }
      for(i in 1:length(cut_vec)) cut_vec[[i]] <- cut_vec[[i]][2:(length(cut_vec[[i]]) - 1)]
      return(cut_vec)
  }


#' @title Internal function: Categorizing continuous variables based on fixed cut_vec
#' @param df dataset to be processed
#' @param cut_vec fixed cut vector
#' @return  Processed \code{data.frame} after categorizing based on fixed cut_vec
transform_df_fixed <- function(df, cut_vec) {
  j <- 1

  # for loop going through all variables
  for (i in 1:(length(df) - 1)) {

    if (class(df[, i]) == "factor") {
      if (length(levels(df[, i])) < 10)
        (next)() else stop("ERROR: The number of categories should be less than 9")
    }

    ## make conresponding cutvec for validation_set: cut_vec_new
    #df<-validation_set_1
    #df<-train_set_1
    vec<-df[, i]
    cut_vec_new <- cut_vec[[j]]
    if(min(vec) < cut_vec[[j]][1])   cut_vec_new <- c(floor(min(df[, i]))-100, cut_vec_new)
    if(max(vec) >= cut_vec[[j]][length(cut_vec[[j]])] )  cut_vec_new <- c(cut_vec_new, ceiling(max(df[, i])+100))

    cut_vec_new_tmp <- signif(cut_vec_new, 3)
    cut_vec_new_tmp <- unique(cut_vec_new_tmp)  ###revised update##
    df[, i] <- cut(df[, i], breaks = cut_vec_new_tmp, right = F, include.lowest = F, dig.lab = 3)
    # xmin<-as.character(min(cut_vec_new_tmp)) xmax<-as.character(max(cut_vec_new_tmp))

    ## delete min and max for the Interval after discretion: validation_set
    if(min(vec) < cut_vec[[j]][1]) levels(df[, i])[1] <- gsub(".*,", "(,", levels(df[, i])[1])
    if(max(vec) >= cut_vec[[j]][length(cut_vec[[j]])] ) levels(df[, i])[length(levels(df[, i]))] <- gsub(",.*", ",)", levels(df[, i])[length(levels(df[, i]))])


    j <- j + 1
  }
  return(df)
}


#' @title Internal Function: Plotting ROC curve
#' @param prob Predicate probability
#' @param labels Actual outcome(binary)
#' @param quiet if set to TRUE, there will be no trace printing
#' @import pROC
plot_roc_curve <- function(prob, labels, quiet = TRUE) {
  #library(pROC)
  # prob<-predict(model.glm,newdata=X_test, type = 'response')
  model_roc <- roc(labels, prob, quiet = quiet)
  auc <- auc(model_roc)

  roc.data <- data.frame(fpr = as.vector(coords(model_roc, "local maximas", ret = "1-specificity", transpose = TRUE)), tpr = as.vector(coords(model_roc,
                                                                                                                                             "local maximas", ret = "sensitivity", transpose = TRUE)))
  p <- ggplot(roc.data, aes_string(x = "fpr", ymin = 0, ymax = "tpr")) + geom_ribbon(alpha = 0.2) + geom_line(aes_string(y = "tpr")) + xlab("1-Specificity") +
    ylab("Sensitivity") + ggtitle(paste0("Receiver Operating Characteristic (ROC) Curve \nAUC=", round(auc, digits = 4)))
  print(p)
}


#' @title Build-in Function: Change Reference category after first-step logistic regression
#' @param df A \code{data.frame} used for logistic regression
#' @param coef_vec Generated from logistic regression
#' @return Processed \code{data.frame} after changing reference category
change_reference <- function(df, coef_vec) {
  # delete label first
  df_tmp <- subset(df, select = names(df)[names(df)!="label"])

  # one loops to go through all variable
  for (i in (1:length(df_tmp))) {
    char_tmp <- paste("^", names(df_tmp)[i], sep = "")
    coef_tmp <- coef_vec[grepl(char_tmp, names(coef_vec))]
    coef_tmp<- coef_tmp[!is.na(coef_tmp)]

    # if min(coef_tmp)<0, the current lowest one will be used for reference
    if (min(coef_tmp) < 0) {
      ref <- gsub(names(df_tmp)[i], "", names(coef_tmp)[which.min(coef_tmp)])
      df_tmp[, i] <- relevel(df_tmp[, i], ref = ref)
    }
  }

  # add lable again
  df_tmp$label <- df$label#df_tmp
  return(df_tmp)
}


#' @title Internal Function: Add baselines after second-step logistic regression
#' @param df A \code{data.frame} used for logistic regression
#' @param coef_vec Generated from logistic regression
#' @return Processed \code{vector} for generating the scoring table
add_baseline <- function(df, coef_vec) { # Proposed new version
  df <- subset(df, select = names(df)[names(df)!="label"])
  coef_names_all <- unlist(lapply(names(df), function(var_name) {
    paste0(var_name, levels(df[, var_name]))
  }))
  coef_vec_all <- numeric(length(coef_names_all))
  names(coef_vec_all) <- coef_names_all
  # Remove items in coef_vec that are not meant to be in coef_vec_all
  # (i.e., the intercept)
  coef_vec_core <- coef_vec[which(names(coef_vec) %in% names(coef_vec_all))]
  i_coef <- match(x = names(coef_vec_core), table = names(coef_vec_all))
  coef_vec_all[i_coef] <- coef_vec_core
  coef_vec_all
}


#' @title Internal Function: Automatically assign scores to each subjects given Test Set and scoring table.
#' @param df A \code{data.frame} used for testing, where variables keep before categorization
#' @param score_table A \code{vector} containing the scoring table
#' @return Processed \code{data.frame} with assigned scores for each variables
assign_score <- function(df, score_table) {
  for (i in 1:(length(names(df))-1)) {
    score_table_tmp <- score_table[grepl(names(df)[i], names(score_table))]
    df[, i] <- as.character(df[, i])
    for (j in 1:length(names(score_table_tmp))) {
      df[, i][df[, i] %in% gsub(names(df)[i], "", names(score_table_tmp)[j])] <- score_table_tmp[j]
    }

    df[, i] <- as.numeric(df[, i])
  }

  return(df)
}


#' 20000 simulated ICU admission data, with the same distribution as the data in the MIMIC-III ICU database
#'
#' @description 20000 simulated samples, with the same distribution as the data in the MIMIC-III ICU database. It is used for demonstration only in the Guidebook.
"sample_data"


#' 1000 simulated ICU admission data, with the same distribution as the data in the MIMIC-III ICU database
#'
#' @description 1000 simulated samples, with the same distribution as the data in the MIMIC-III ICU database. It is used for demonstration only in the Guidebook.
"sample_data_small"



