---
title: "AutoScore: An Interpretable Machine Learning-Based Automatic Clinical Score Generator"
output: 
  rmarkdown::html_vignette:
    toc: yes
vignette: >
  %\VignetteIndexEntry{Guide_book}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  collapse = TRUE,
  comment = "#>",
  cache = FALSE, fig.width=7, fig.height=5
)

options(rmarkdown.html_vignette.check_title = FALSE)
knitr::opts_chunk$set()

```


- **AutoScore R package (version 0.2.0)**

# **AutoScore Introduction**
### Description

AutoScore is a novel machine learning framework to automate the development of interpretable clinical scoring models. AutoScore consists of six modules: 1) variable ranking with machine learning, 2) variable transformation, 3) score derivation, 4) model selection, 5) domain knowledge-based score fine-tuning, and 6) performance evaluation. The AutoScore is elaborated in the article (<http://dx.doi.org/10.2196/21798>) and its flowchart is shown in the following figure. AutoScore could seamlessly generate risk scores using a parsimonious set of variables, which can be easily implemented and validated in clinical practice. Moreover, it enables users to build transparent and interpretable clinical scores quickly in a straightforward manner.

```{r pressure, echo=FALSE, fig.cap="The flowchart of AutoScore", out.width = '75%', fig.align="center"}
knitr::include_graphics("Figure1.png")
```


### Functions and pipeline

The five pipeline functions:  `AutoScore_rank()`, `AutoScore_parsimony()`, `AutoScore_weighting()`, `AutoScore_fine_tuning()` and
`AutoScore_testing()` constitute the 5-step AutoScore-based process for generating point-based clinical scores. This 5-step process gives users the flexibility of customization (e.g., determining the final list of variables according to the parsimony plot, and fine-tuning the cutoffs in variable transformation). Please follow the step-by-step instructions (in Demos #1 and #2) to build your own scores.

* STEP(i): `AutoScore_rank()` - Rank variables with machine learning (AutoScore Module 1)
* STEP(ii): `AutoScore_parsimony()` - Select the best model with parsimony plot (AutoScore Modules 2+3+4)
* STEP(iii): `AutoScore_weighting()` - Generate the initial score with the final list of variables (Re-run AutoScore Modules 2+3)
* STEP(iv): `AutoScore_fine_tuning()` - Fine-tune the score by revising `cut_vec` with domain knowledge (AutoScore Module 5)
* STEP(v): `AutoScore_testing()` - Evaluate the final score with ROC analysis (AutoScore Module 6)

We also include several optional functions in the package, which could help with data analysis and result reporting. These functions are `compute_descriptive_table()` for generating the table of descriptive analysis for your dataset, `uni_table()` for creating the table of univariable analysis for your dataset, and `multi_table()` for generating the table of multivariable analysis for your dataset.



### Citation
Xie F, Chakraborty B, Ong MEH, Goldstein BA, Liu N. AutoScore: A Machine Learning-Based Automatic Clinical Score Generator and Its Application to Mortality Prediction Using Electronic Health Records. JMIR Medical Informatics 2020;8(10):e21798 (<http://dx.doi.org/10.2196/21798>)

### Contact
- Feng Xie (Email: <xief@u.duke.nus.edu>)
- Nan Liu (Email: <liu.nan@duke-nus.edu.sg>)

# **AutoScore Demonstration**

- [Install /load the AutoScore package](#Demo0) and prepare the data.
- In [Demo #1](#Demo1), we demonstrate the use of AutoScore on a comparably large dataset where separate training and validation datasets are available. 
- In [Demo #2](#Demo2), we demonstrate the use of AutoScore on a comparably small dataset where no sufficient samples are available to form separate training and validation datasets. Thus, cross-validation is employed to create the parsimony plot.

<h id="Demo0">

## **Install the package and prepare data**
### Install the development version from GitHub or the stable version from CRAN (recommended):


```{r basic, eval=FALSE}
# From Github
install.packages("devtools")
library(devtools)
install_github(repo = "nliulab/AutoScore")

# From CRAN (recommended)
install.packages("AutoScore")
```
[devtools]: https://github.com/hadley/devtools

### Load R package

```{r  library, results = "hide", warning=FALSE, message=FALSE}
library(AutoScore)
```

### Load data 
- Read data from CSV or Excel files.
- For this demo, use the integrated `sample_data` in the package.
- `sample_data` has 20000 simulated samples, with the same distribution as the data in the MIMIC-III ICU database (<https://mimic.physionet.org/>).
```{r}
data("sample_data")
head(sample_data)
```

### Data preprocessing (Users to check the following)
- Handle missing values (AutoScore requires a complete dataset).
- Remove special characters from variable names, e.g., `[`, `]`, `(`, `)`,`,`.
- Ensure that the dependent variable (outcome) should be binary, and its name should be changed to "label" (Can use the codes below to do it).
- Independent variables should be numeric (class: num/int) or categorical (class: factor/logic).
- Handle outliers (optional).
- Check variable distribution (optional).

### AutoScore preprocessing (Users to check the following)
- Change the name of outcome to "label" (make sure no variables using the same name).
```{r}
names(sample_data)[names(sample_data) == "Mortality_inpatient"] <- "label"
```

- Check if data fulfill the basic requirement by AutoScore.
- Fix the problem if you see any warnings.
```{r}
check_data(sample_data)
```

- Modify your data to ensure no warning messages.

<h id="Demo1">

## **AutoScore Demo #1: Large dataset (sample size = 20000)**


In Demo #1, we demonstrate the use of AutoScore on a comparably large dataset where separate training and validation sets are available. 
Please note that it is just a demo using simulated data, and thus, the result might not be clinically meaningful.

### Prepare training, validation, and test datasets
- Option 1: Prepare three separate datasets to train, validate, and test models.
- Option 2: Use demo codes below to randomly split your dataset into training, validation, and test datasets (70%, 10%, 20%, respectively).
```{r}
set.seed(4)
out_split <- split_data(data = sample_data, ratio = c(0.7, 0.1, 0.2))
train_set <- out_split$train_set
validation_set <- out_split$validation_set
test_set <- out_split$test_set
```


### STEP(i): Generate variable ranking list (AutoScore Module 1)
- `ntree`: Number of trees in the random forest algorithm (Default: 100).
```{r}
ranking <- AutoScore_rank(train_set, ntree = 100)
```

### STEP(ii): Select the best model with parsimony plot (AutoScore Modules 2+3+4)
- `nmin`: Minimum number of selected variables (Default: 1).
- `nmax`: Maximum number of selected variables (Default: 20).
- `categorize`: Methods for categorizing continuous variables. Options include `"quantile"` or `"kmeans"` (Default: `"quantile"`).
- `quantiles`: Predefined quantiles to convert continuous variables to categorical ones. (Default: `c(0, 0.05, 0.2, 0.8, 0.95, 1)`) Available if `categorize = "quantile"`.
- `max_cluster`: The max number of cluster (Default: 5). Available if `categorize = "kmeans"`.
- `max_score`: Maximum total score (Default: 100).
```{r, }
AUC <- AutoScore_parsimony(
    train_set,
    validation_set,
    rank = ranking,
    max_score = 100,
    n_min = 1,
    n_max = 20,
    categorize = "quantile",
    quantiles = c(0, 0.05, 0.2, 0.8, 0.95, 1)
  )

```

- Determine the optimal number of variables (`num_var`) based on the parsimony plot obtained in STEP(ii). 
- The final list of variables is the first `num_var` variables in the ranked list `ranking` obtained in STEP(i). 
- Optional: User can adjust the finally included variables `final_variables` based on the clinical preferences and knowledge.
```{r}
# Example 1: Top 6 variables are selected
num_var <- 6
final_variables <- names(ranking[1:num_var])

# Example 2: Top 9 variables are selected
num_var <- 9
final_variables <- names(ranking[1:num_var])

# Example 3: Top 6 variables, the 9th and 10th variable are selected
num_var <- 6
final_variables <- names(ranking[c(1:num_var, 9, 10)])
```


```{r finalvariab2, results = "hide", warning=TRUE, message=FALSE,eval=TRUE,include=FALSE}
num_var <- 6
final_variables <- names(ranking[1:num_var])
```

### STEP(iii): Generate initial scores with the final list of variables (Re-run AutoScore Modules 2+3)
- Generate `cut_vec` with current cutoffs of continuous variables, which can be fine-tuned in STEP(iv).
```{r weighting,  warning = FALSE}
cut_vec <- AutoScore_weighting( 
    train_set,
    validation_set,
    final_variables,
    max_score = 100,
    categorize = "quantile",
    quantiles = c(0, 0.05, 0.2, 0.8, 0.95, 1)
  )

```

### STEP(iv): Fine-tune the initial score generated in STEP(iii) (AutoScore Module 5 & Re-run AutoScore Modules 2+3) 
- Revise `cut_vec` with domain knowledge to update the scoring table (AutoScore Module 5).
- Re-run AutoScore Modules 2+3 to generate the updated scores.
- Users can choose any cutoff values and/or any number of categories, but are suggested to choose numbers close to the automatically determined values.

```{r}
## For example, we have current cutoffs of continuous variable: Age 
## ==============  ===========  =====
## variable        interval     point
## ==============  ===========  =====
## Age             <35            0  
##                 [35,49)        7  
##                 [49,76)       17  
##                 [76,89)       23  
##                 >=89          27  
```

- Current cutoffs:`c(35, 49, 76, 89)`. We can fine tune the cutoffs as follows:
```{r}

# Example 1: rounding up to a nice number
cut_vec$Age <- c(35, 50, 75, 90)

# Example 2: changing cutoffs according to clinical knowledge or preference 
cut_vec$Age <- c(35, 50, 75, 90)

# Example 3: combining categories
cut_vec$Age <- c(50, 75, 90)

```


```{r scoring, warning = FALSE}
cut_vec$lactate_mean <- c(0.2, 1, 3, 4)
cut_vec$bun_mean <- c(10, 40)
cut_vec$aniongap_mean <- c(10, 17)
cut_vec$heartrate_mean<- c(70, 98)
scoring_table <- AutoScore_fine_tuning(train_set,
                        validation_set,
                        final_variables,
                        cut_vec,
                        max_score = 100)

```

### STEP(v): Evaluate final risk scores on test dataset (AutoScore Module 6)
- `threshold`: Score threshold for the ROC analysis to generate sensitivity, specificity, etc. If set to `"best"`, the optimal threshold will be calculated (Default: `"best"`).
- `with_label`: Set to `TRUE` if there are labels in the `test_set` and performance will be evaluated accordingly (Default: `TRUE`).
```{r}
pred_score <- AutoScore_testing(test_set, final_variables, cut_vec, scoring_table, threshold = "best", with_label = TRUE)
head(pred_score)
```

- Users could use the `pred_score` for further analysis or export it as the CSV to other software.
- Use `print_roc_performance()` to generate the performance under different score thresholds (e.g., 50).
```{r}
print_roc_performance(pred_score$Label, pred_score$pred_score, threshold = 50)
```


<h id="Demo2">

## **AutoScore Demo #2: Small dataset (sample size = 1000) with cross-validation**

In Demo #2, we demonstrate the use of AutoScore on a comparably small dataset where there are no sufficient samples to form a separate training and validation datasets. Thus, the cross validation is employed to generate the parsimony plot.

### Get small dataset with 1000 samples
```{r}
data("sample_data_small")
```

### Prepare training and test datasets
- Option 1: Prepare two separate datasets to train and test models.
- Option 2: Use demo codes below to randomly split your dataset into training and test datasets (70% and 30%, respectively). For cross-validation, `train_set` is equal to `validation_set` and the ratio of `validation_set` should be 0. Then cross-validation will be implemented in the STEP(ii) `AutoScore_parsimony()`.
```{r}
set.seed(4)
out_split <- split_data(data = sample_data_small, ratio = c(0.7, 0, 0.3), cross_validation = TRUE)
train_set <- out_split$train_set
validation_set <- out_split$validation_set
test_set <- out_split$test_set
```


### STEP(i): Generate variable ranking list (AutoScore Module 1)
- `ntree`: umber of trees in the random forest algorithm (Default: 100).
```{r}
ranking <- AutoScore_rank(train_set, ntree=100)
```

### STEP(ii): Select the best model with parsimony plot (AutoScore Modules 2+3+4)
- `nmin`: Minimum number of selected variables (Default: 1).
- `nmax`: Maximum number of selected variables (Default: 20).
- `categorize`: Methods for categorize continuous variables. Options include `"quantile"` or `"kmeans"` (Default: `"quantile"`).
- `quantiles`: Predefined quantiles to convert continuous variables to categorical ones. (Default: `c(0, 0.05, 0.2, 0.8, 0.95, 1)`) Available if `categorize = "quantile"`.
- `max_cluster`: The max number of cluster (Default: 5). Available if `categorize = "kmeans"`.
- `max_score` Maximum total score (Default: 100).
- `cross_validation` : `TRUE` if cross-validation is needed, especially for small datasets.
- `fold` The number of folds used in cross validation (Default: 10). Available if `cross_validation = TRUE`.
- `do_trace` If set to `TRUE`, all results based on each fold of cross-validation would be printed out and plotted (Default: `FALSE`). Available if `cross_validation = TRUE`.
```{r parsi, warning = FALSE}
AUC <- AutoScore_parsimony(
    train_set,
    validation_set,
    rank = ranking,
    max_score = 100,
    n_min = 1,
    n_max = 20,
    cross_validation = TRUE,
    categorize = "quantile",
    fold = 10,
    quantiles = c(0, 0.25, 0.5, 0.75, 1), #c(0, 0.05, 0.2, 0.8, 0.95, 1)
    do_trace = FALSE
  )

```

- Determine the optimal number of variables (`num_var`) based on the parsimony plot obtained in STEP(ii). 
- The final list of variables is the first `num_var` variables in the ranked list `ranking` obtained in STEP(i). 
- Optional: User can adjust the finally included variables `final_variables` based on the clinical preferences and knowledge).
```{r}
# Example 1: Top 6 variables are selected
num_var <- 6
final_variables <- names(ranking[1:num_var])

# Example 2: Top 9 variables are selected
num_var <- 9
final_variables <- names(ranking[1:num_var])

# Example 3: Top 6 variables, the 9th and 10th variable are selected
num_var <- 6
final_variables <- names(ranking[c(1:num_var, 9, 10)])
```

```{r finalvariab, results = "hide", warning=FALSE, message=FALSE,eval=TRUE,include=FALSE}
num_var <- 5
final_variables <- names(ranking[1:num_var])
```
### STEP(iii): Generate initial scores with the final list of variables (Re-run AutoScore Modules 2+3)
- Generate `cut_vec` with current cutoffs of continuous variables, which can be fine-tuned in STEP(iv).
```{r weighting2,  warning = TRUE}
cut_vec <- AutoScore_weighting( 
    train_set,
    validation_set,
    final_variables,
    max_score = 100,
    categorize = "quantile",
    quantiles = c(0, 0.05, 0.2, 0.8, 0.95, 1)
  )

```

### STEP(iv): Fine-tune the initial score generated in STEP(iii) (AutoScore Module 5 & Re-run AutoScore Modules 2+3) 
- Revise `cut_vec` with domain knowledge to update the scoring table (AutoScore Module 5).
- Re-run AutoScore Modules 2+3 to generate the updated scores.
- Users can choose any cutoff values and/or any number of categories, but are suggested to choose numbers close to the automatically determined values.
```{r}
## For example, we have current cutoffs of continuous variable: Age 
## ==============  ===========  =====
## variable        interval     point
## ==============  ===========  =====
#> bun_mean       <9             0  
#>                [9,43.2)       1  
#>                [43.2,59)      9  
#>                >=59          13  
```

- Current cutoffs: `c(9, 43.2, 59)`. We can fine tune the cutoffs as follows:
- Note: It is just a demo using simulated data, and thus, the result might not be clinically meaningful.
```{r}

# Example 1: rounding up to a nice number
cut_vec$bun_mean <- c(9, 45, 60)

# Example 2: changing cutoffs according to clinical knowledge or preference 
cut_vec$bun_mean <- c(15, 45, 60)

# Example 3: combining categories
cut_vec$bun_mean <- c(45, 60)

```

```{r, results = "hide", warning=FALSE, message=FALSE,eval=TRUE,include=FALSE}
cut_vec$bun_mean <- c(45, 60)
```

```{r scoring2, warning = TRUE}
cut_vec$lactate_mean <- c(1, 2, 3)
cut_vec$Age <- c(35, 50, 80)
cut_vec$aniongap_mean <- c(8, 12, 18)
cut_vec$resprate_mean <- c(15, 22)
scoring_table <- AutoScore_fine_tuning(train_set,
                        validation_set,
                        final_variables,
                        cut_vec,
                        max_score = 100)

```

### STEP(v): Evaluate final risk scores on test dataset (AutoScore Module 6)
- `threshold`: Score threshold for the ROC analysis to generate sensitivity, specificity, etc. If set to `"best"`, the optimal threshold will be calculated (Default: `"best"`).
- `with_label`: Set to `TRUE` if there are labels in the `test_set` and performance will be evaluated accordingly (Default: `TRUE`).
```{r}
pred_score <- AutoScore_testing(test_set, final_variables, cut_vec, scoring_table, threshold = "best", with_label = TRUE)
head(pred_score)
```

- Users could use the `pred_score` for further analysis or export it as the CSV to other software.
- Use `print_roc_performance()` to generate the performance under different score thresholds (e.g., 90).
- Note: It is just a demo using simulated data, and thus, the result might not be clinically meaningful.
```{r}
print_roc_performance(pred_score$Label, pred_score$pred_score, threshold = 90)
```


## **Appendix: Other functions**


- Compute descriptive table (usually Table 1 in medical literature) for the dataset.
```{r table one, warning = FALSE}
compute_descriptive_table(sample_data)
```

- Perform univariable analysis and generate the result table with odd ratios.
```{r}
uni_table<-compute_uni_variable_table(sample_data)
print(uni_table)
```

- Perform multivariable analysis and generate the result table with adjusted odd ratios.
```{r}
multi_table<-compute_multi_variable_table(sample_data)
print(multi_table)
```
